{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import copy\n",
    "import wandb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel, affine=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel, affine=False)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel, affine=False)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)        \n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)        \n",
    "        self.fc = nn.Linear(512, num_classes, bias = False)\n",
    "            \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class QuantizeLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(QuantizeLayer, self).__init__()\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            weight_q = torch.sign(self.layer.weight)\n",
    "            if self.layer.bias is not None:\n",
    "                bias_q = torch.sign(self.layer.bias)\n",
    "                return nn.functional.conv2d(x, weight_q, bias_q, self.layer.stride, self.layer.padding, self.layer.dilation, self.layer.groups)\n",
    "            else:\n",
    "                return nn.functional.conv2d(x, weight_q, None, self.layer.stride, self.layer.padding, self.layer.dilation, self.layer.groups)\n",
    "        elif isinstance(self.layer, nn.Linear):\n",
    "            weight_q = torch.sign(self.layer.weight)\n",
    "            if self.layer.bias is not None:\n",
    "                bias_q = torch.sign(self.layer.bias)\n",
    "                return nn.functional.linear(x, weight_q, bias_q)\n",
    "            else:\n",
    "                return nn.functional.linear(x, weight_q, None)\n",
    "        else:\n",
    "            return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w6kmgjb8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▂▄▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>quantized_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.2858</td></tr><tr><td>loss</td><td>1.81862</td></tr><tr><td>quantized_accuracy</td><td>0.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ASkewSGD</strong> at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3/runs/w6kmgjb8' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3/runs/w6kmgjb8</a><br/> View project at: <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241228_040608-w6kmgjb8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w6kmgjb8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Programming\\experiment_2024summer\\exp\\wandb\\run-20241228_041408-ar24jzoi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3/runs/ar24jzoi' target=\"_blank\">ASkewSGD</a></strong> to <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3/runs/ar24jzoi' target=\"_blank\">https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3/runs/ar24jzoi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hokfong-the-chinese-university-of-hong-kong/exp3/runs/ar24jzoi?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2172c826d60>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the ResNet18 on CIFAR-10\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set hyperparameter\n",
    "EPOCH = 100\n",
    "pre_epoch = 0\n",
    "BATCH_SIZE = 100\n",
    "LR = 0.06\n",
    "alpha = 0.5\n",
    "\n",
    "#prepare dataset and preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(40),\n",
    "    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "    ratio=(1.0, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=3)\n",
    "\n",
    "# Labels in CIFAR10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define ResNet18\n",
    "net = ResNet18().to(device)\n",
    "\n",
    "# Define loss funtion & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"exp3\",\n",
    "    name=\"ASkewSGD\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"batch_size\": 100,\n",
    "        \"architecture\": \"ASkewSGD\",\n",
    "        \"dataset\": \"CIFAR10\",\n",
    "        \"lr\": 0.06,\n",
    "        \"alpha\": 0.5\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE = 'resnet18_original.pt'\n",
    "\n",
    "# net = ResNet18().to(device)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# checkpoint = torch.load(FILE)\n",
    "# net.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# torch.save({'model_state_dict': net.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:1] Loss: 2.420 | Acc: 7.000% \n",
      "[epoch:1, iter:2] Loss: 9.721 | Acc: 8.000% \n",
      "[epoch:1, iter:3] Loss: 14.689 | Acc: 8.667% \n",
      "[epoch:1, iter:4] Loss: 13.744 | Acc: 9.250% \n",
      "[epoch:1, iter:5] Loss: 14.141 | Acc: 9.200% \n",
      "[epoch:1, iter:6] Loss: 13.156 | Acc: 9.667% \n",
      "[epoch:1, iter:7] Loss: 12.486 | Acc: 10.143% \n",
      "[epoch:1, iter:8] Loss: 11.707 | Acc: 9.375% \n",
      "[epoch:1, iter:9] Loss: 11.081 | Acc: 9.778% \n",
      "[epoch:1, iter:10] Loss: 10.640 | Acc: 9.700% \n",
      "[epoch:1, iter:11] Loss: 10.225 | Acc: 9.909% \n",
      "[epoch:1, iter:12] Loss: 9.832 | Acc: 10.500% \n",
      "[epoch:1, iter:13] Loss: 9.714 | Acc: 10.154% \n",
      "[epoch:1, iter:14] Loss: 9.822 | Acc: 10.357% \n",
      "[epoch:1, iter:15] Loss: 9.853 | Acc: 10.267% \n",
      "[epoch:1, iter:16] Loss: 9.729 | Acc: 10.562% \n",
      "[epoch:1, iter:17] Loss: 9.438 | Acc: 10.471% \n",
      "[epoch:1, iter:18] Loss: 9.267 | Acc: 10.444% \n",
      "[epoch:1, iter:19] Loss: 9.090 | Acc: 10.421% \n",
      "[epoch:1, iter:20] Loss: 8.839 | Acc: 10.550% \n",
      "[epoch:1, iter:21] Loss: 8.671 | Acc: 10.476% \n",
      "[epoch:1, iter:22] Loss: 8.504 | Acc: 10.773% \n",
      "[epoch:1, iter:23] Loss: 8.336 | Acc: 10.870% \n",
      "[epoch:1, iter:24] Loss: 8.157 | Acc: 11.000% \n",
      "[epoch:1, iter:25] Loss: 7.979 | Acc: 11.000% \n",
      "[epoch:1, iter:26] Loss: 7.786 | Acc: 10.962% \n",
      "[epoch:1, iter:27] Loss: 7.601 | Acc: 11.000% \n",
      "[epoch:1, iter:28] Loss: 7.453 | Acc: 11.000% \n",
      "[epoch:1, iter:29] Loss: 7.305 | Acc: 11.000% \n",
      "[epoch:1, iter:30] Loss: 7.171 | Acc: 10.867% \n",
      "[epoch:1, iter:31] Loss: 7.088 | Acc: 10.839% \n",
      "[epoch:1, iter:32] Loss: 6.961 | Acc: 10.719% \n",
      "[epoch:1, iter:33] Loss: 6.834 | Acc: 10.667% \n",
      "[epoch:1, iter:34] Loss: 6.707 | Acc: 10.765% \n",
      "[epoch:1, iter:35] Loss: 6.632 | Acc: 10.914% \n",
      "[epoch:1, iter:36] Loss: 6.545 | Acc: 10.806% \n",
      "[epoch:1, iter:37] Loss: 6.463 | Acc: 10.784% \n",
      "[epoch:1, iter:38] Loss: 6.383 | Acc: 10.816% \n",
      "[epoch:1, iter:39] Loss: 6.307 | Acc: 10.795% \n",
      "[epoch:1, iter:40] Loss: 6.220 | Acc: 10.875% \n",
      "[epoch:1, iter:41] Loss: 6.131 | Acc: 10.951% \n",
      "[epoch:1, iter:42] Loss: 6.058 | Acc: 11.095% \n",
      "[epoch:1, iter:43] Loss: 5.989 | Acc: 10.977% \n",
      "[epoch:1, iter:44] Loss: 5.910 | Acc: 11.045% \n",
      "[epoch:1, iter:45] Loss: 5.837 | Acc: 11.022% \n",
      "[epoch:1, iter:46] Loss: 5.762 | Acc: 11.152% \n",
      "[epoch:1, iter:47] Loss: 5.691 | Acc: 11.213% \n",
      "[epoch:1, iter:48] Loss: 5.645 | Acc: 11.208% \n",
      "[epoch:1, iter:49] Loss: 5.599 | Acc: 11.245% \n",
      "[epoch:1, iter:50] Loss: 5.538 | Acc: 11.260% \n",
      "[epoch:1, iter:51] Loss: 5.490 | Acc: 11.294% \n",
      "[epoch:1, iter:52] Loss: 5.435 | Acc: 11.442% \n",
      "[epoch:1, iter:53] Loss: 5.381 | Acc: 11.509% \n",
      "[epoch:1, iter:54] Loss: 5.325 | Acc: 11.444% \n",
      "[epoch:1, iter:55] Loss: 5.275 | Acc: 11.509% \n",
      "[epoch:1, iter:56] Loss: 5.226 | Acc: 11.482% \n",
      "[epoch:1, iter:57] Loss: 5.183 | Acc: 11.579% \n",
      "[epoch:1, iter:58] Loss: 5.139 | Acc: 11.655% \n",
      "[epoch:1, iter:59] Loss: 5.093 | Acc: 11.644% \n",
      "[epoch:1, iter:60] Loss: 5.045 | Acc: 11.817% \n",
      "[epoch:1, iter:61] Loss: 5.001 | Acc: 11.852% \n",
      "[epoch:1, iter:62] Loss: 4.963 | Acc: 11.919% \n",
      "[epoch:1, iter:63] Loss: 4.927 | Acc: 11.937% \n",
      "[epoch:1, iter:64] Loss: 4.891 | Acc: 12.109% \n",
      "[epoch:1, iter:65] Loss: 4.857 | Acc: 12.138% \n",
      "[epoch:1, iter:66] Loss: 4.824 | Acc: 12.288% \n",
      "[epoch:1, iter:67] Loss: 4.792 | Acc: 12.373% \n",
      "[epoch:1, iter:68] Loss: 4.757 | Acc: 12.471% \n",
      "[epoch:1, iter:69] Loss: 4.721 | Acc: 12.536% \n",
      "[epoch:1, iter:70] Loss: 4.696 | Acc: 12.629% \n",
      "[epoch:1, iter:71] Loss: 4.666 | Acc: 12.648% \n",
      "[epoch:1, iter:72] Loss: 4.644 | Acc: 12.556% \n",
      "[epoch:1, iter:73] Loss: 4.610 | Acc: 12.658% \n",
      "[epoch:1, iter:74] Loss: 4.586 | Acc: 12.581% \n",
      "[epoch:1, iter:75] Loss: 4.560 | Acc: 12.680% \n",
      "[epoch:1, iter:76] Loss: 4.538 | Acc: 12.658% \n",
      "[epoch:1, iter:77] Loss: 4.508 | Acc: 12.727% \n",
      "[epoch:1, iter:78] Loss: 4.482 | Acc: 12.731% \n",
      "[epoch:1, iter:79] Loss: 4.459 | Acc: 12.658% \n",
      "[epoch:1, iter:80] Loss: 4.437 | Acc: 12.712% \n",
      "[epoch:1, iter:81] Loss: 4.424 | Acc: 12.691% \n",
      "[epoch:1, iter:82] Loss: 4.399 | Acc: 12.756% \n",
      "[epoch:1, iter:83] Loss: 4.382 | Acc: 12.747% \n",
      "[epoch:1, iter:84] Loss: 4.361 | Acc: 12.857% \n",
      "[epoch:1, iter:85] Loss: 4.339 | Acc: 12.988% \n",
      "[epoch:1, iter:86] Loss: 4.317 | Acc: 13.081% \n",
      "[epoch:1, iter:87] Loss: 4.298 | Acc: 13.195% \n",
      "[epoch:1, iter:88] Loss: 4.277 | Acc: 13.295% \n",
      "[epoch:1, iter:89] Loss: 4.261 | Acc: 13.371% \n",
      "[epoch:1, iter:90] Loss: 4.241 | Acc: 13.511% \n",
      "[epoch:1, iter:91] Loss: 4.221 | Acc: 13.615% \n",
      "[epoch:1, iter:92] Loss: 4.200 | Acc: 13.609% \n",
      "[epoch:1, iter:93] Loss: 4.180 | Acc: 13.710% \n",
      "[epoch:1, iter:94] Loss: 4.158 | Acc: 13.734% \n",
      "[epoch:1, iter:95] Loss: 4.139 | Acc: 13.758% \n",
      "[epoch:1, iter:96] Loss: 4.122 | Acc: 13.844% \n",
      "[epoch:1, iter:97] Loss: 4.101 | Acc: 13.938% \n",
      "[epoch:1, iter:98] Loss: 4.081 | Acc: 14.010% \n",
      "[epoch:1, iter:99] Loss: 4.067 | Acc: 14.020% \n",
      "[epoch:1, iter:100] Loss: 4.052 | Acc: 14.120% \n",
      "[epoch:1, iter:101] Loss: 4.035 | Acc: 14.139% \n",
      "[epoch:1, iter:102] Loss: 4.018 | Acc: 14.176% \n",
      "[epoch:1, iter:103] Loss: 4.003 | Acc: 14.282% \n",
      "[epoch:1, iter:104] Loss: 3.985 | Acc: 14.365% \n",
      "[epoch:1, iter:105] Loss: 3.968 | Acc: 14.448% \n",
      "[epoch:1, iter:106] Loss: 3.953 | Acc: 14.519% \n",
      "[epoch:1, iter:107] Loss: 3.937 | Acc: 14.533% \n",
      "[epoch:1, iter:108] Loss: 3.920 | Acc: 14.611% \n",
      "[epoch:1, iter:109] Loss: 3.904 | Acc: 14.661% \n",
      "[epoch:1, iter:110] Loss: 3.890 | Acc: 14.691% \n",
      "[epoch:1, iter:111] Loss: 3.874 | Acc: 14.703% \n",
      "[epoch:1, iter:112] Loss: 3.858 | Acc: 14.777% \n",
      "[epoch:1, iter:113] Loss: 3.845 | Acc: 14.796% \n",
      "[epoch:1, iter:114] Loss: 3.830 | Acc: 14.807% \n",
      "[epoch:1, iter:115] Loss: 3.814 | Acc: 14.861% \n",
      "[epoch:1, iter:116] Loss: 3.802 | Acc: 14.845% \n",
      "[epoch:1, iter:117] Loss: 3.787 | Acc: 14.880% \n",
      "[epoch:1, iter:118] Loss: 3.774 | Acc: 14.983% \n",
      "[epoch:1, iter:119] Loss: 3.760 | Acc: 15.034% \n",
      "[epoch:1, iter:120] Loss: 3.748 | Acc: 15.092% \n",
      "[epoch:1, iter:121] Loss: 3.735 | Acc: 15.132% \n",
      "[epoch:1, iter:122] Loss: 3.721 | Acc: 15.156% \n",
      "[epoch:1, iter:123] Loss: 3.710 | Acc: 15.179% \n",
      "[epoch:1, iter:124] Loss: 3.701 | Acc: 15.137% \n",
      "[epoch:1, iter:125] Loss: 3.688 | Acc: 15.240% \n",
      "[epoch:1, iter:126] Loss: 3.678 | Acc: 15.310% \n",
      "[epoch:1, iter:127] Loss: 3.670 | Acc: 15.291% \n",
      "[epoch:1, iter:128] Loss: 3.659 | Acc: 15.328% \n",
      "[epoch:1, iter:129] Loss: 3.649 | Acc: 15.357% \n",
      "[epoch:1, iter:130] Loss: 3.639 | Acc: 15.362% \n",
      "[epoch:1, iter:131] Loss: 3.629 | Acc: 15.344% \n",
      "[epoch:1, iter:132] Loss: 3.619 | Acc: 15.364% \n",
      "[epoch:1, iter:133] Loss: 3.611 | Acc: 15.421% \n",
      "[epoch:1, iter:134] Loss: 3.599 | Acc: 15.485% \n",
      "[epoch:1, iter:135] Loss: 3.589 | Acc: 15.481% \n",
      "[epoch:1, iter:136] Loss: 3.582 | Acc: 15.551% \n",
      "[epoch:1, iter:137] Loss: 3.569 | Acc: 15.650% \n",
      "[epoch:1, iter:138] Loss: 3.561 | Acc: 15.667% \n",
      "[epoch:1, iter:139] Loss: 3.550 | Acc: 15.748% \n",
      "[epoch:1, iter:140] Loss: 3.540 | Acc: 15.736% \n",
      "[epoch:1, iter:141] Loss: 3.529 | Acc: 15.787% \n",
      "[epoch:1, iter:142] Loss: 3.520 | Acc: 15.803% \n",
      "[epoch:1, iter:143] Loss: 3.511 | Acc: 15.888% \n",
      "[epoch:1, iter:144] Loss: 3.502 | Acc: 15.931% \n",
      "[epoch:1, iter:145] Loss: 3.492 | Acc: 15.945% \n",
      "[epoch:1, iter:146] Loss: 3.482 | Acc: 16.048% \n",
      "[epoch:1, iter:147] Loss: 3.474 | Acc: 16.122% \n",
      "[epoch:1, iter:148] Loss: 3.463 | Acc: 16.243% \n",
      "[epoch:1, iter:149] Loss: 3.454 | Acc: 16.322% \n",
      "[epoch:1, iter:150] Loss: 3.445 | Acc: 16.367% \n",
      "[epoch:1, iter:151] Loss: 3.437 | Acc: 16.384% \n",
      "[epoch:1, iter:152] Loss: 3.428 | Acc: 16.447% \n",
      "[epoch:1, iter:153] Loss: 3.420 | Acc: 16.484% \n",
      "[epoch:1, iter:154] Loss: 3.411 | Acc: 16.552% \n",
      "[epoch:1, iter:155] Loss: 3.401 | Acc: 16.613% \n",
      "[epoch:1, iter:156] Loss: 3.394 | Acc: 16.615% \n",
      "[epoch:1, iter:157] Loss: 3.386 | Acc: 16.611% \n",
      "[epoch:1, iter:158] Loss: 3.378 | Acc: 16.633% \n",
      "[epoch:1, iter:159] Loss: 3.371 | Acc: 16.654% \n",
      "[epoch:1, iter:160] Loss: 3.363 | Acc: 16.731% \n",
      "[epoch:1, iter:161] Loss: 3.356 | Acc: 16.770% \n",
      "[epoch:1, iter:162] Loss: 3.348 | Acc: 16.778% \n",
      "[epoch:1, iter:163] Loss: 3.340 | Acc: 16.798% \n",
      "[epoch:1, iter:164] Loss: 3.332 | Acc: 16.909% \n",
      "[epoch:1, iter:165] Loss: 3.325 | Acc: 16.945% \n",
      "[epoch:1, iter:166] Loss: 3.317 | Acc: 17.000% \n",
      "[epoch:1, iter:167] Loss: 3.309 | Acc: 17.060% \n",
      "[epoch:1, iter:168] Loss: 3.301 | Acc: 17.125% \n",
      "[epoch:1, iter:169] Loss: 3.294 | Acc: 17.154% \n",
      "[epoch:1, iter:170] Loss: 3.287 | Acc: 17.188% \n",
      "[epoch:1, iter:171] Loss: 3.280 | Acc: 17.205% \n",
      "[epoch:1, iter:172] Loss: 3.272 | Acc: 17.256% \n",
      "[epoch:1, iter:173] Loss: 3.268 | Acc: 17.335% \n",
      "[epoch:1, iter:174] Loss: 3.261 | Acc: 17.345% \n",
      "[epoch:1, iter:175] Loss: 3.256 | Acc: 17.349% \n",
      "[epoch:1, iter:176] Loss: 3.250 | Acc: 17.415% \n",
      "[epoch:1, iter:177] Loss: 3.243 | Acc: 17.441% \n",
      "[epoch:1, iter:178] Loss: 3.236 | Acc: 17.461% \n",
      "[epoch:1, iter:179] Loss: 3.230 | Acc: 17.475% \n",
      "[epoch:1, iter:180] Loss: 3.225 | Acc: 17.528% \n",
      "[epoch:1, iter:181] Loss: 3.219 | Acc: 17.564% \n",
      "[epoch:1, iter:182] Loss: 3.213 | Acc: 17.621% \n",
      "[epoch:1, iter:183] Loss: 3.205 | Acc: 17.710% \n",
      "[epoch:1, iter:184] Loss: 3.200 | Acc: 17.783% \n",
      "[epoch:1, iter:185] Loss: 3.194 | Acc: 17.805% \n",
      "[epoch:1, iter:186] Loss: 3.188 | Acc: 17.855% \n",
      "[epoch:1, iter:187] Loss: 3.182 | Acc: 17.882% \n",
      "[epoch:1, iter:188] Loss: 3.175 | Acc: 17.920% \n",
      "[epoch:1, iter:189] Loss: 3.170 | Acc: 17.915% \n",
      "[epoch:1, iter:190] Loss: 3.164 | Acc: 17.958% \n",
      "[epoch:1, iter:191] Loss: 3.157 | Acc: 18.052% \n",
      "[epoch:1, iter:192] Loss: 3.152 | Acc: 18.047% \n",
      "[epoch:1, iter:193] Loss: 3.146 | Acc: 18.093% \n",
      "[epoch:1, iter:194] Loss: 3.140 | Acc: 18.175% \n",
      "[epoch:1, iter:195] Loss: 3.134 | Acc: 18.241% \n",
      "[epoch:1, iter:196] Loss: 3.128 | Acc: 18.281% \n",
      "[epoch:1, iter:197] Loss: 3.123 | Acc: 18.355% \n",
      "[epoch:1, iter:198] Loss: 3.118 | Acc: 18.429% \n",
      "[epoch:1, iter:199] Loss: 3.114 | Acc: 18.487% \n",
      "[epoch:1, iter:200] Loss: 3.109 | Acc: 18.500% \n",
      "[epoch:1, iter:201] Loss: 3.104 | Acc: 18.517% \n",
      "[epoch:1, iter:202] Loss: 3.099 | Acc: 18.594% \n",
      "[epoch:1, iter:203] Loss: 3.094 | Acc: 18.626% \n",
      "[epoch:1, iter:204] Loss: 3.089 | Acc: 18.691% \n",
      "[epoch:1, iter:205] Loss: 3.084 | Acc: 18.737% \n",
      "[epoch:1, iter:206] Loss: 3.080 | Acc: 18.772% \n",
      "[epoch:1, iter:207] Loss: 3.075 | Acc: 18.778% \n",
      "[epoch:1, iter:208] Loss: 3.070 | Acc: 18.837% \n",
      "[epoch:1, iter:209] Loss: 3.066 | Acc: 18.890% \n",
      "[epoch:1, iter:210] Loss: 3.061 | Acc: 18.914% \n",
      "[epoch:1, iter:211] Loss: 3.055 | Acc: 18.953% \n",
      "[epoch:1, iter:212] Loss: 3.050 | Acc: 18.986% \n",
      "[epoch:1, iter:213] Loss: 3.048 | Acc: 18.986% \n",
      "[epoch:1, iter:214] Loss: 3.043 | Acc: 19.028% \n",
      "[epoch:1, iter:215] Loss: 3.041 | Acc: 19.056% \n",
      "[epoch:1, iter:216] Loss: 3.039 | Acc: 19.079% \n",
      "[epoch:1, iter:217] Loss: 3.033 | Acc: 19.092% \n",
      "[epoch:1, iter:218] Loss: 3.030 | Acc: 19.156% \n",
      "[epoch:1, iter:219] Loss: 3.027 | Acc: 19.196% \n",
      "[epoch:1, iter:220] Loss: 3.022 | Acc: 19.236% \n",
      "[epoch:1, iter:221] Loss: 3.018 | Acc: 19.294% \n",
      "[epoch:1, iter:222] Loss: 3.013 | Acc: 19.320% \n",
      "[epoch:1, iter:223] Loss: 3.009 | Acc: 19.390% \n",
      "[epoch:1, iter:224] Loss: 3.006 | Acc: 19.379% \n",
      "[epoch:1, iter:225] Loss: 3.002 | Acc: 19.431% \n",
      "[epoch:1, iter:226] Loss: 2.998 | Acc: 19.482% \n",
      "[epoch:1, iter:227] Loss: 2.993 | Acc: 19.511% \n",
      "[epoch:1, iter:228] Loss: 2.988 | Acc: 19.575% \n",
      "[epoch:1, iter:229] Loss: 2.984 | Acc: 19.563% \n",
      "[epoch:1, iter:230] Loss: 2.979 | Acc: 19.596% \n",
      "[epoch:1, iter:231] Loss: 2.974 | Acc: 19.619% \n",
      "[epoch:1, iter:232] Loss: 2.970 | Acc: 19.659% \n",
      "[epoch:1, iter:233] Loss: 2.965 | Acc: 19.708% \n",
      "[epoch:1, iter:234] Loss: 2.961 | Acc: 19.744% \n",
      "[epoch:1, iter:235] Loss: 2.956 | Acc: 19.770% \n",
      "[epoch:1, iter:236] Loss: 2.952 | Acc: 19.788% \n",
      "[epoch:1, iter:237] Loss: 2.948 | Acc: 19.835% \n",
      "[epoch:1, iter:238] Loss: 2.945 | Acc: 19.832% \n",
      "[epoch:1, iter:239] Loss: 2.941 | Acc: 19.841% \n",
      "[epoch:1, iter:240] Loss: 2.937 | Acc: 19.854% \n",
      "[epoch:1, iter:241] Loss: 2.934 | Acc: 19.876% \n",
      "[epoch:1, iter:242] Loss: 2.930 | Acc: 19.888% \n",
      "[epoch:1, iter:243] Loss: 2.926 | Acc: 19.922% \n",
      "[epoch:1, iter:244] Loss: 2.921 | Acc: 19.959% \n",
      "[epoch:1, iter:245] Loss: 2.917 | Acc: 20.000% \n",
      "[epoch:1, iter:246] Loss: 2.914 | Acc: 20.037% \n",
      "[epoch:1, iter:247] Loss: 2.910 | Acc: 20.081% \n",
      "[epoch:1, iter:248] Loss: 2.907 | Acc: 20.081% \n",
      "[epoch:1, iter:249] Loss: 2.905 | Acc: 20.104% \n",
      "[epoch:1, iter:250] Loss: 2.901 | Acc: 20.152% \n",
      "[epoch:1, iter:251] Loss: 2.898 | Acc: 20.175% \n",
      "[epoch:1, iter:252] Loss: 2.894 | Acc: 20.226% \n",
      "[epoch:1, iter:253] Loss: 2.890 | Acc: 20.265% \n",
      "[epoch:1, iter:254] Loss: 2.887 | Acc: 20.260% \n",
      "[epoch:1, iter:255] Loss: 2.884 | Acc: 20.278% \n",
      "[epoch:1, iter:256] Loss: 2.881 | Acc: 20.309% \n",
      "[epoch:1, iter:257] Loss: 2.876 | Acc: 20.362% \n",
      "[epoch:1, iter:258] Loss: 2.872 | Acc: 20.457% \n",
      "[epoch:1, iter:259] Loss: 2.868 | Acc: 20.467% \n",
      "[epoch:1, iter:260] Loss: 2.864 | Acc: 20.535% \n",
      "[epoch:1, iter:261] Loss: 2.860 | Acc: 20.552% \n",
      "[epoch:1, iter:262] Loss: 2.857 | Acc: 20.557% \n",
      "[epoch:1, iter:263] Loss: 2.854 | Acc: 20.627% \n",
      "[epoch:1, iter:264] Loss: 2.850 | Acc: 20.667% \n",
      "[epoch:1, iter:265] Loss: 2.846 | Acc: 20.687% \n",
      "[epoch:1, iter:266] Loss: 2.842 | Acc: 20.737% \n",
      "[epoch:1, iter:267] Loss: 2.839 | Acc: 20.772% \n",
      "[epoch:1, iter:268] Loss: 2.836 | Acc: 20.795% \n",
      "[epoch:1, iter:269] Loss: 2.833 | Acc: 20.810% \n",
      "[epoch:1, iter:270] Loss: 2.829 | Acc: 20.852% \n",
      "[epoch:1, iter:271] Loss: 2.827 | Acc: 20.860% \n",
      "[epoch:1, iter:272] Loss: 2.824 | Acc: 20.901% \n",
      "[epoch:1, iter:273] Loss: 2.822 | Acc: 20.938% \n",
      "[epoch:1, iter:274] Loss: 2.818 | Acc: 20.971% \n",
      "[epoch:1, iter:275] Loss: 2.815 | Acc: 21.000% \n",
      "[epoch:1, iter:276] Loss: 2.812 | Acc: 21.040% \n",
      "[epoch:1, iter:277] Loss: 2.809 | Acc: 21.061% \n",
      "[epoch:1, iter:278] Loss: 2.807 | Acc: 21.076% \n",
      "[epoch:1, iter:279] Loss: 2.803 | Acc: 21.125% \n",
      "[epoch:1, iter:280] Loss: 2.801 | Acc: 21.136% \n",
      "[epoch:1, iter:281] Loss: 2.798 | Acc: 21.139% \n",
      "[epoch:1, iter:282] Loss: 2.796 | Acc: 21.167% \n",
      "[epoch:1, iter:283] Loss: 2.793 | Acc: 21.205% \n",
      "[epoch:1, iter:284] Loss: 2.789 | Acc: 21.232% \n",
      "[epoch:1, iter:285] Loss: 2.786 | Acc: 21.253% \n",
      "[epoch:1, iter:286] Loss: 2.783 | Acc: 21.259% \n",
      "[epoch:1, iter:287] Loss: 2.780 | Acc: 21.293% \n",
      "[epoch:1, iter:288] Loss: 2.777 | Acc: 21.323% \n",
      "[epoch:1, iter:289] Loss: 2.774 | Acc: 21.343% \n",
      "[epoch:1, iter:290] Loss: 2.771 | Acc: 21.383% \n",
      "[epoch:1, iter:291] Loss: 2.769 | Acc: 21.405% \n",
      "[epoch:1, iter:292] Loss: 2.767 | Acc: 21.418% \n",
      "[epoch:1, iter:293] Loss: 2.764 | Acc: 21.454% \n",
      "[epoch:1, iter:294] Loss: 2.761 | Acc: 21.463% \n",
      "[epoch:1, iter:295] Loss: 2.758 | Acc: 21.512% \n",
      "[epoch:1, iter:296] Loss: 2.755 | Acc: 21.517% \n",
      "[epoch:1, iter:297] Loss: 2.752 | Acc: 21.562% \n",
      "[epoch:1, iter:298] Loss: 2.749 | Acc: 21.597% \n",
      "[epoch:1, iter:299] Loss: 2.745 | Acc: 21.645% \n",
      "[epoch:1, iter:300] Loss: 2.742 | Acc: 21.683% \n",
      "[epoch:1, iter:301] Loss: 2.740 | Acc: 21.684% \n",
      "[epoch:1, iter:302] Loss: 2.737 | Acc: 21.738% \n",
      "[epoch:1, iter:303] Loss: 2.734 | Acc: 21.772% \n",
      "[epoch:1, iter:304] Loss: 2.731 | Acc: 21.796% \n",
      "[epoch:1, iter:305] Loss: 2.729 | Acc: 21.826% \n",
      "[epoch:1, iter:306] Loss: 2.725 | Acc: 21.856% \n",
      "[epoch:1, iter:307] Loss: 2.722 | Acc: 21.889% \n",
      "[epoch:1, iter:308] Loss: 2.719 | Acc: 21.942% \n",
      "[epoch:1, iter:309] Loss: 2.717 | Acc: 21.964% \n",
      "[epoch:1, iter:310] Loss: 2.714 | Acc: 21.971% \n",
      "[epoch:1, iter:311] Loss: 2.711 | Acc: 21.977% \n",
      "[epoch:1, iter:312] Loss: 2.709 | Acc: 22.003% \n",
      "[epoch:1, iter:313] Loss: 2.706 | Acc: 22.048% \n",
      "[epoch:1, iter:314] Loss: 2.703 | Acc: 22.076% \n",
      "[epoch:1, iter:315] Loss: 2.702 | Acc: 22.070% \n",
      "[epoch:1, iter:316] Loss: 2.699 | Acc: 22.104% \n",
      "[epoch:1, iter:317] Loss: 2.696 | Acc: 22.151% \n",
      "[epoch:1, iter:318] Loss: 2.694 | Acc: 22.173% \n",
      "[epoch:1, iter:319] Loss: 2.691 | Acc: 22.210% \n",
      "[epoch:1, iter:320] Loss: 2.690 | Acc: 22.222% \n",
      "[epoch:1, iter:321] Loss: 2.687 | Acc: 22.240% \n",
      "[epoch:1, iter:322] Loss: 2.685 | Acc: 22.245% \n",
      "[epoch:1, iter:323] Loss: 2.683 | Acc: 22.257% \n",
      "[epoch:1, iter:324] Loss: 2.680 | Acc: 22.296% \n",
      "[epoch:1, iter:325] Loss: 2.678 | Acc: 22.308% \n",
      "[epoch:1, iter:326] Loss: 2.677 | Acc: 22.294% \n",
      "[epoch:1, iter:327] Loss: 2.674 | Acc: 22.336% \n",
      "[epoch:1, iter:328] Loss: 2.671 | Acc: 22.369% \n",
      "[epoch:1, iter:329] Loss: 2.668 | Acc: 22.416% \n",
      "[epoch:1, iter:330] Loss: 2.666 | Acc: 22.442% \n",
      "[epoch:1, iter:331] Loss: 2.664 | Acc: 22.447% \n",
      "[epoch:1, iter:332] Loss: 2.662 | Acc: 22.503% \n",
      "[epoch:1, iter:333] Loss: 2.659 | Acc: 22.532% \n",
      "[epoch:1, iter:334] Loss: 2.657 | Acc: 22.569% \n",
      "[epoch:1, iter:335] Loss: 2.655 | Acc: 22.585% \n",
      "[epoch:1, iter:336] Loss: 2.654 | Acc: 22.586% \n",
      "[epoch:1, iter:337] Loss: 2.651 | Acc: 22.608% \n",
      "[epoch:1, iter:338] Loss: 2.649 | Acc: 22.627% \n",
      "[epoch:1, iter:339] Loss: 2.646 | Acc: 22.670% \n",
      "[epoch:1, iter:340] Loss: 2.644 | Acc: 22.706% \n",
      "[epoch:1, iter:341] Loss: 2.641 | Acc: 22.724% \n",
      "[epoch:1, iter:342] Loss: 2.639 | Acc: 22.749% \n",
      "[epoch:1, iter:343] Loss: 2.637 | Acc: 22.778% \n",
      "[epoch:1, iter:344] Loss: 2.635 | Acc: 22.794% \n",
      "[epoch:1, iter:345] Loss: 2.632 | Acc: 22.843% \n",
      "[epoch:1, iter:346] Loss: 2.630 | Acc: 22.867% \n",
      "[epoch:1, iter:347] Loss: 2.628 | Acc: 22.896% \n",
      "[epoch:1, iter:348] Loss: 2.625 | Acc: 22.937% \n",
      "[epoch:1, iter:349] Loss: 2.623 | Acc: 22.963% \n",
      "[epoch:1, iter:350] Loss: 2.620 | Acc: 23.006% \n",
      "[epoch:1, iter:351] Loss: 2.618 | Acc: 23.020% \n",
      "[epoch:1, iter:352] Loss: 2.616 | Acc: 23.057% \n",
      "[epoch:1, iter:353] Loss: 2.614 | Acc: 23.102% \n",
      "[epoch:1, iter:354] Loss: 2.611 | Acc: 23.130% \n",
      "[epoch:1, iter:355] Loss: 2.609 | Acc: 23.141% \n",
      "[epoch:1, iter:356] Loss: 2.608 | Acc: 23.146% \n",
      "[epoch:1, iter:357] Loss: 2.605 | Acc: 23.188% \n",
      "[epoch:1, iter:358] Loss: 2.602 | Acc: 23.229% \n",
      "[epoch:1, iter:359] Loss: 2.600 | Acc: 23.279% \n",
      "[epoch:1, iter:360] Loss: 2.598 | Acc: 23.317% \n",
      "[epoch:1, iter:361] Loss: 2.596 | Acc: 23.324% \n",
      "[epoch:1, iter:362] Loss: 2.594 | Acc: 23.345% \n",
      "[epoch:1, iter:363] Loss: 2.591 | Acc: 23.366% \n",
      "[epoch:1, iter:364] Loss: 2.590 | Acc: 23.385% \n",
      "[epoch:1, iter:365] Loss: 2.588 | Acc: 23.414% \n",
      "[epoch:1, iter:366] Loss: 2.586 | Acc: 23.429% \n",
      "[epoch:1, iter:367] Loss: 2.583 | Acc: 23.458% \n",
      "[epoch:1, iter:368] Loss: 2.581 | Acc: 23.481% \n",
      "[epoch:1, iter:369] Loss: 2.579 | Acc: 23.499% \n",
      "[epoch:1, iter:370] Loss: 2.578 | Acc: 23.516% \n",
      "[epoch:1, iter:371] Loss: 2.575 | Acc: 23.539% \n",
      "[epoch:1, iter:372] Loss: 2.573 | Acc: 23.565% \n",
      "[epoch:1, iter:373] Loss: 2.571 | Acc: 23.606% \n",
      "[epoch:1, iter:374] Loss: 2.569 | Acc: 23.620% \n",
      "[epoch:1, iter:375] Loss: 2.568 | Acc: 23.629% \n",
      "[epoch:1, iter:376] Loss: 2.566 | Acc: 23.652% \n",
      "[epoch:1, iter:377] Loss: 2.564 | Acc: 23.679% \n",
      "[epoch:1, iter:378] Loss: 2.562 | Acc: 23.712% \n",
      "[epoch:1, iter:379] Loss: 2.560 | Acc: 23.726% \n",
      "[epoch:1, iter:380] Loss: 2.559 | Acc: 23.739% \n",
      "[epoch:1, iter:381] Loss: 2.557 | Acc: 23.761% \n",
      "[epoch:1, iter:382] Loss: 2.555 | Acc: 23.785% \n",
      "[epoch:1, iter:383] Loss: 2.553 | Acc: 23.809% \n",
      "[epoch:1, iter:384] Loss: 2.551 | Acc: 23.849% \n",
      "[epoch:1, iter:385] Loss: 2.549 | Acc: 23.862% \n",
      "[epoch:1, iter:386] Loss: 2.548 | Acc: 23.868% \n",
      "[epoch:1, iter:387] Loss: 2.546 | Acc: 23.886% \n",
      "[epoch:1, iter:388] Loss: 2.545 | Acc: 23.907% \n",
      "[epoch:1, iter:389] Loss: 2.543 | Acc: 23.920% \n",
      "[epoch:1, iter:390] Loss: 2.541 | Acc: 23.941% \n",
      "[epoch:1, iter:391] Loss: 2.539 | Acc: 23.946% \n",
      "[epoch:1, iter:392] Loss: 2.537 | Acc: 23.974% \n",
      "[epoch:1, iter:393] Loss: 2.536 | Acc: 23.987% \n",
      "[epoch:1, iter:394] Loss: 2.535 | Acc: 24.005% \n",
      "[epoch:1, iter:395] Loss: 2.533 | Acc: 24.005% \n",
      "[epoch:1, iter:396] Loss: 2.532 | Acc: 24.025% \n",
      "[epoch:1, iter:397] Loss: 2.530 | Acc: 24.060% \n",
      "[epoch:1, iter:398] Loss: 2.528 | Acc: 24.078% \n",
      "[epoch:1, iter:399] Loss: 2.527 | Acc: 24.098% \n",
      "[epoch:1, iter:400] Loss: 2.525 | Acc: 24.112% \n",
      "[epoch:1, iter:401] Loss: 2.523 | Acc: 24.130% \n",
      "[epoch:1, iter:402] Loss: 2.522 | Acc: 24.147% \n",
      "[epoch:1, iter:403] Loss: 2.520 | Acc: 24.174% \n",
      "[epoch:1, iter:404] Loss: 2.518 | Acc: 24.205% \n",
      "[epoch:1, iter:405] Loss: 2.516 | Acc: 24.230% \n",
      "[epoch:1, iter:406] Loss: 2.514 | Acc: 24.251% \n",
      "[epoch:1, iter:407] Loss: 2.512 | Acc: 24.290% \n",
      "[epoch:1, iter:408] Loss: 2.511 | Acc: 24.297% \n",
      "[epoch:1, iter:409] Loss: 2.509 | Acc: 24.330% \n",
      "[epoch:1, iter:410] Loss: 2.507 | Acc: 24.354% \n",
      "[epoch:1, iter:411] Loss: 2.505 | Acc: 24.389% \n",
      "[epoch:1, iter:412] Loss: 2.505 | Acc: 24.383% \n",
      "[epoch:1, iter:413] Loss: 2.503 | Acc: 24.400% \n",
      "[epoch:1, iter:414] Loss: 2.502 | Acc: 24.420% \n",
      "[epoch:1, iter:415] Loss: 2.500 | Acc: 24.436% \n",
      "[epoch:1, iter:416] Loss: 2.498 | Acc: 24.464% \n",
      "[epoch:1, iter:417] Loss: 2.496 | Acc: 24.484% \n",
      "[epoch:1, iter:418] Loss: 2.495 | Acc: 24.498% \n",
      "[epoch:1, iter:419] Loss: 2.493 | Acc: 24.537% \n",
      "[epoch:1, iter:420] Loss: 2.491 | Acc: 24.571% \n",
      "[epoch:1, iter:421] Loss: 2.489 | Acc: 24.591% \n",
      "[epoch:1, iter:422] Loss: 2.488 | Acc: 24.604% \n",
      "[epoch:1, iter:423] Loss: 2.486 | Acc: 24.624% \n",
      "[epoch:1, iter:424] Loss: 2.485 | Acc: 24.651% \n",
      "[epoch:1, iter:425] Loss: 2.484 | Acc: 24.671% \n",
      "[epoch:1, iter:426] Loss: 2.482 | Acc: 24.690% \n",
      "[epoch:1, iter:427] Loss: 2.481 | Acc: 24.707% \n",
      "[epoch:1, iter:428] Loss: 2.479 | Acc: 24.724% \n",
      "[epoch:1, iter:429] Loss: 2.478 | Acc: 24.725% \n",
      "[epoch:1, iter:430] Loss: 2.477 | Acc: 24.735% \n",
      "[epoch:1, iter:431] Loss: 2.475 | Acc: 24.763% \n",
      "[epoch:1, iter:432] Loss: 2.473 | Acc: 24.773% \n",
      "[epoch:1, iter:433] Loss: 2.472 | Acc: 24.801% \n",
      "[epoch:1, iter:434] Loss: 2.470 | Acc: 24.820% \n",
      "[epoch:1, iter:435] Loss: 2.469 | Acc: 24.830% \n",
      "[epoch:1, iter:436] Loss: 2.468 | Acc: 24.849% \n",
      "[epoch:1, iter:437] Loss: 2.466 | Acc: 24.867% \n",
      "[epoch:1, iter:438] Loss: 2.465 | Acc: 24.897% \n",
      "[epoch:1, iter:439] Loss: 2.463 | Acc: 24.925% \n",
      "[epoch:1, iter:440] Loss: 2.462 | Acc: 24.945% \n",
      "[epoch:1, iter:441] Loss: 2.460 | Acc: 24.968% \n",
      "[epoch:1, iter:442] Loss: 2.459 | Acc: 24.966% \n",
      "[epoch:1, iter:443] Loss: 2.458 | Acc: 24.975% \n",
      "[epoch:1, iter:444] Loss: 2.457 | Acc: 24.977% \n",
      "[epoch:1, iter:445] Loss: 2.456 | Acc: 24.982% \n",
      "[epoch:1, iter:446] Loss: 2.455 | Acc: 25.000% \n",
      "[epoch:1, iter:447] Loss: 2.453 | Acc: 25.020% \n",
      "[epoch:1, iter:448] Loss: 2.451 | Acc: 25.054% \n",
      "[epoch:1, iter:449] Loss: 2.450 | Acc: 25.065% \n",
      "[epoch:1, iter:450] Loss: 2.448 | Acc: 25.087% \n",
      "[epoch:1, iter:451] Loss: 2.446 | Acc: 25.126% \n",
      "[epoch:1, iter:452] Loss: 2.445 | Acc: 25.144% \n",
      "[epoch:1, iter:453] Loss: 2.443 | Acc: 25.161% \n",
      "[epoch:1, iter:454] Loss: 2.442 | Acc: 25.172% \n",
      "[epoch:1, iter:455] Loss: 2.441 | Acc: 25.193% \n",
      "[epoch:1, iter:456] Loss: 2.439 | Acc: 25.217% \n",
      "[epoch:1, iter:457] Loss: 2.438 | Acc: 25.230% \n",
      "[epoch:1, iter:458] Loss: 2.437 | Acc: 25.231% \n",
      "[epoch:1, iter:459] Loss: 2.435 | Acc: 25.244% \n",
      "[epoch:1, iter:460] Loss: 2.434 | Acc: 25.250% \n",
      "[epoch:1, iter:461] Loss: 2.433 | Acc: 25.267% \n",
      "[epoch:1, iter:462] Loss: 2.431 | Acc: 25.286% \n",
      "[epoch:1, iter:463] Loss: 2.429 | Acc: 25.320% \n",
      "[epoch:1, iter:464] Loss: 2.428 | Acc: 25.325% \n",
      "[epoch:1, iter:465] Loss: 2.426 | Acc: 25.355% \n",
      "[epoch:1, iter:466] Loss: 2.425 | Acc: 25.365% \n",
      "[epoch:1, iter:467] Loss: 2.423 | Acc: 25.390% \n",
      "[epoch:1, iter:468] Loss: 2.422 | Acc: 25.408% \n",
      "[epoch:1, iter:469] Loss: 2.420 | Acc: 25.429% \n",
      "[epoch:1, iter:470] Loss: 2.419 | Acc: 25.468% \n",
      "[epoch:1, iter:471] Loss: 2.417 | Acc: 25.499% \n",
      "[epoch:1, iter:472] Loss: 2.416 | Acc: 25.506% \n",
      "[epoch:1, iter:473] Loss: 2.415 | Acc: 25.531% \n",
      "[epoch:1, iter:474] Loss: 2.413 | Acc: 25.568% \n",
      "[epoch:1, iter:475] Loss: 2.412 | Acc: 25.589% \n",
      "[epoch:1, iter:476] Loss: 2.411 | Acc: 25.609% \n",
      "[epoch:1, iter:477] Loss: 2.409 | Acc: 25.625% \n",
      "[epoch:1, iter:478] Loss: 2.408 | Acc: 25.646% \n",
      "[epoch:1, iter:479] Loss: 2.406 | Acc: 25.670% \n",
      "[epoch:1, iter:480] Loss: 2.405 | Acc: 25.677% \n",
      "[epoch:1, iter:481] Loss: 2.404 | Acc: 25.699% \n",
      "[epoch:1, iter:482] Loss: 2.402 | Acc: 25.712% \n",
      "[epoch:1, iter:483] Loss: 2.401 | Acc: 25.723% \n",
      "[epoch:1, iter:484] Loss: 2.400 | Acc: 25.727% \n",
      "[epoch:1, iter:485] Loss: 2.399 | Acc: 25.753% \n",
      "[epoch:1, iter:486] Loss: 2.397 | Acc: 25.774% \n",
      "[epoch:1, iter:487] Loss: 2.396 | Acc: 25.791% \n",
      "[epoch:1, iter:488] Loss: 2.395 | Acc: 25.793% \n",
      "[epoch:1, iter:489] Loss: 2.394 | Acc: 25.804% \n",
      "[epoch:1, iter:490] Loss: 2.392 | Acc: 25.841% \n",
      "[epoch:1, iter:491] Loss: 2.391 | Acc: 25.866% \n",
      "[epoch:1, iter:492] Loss: 2.389 | Acc: 25.902% \n",
      "[epoch:1, iter:493] Loss: 2.388 | Acc: 25.921% \n",
      "[epoch:1, iter:494] Loss: 2.387 | Acc: 25.931% \n",
      "[epoch:1, iter:495] Loss: 2.385 | Acc: 25.954% \n",
      "[epoch:1, iter:496] Loss: 2.384 | Acc: 25.972% \n",
      "[epoch:1, iter:497] Loss: 2.383 | Acc: 25.988% \n",
      "[epoch:1, iter:498] Loss: 2.382 | Acc: 25.988% \n",
      "[epoch:1, iter:499] Loss: 2.380 | Acc: 26.016% \n",
      "[epoch:1, iter:500] Loss: 2.379 | Acc: 26.020% \n",
      "Waiting Test...\n",
      "Test's accuracy (before quantization) is: 38.360%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.999))\n",
    "lr_decay_epochs=[20, 40]\n",
    "for decay_epoch in lr_decay_epochs:\n",
    "    if pre_epoch>decay_epoch:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.5\n",
    "\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print('\\nEpoch: %d' % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    if epoch<50:\n",
    "        epsilon=1\n",
    "    else:\n",
    "        epsilon=.88**(epoch-50)\n",
    "\n",
    "    if epoch in lr_decay_epochs:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *=  0.5\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        #prepare dataset\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #forward & backward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"loss\": loss,\n",
    "            }\n",
    "        )\n",
    "        for param_group in optimizer.param_groups:\n",
    "                for idx, p in enumerate(param_group['params']):\n",
    "                    constr = epsilon-(p.data**2-1)**2\n",
    "                    Kx = -4 * (p.data**2 - 1) * p.data\n",
    "                    direct_grad = torch.logical_or(torch.logical_or(constr >= 0, Kx == 0), torch.logical_and(constr < 0, (-Kx * p.grad.data) >= -alpha * constr))\n",
    "                    p.grad.data[direct_grad] = p.grad.data[direct_grad]\n",
    "                    p.grad.data[~direct_grad] = torch.clip(alpha * constr / Kx, -12, 12)[~direct_grad]\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            for name, param in net.named_parameters():\n",
    "                torch.clamp_(param.data,-1.5,1.5)\n",
    "        sum_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% ' \n",
    "              % (epoch + 1, (i + 1 + (epoch) * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "    print('Waiting Test...')\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            net.eval()\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print('Test\\'s accuracy (before quantization) is: %.3f%%' % (100 * correct / total))\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"accuracy\": correct / total,\n",
    "        }\n",
    "    )\n",
    "    model_copy = copy.deepcopy(net)\n",
    "    with torch.no_grad():\n",
    "        for name, param in model_copy.named_parameters():\n",
    "                param.data = torch.sign(param.data)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            model_copy.eval()\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_copy(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print('Test\\'s accuracy (after quantization) is: %.3f%%' % (100 * correct / total))\n",
    "        report_quantized_loss = sum_loss/len(testloader)\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"quantized_accuracy\": correct/total,\n",
    "        }\n",
    "    )\n",
    "    if epoch + 1 == 50:\n",
    "        FILE = 'resnet18_original.pt'\n",
    "        torch.save({'model_state_dict': net.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'resnet18_qt.pt'\n",
    "\n",
    "torch.save({'model_state_dict': net.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18648\\757283092.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test's ac is: 81.050%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18648\\757283092.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test's ac is: 10.000%\n"
     ]
    }
   ],
   "source": [
    "FILE = 'resnet18_original.pt'\n",
    "checkpoint = torch.load(FILE)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        net.eval()\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('Test\\'s ac is: %.3f%%' % (100 * correct / total))\n",
    "\n",
    "FILE = 'resnet18_qt.pt'\n",
    "checkpoint = torch.load(FILE)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_copy = copy.deepcopy(net)\n",
    "with torch.no_grad():\n",
    "    for name, param in model_copy.named_parameters():\n",
    "        param.data=torch.sign(param.data)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        model_copy.eval()\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_copy(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('Test\\'s ac is: %.3f%%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
