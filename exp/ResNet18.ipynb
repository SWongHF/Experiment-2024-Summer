{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)        \n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)        \n",
    "        self.fc = nn.Linear(512, num_classes, bias = False)\n",
    "        \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Use the ResNet18 on Cifar-10\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "#check gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set hyperparameter\n",
    "EPOCH = 20\n",
    "pre_epoch = 0\n",
    "BATCH_SIZE = 100\n",
    "LR = 0.1\n",
    "\n",
    "#prepare dataset and preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(40),\n",
    "    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "    ratio=(1.0, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=3)\n",
    "\n",
    "#labels in CIFAR10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# #define ResNet18\n",
    "net = ResNet18().to(device)\n",
    "\n",
    "# #define loss funtion & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train\n",
    "# lr_decay_epochs=[20, 40]\n",
    "\n",
    "# for epoch in range(pre_epoch, EPOCH+20):\n",
    "#     print('\\nEpoch: %d' % (epoch + 1))\n",
    "#     net.train()\n",
    "#     sum_loss = 0.0\n",
    "#     correct = 0.0\n",
    "#     total = 0.0\n",
    "#     if epoch in lr_decay_epochs:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] *=  0.5\n",
    "\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         #prepare dataset\n",
    "#         length = len(trainloader)\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         #forward & backward\n",
    "#         outputs = net(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         #print ac & loss in each batch\n",
    "#         sum_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += predicted.eq(labels.data).cpu().sum()\n",
    "#         print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% ' \n",
    "#               % (epoch + 1, (i + 1 + (epoch) * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "        \n",
    "#     #get the ac with testdataset in each epoch\n",
    "#     print('Waiting Test...')\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for data in testloader:\n",
    "#             net.eval()\n",
    "#             images, labels = data\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = net(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum()\n",
    "#         print('Test\\'s ac is: %.3f%%' % (100 * correct / total))\n",
    "\n",
    "# print('Train has finished, total epoch is %d' % (EPOCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'resnet18.pt'\n",
    "\n",
    "# net = ResNet18().to(device)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# checkpoint = torch.load(FILE)\n",
    "# net.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# torch.save({'model_state_dict': net.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 21\n",
      "[epoch:21, iter:10001] Loss: 1.125 | Acc: 56.000% \n",
      "[epoch:21, iter:10002] Loss: 1.156 | Acc: 57.500% \n",
      "[epoch:21, iter:10003] Loss: 1.084 | Acc: 59.000% \n",
      "[epoch:21, iter:10004] Loss: 1.074 | Acc: 58.750% \n",
      "[epoch:21, iter:10005] Loss: 1.033 | Acc: 61.000% \n",
      "[epoch:21, iter:10006] Loss: 1.043 | Acc: 61.667% \n",
      "[epoch:21, iter:10007] Loss: 1.024 | Acc: 62.000% \n",
      "[epoch:21, iter:10008] Loss: 1.061 | Acc: 61.500% \n",
      "[epoch:21, iter:10009] Loss: 1.057 | Acc: 62.111% \n",
      "[epoch:21, iter:10010] Loss: 1.056 | Acc: 62.300% \n",
      "[epoch:21, iter:10011] Loss: 1.056 | Acc: 61.818% \n",
      "[epoch:21, iter:10012] Loss: 1.056 | Acc: 61.750% \n",
      "[epoch:21, iter:10013] Loss: 1.053 | Acc: 62.000% \n",
      "[epoch:21, iter:10014] Loss: 1.051 | Acc: 61.786% \n",
      "[epoch:21, iter:10015] Loss: 1.052 | Acc: 61.867% \n",
      "[epoch:21, iter:10016] Loss: 1.050 | Acc: 62.250% \n",
      "[epoch:21, iter:10017] Loss: 1.045 | Acc: 62.000% \n",
      "[epoch:21, iter:10018] Loss: 1.050 | Acc: 62.278% \n",
      "[epoch:21, iter:10019] Loss: 1.046 | Acc: 62.421% \n",
      "[epoch:21, iter:10020] Loss: 1.051 | Acc: 62.300% \n",
      "[epoch:21, iter:10021] Loss: 1.046 | Acc: 62.571% \n",
      "[epoch:21, iter:10022] Loss: 1.037 | Acc: 63.000% \n",
      "[epoch:21, iter:10023] Loss: 1.044 | Acc: 63.043% \n",
      "[epoch:21, iter:10024] Loss: 1.042 | Acc: 62.958% \n",
      "[epoch:21, iter:10025] Loss: 1.041 | Acc: 63.000% \n",
      "[epoch:21, iter:10026] Loss: 1.034 | Acc: 63.385% \n",
      "[epoch:21, iter:10027] Loss: 1.033 | Acc: 63.593% \n",
      "[epoch:21, iter:10028] Loss: 1.040 | Acc: 63.179% \n",
      "[epoch:21, iter:10029] Loss: 1.038 | Acc: 63.379% \n",
      "[epoch:21, iter:10030] Loss: 1.042 | Acc: 63.300% \n",
      "[epoch:21, iter:10031] Loss: 1.037 | Acc: 63.387% \n",
      "[epoch:21, iter:10032] Loss: 1.038 | Acc: 63.312% \n",
      "[epoch:21, iter:10033] Loss: 1.033 | Acc: 63.242% \n",
      "[epoch:21, iter:10034] Loss: 1.033 | Acc: 63.324% \n",
      "[epoch:21, iter:10035] Loss: 1.031 | Acc: 63.171% \n",
      "[epoch:21, iter:10036] Loss: 1.030 | Acc: 63.222% \n",
      "[epoch:21, iter:10037] Loss: 1.027 | Acc: 63.459% \n",
      "[epoch:21, iter:10038] Loss: 1.030 | Acc: 63.395% \n",
      "[epoch:21, iter:10039] Loss: 1.029 | Acc: 63.487% \n",
      "[epoch:21, iter:10040] Loss: 1.026 | Acc: 63.725% \n",
      "[epoch:21, iter:10041] Loss: 1.023 | Acc: 63.756% \n",
      "[epoch:21, iter:10042] Loss: 1.021 | Acc: 63.929% \n",
      "[epoch:21, iter:10043] Loss: 1.023 | Acc: 63.814% \n",
      "[epoch:21, iter:10044] Loss: 1.019 | Acc: 63.886% \n",
      "[epoch:21, iter:10045] Loss: 1.022 | Acc: 63.778% \n",
      "[epoch:21, iter:10046] Loss: 1.023 | Acc: 63.761% \n",
      "[epoch:21, iter:10047] Loss: 1.022 | Acc: 63.766% \n",
      "[epoch:21, iter:10048] Loss: 1.020 | Acc: 63.708% \n",
      "[epoch:21, iter:10049] Loss: 1.017 | Acc: 63.776% \n",
      "[epoch:21, iter:10050] Loss: 1.016 | Acc: 63.780% \n",
      "[epoch:21, iter:10051] Loss: 1.012 | Acc: 63.863% \n",
      "[epoch:21, iter:10052] Loss: 1.013 | Acc: 63.885% \n",
      "[epoch:21, iter:10053] Loss: 1.010 | Acc: 64.038% \n",
      "[epoch:21, iter:10054] Loss: 1.011 | Acc: 64.074% \n",
      "[epoch:21, iter:10055] Loss: 1.007 | Acc: 64.273% \n",
      "[epoch:21, iter:10056] Loss: 1.005 | Acc: 64.339% \n",
      "[epoch:21, iter:10057] Loss: 1.009 | Acc: 64.193% \n",
      "[epoch:21, iter:10058] Loss: 1.010 | Acc: 64.138% \n",
      "[epoch:21, iter:10059] Loss: 1.008 | Acc: 64.169% \n",
      "[epoch:21, iter:10060] Loss: 1.006 | Acc: 64.233% \n",
      "[epoch:21, iter:10061] Loss: 1.006 | Acc: 64.295% \n",
      "[epoch:21, iter:10062] Loss: 1.004 | Acc: 64.371% \n",
      "[epoch:21, iter:10063] Loss: 1.005 | Acc: 64.254% \n",
      "[epoch:21, iter:10064] Loss: 1.005 | Acc: 64.281% \n",
      "[epoch:21, iter:10065] Loss: 1.009 | Acc: 64.062% \n",
      "[epoch:21, iter:10066] Loss: 1.011 | Acc: 63.939% \n",
      "[epoch:21, iter:10067] Loss: 1.013 | Acc: 63.806% \n",
      "[epoch:21, iter:10068] Loss: 1.013 | Acc: 63.721% \n",
      "[epoch:21, iter:10069] Loss: 1.014 | Acc: 63.768% \n",
      "[epoch:21, iter:10070] Loss: 1.012 | Acc: 63.857% \n",
      "[epoch:21, iter:10071] Loss: 1.012 | Acc: 63.831% \n",
      "[epoch:21, iter:10072] Loss: 1.009 | Acc: 64.014% \n",
      "[epoch:21, iter:10073] Loss: 1.008 | Acc: 64.014% \n",
      "[epoch:21, iter:10074] Loss: 1.006 | Acc: 64.041% \n",
      "[epoch:21, iter:10075] Loss: 1.006 | Acc: 64.000% \n",
      "[epoch:21, iter:10076] Loss: 1.007 | Acc: 63.921% \n",
      "[epoch:21, iter:10077] Loss: 1.007 | Acc: 63.909% \n",
      "[epoch:21, iter:10078] Loss: 1.008 | Acc: 63.885% \n",
      "[epoch:21, iter:10079] Loss: 1.010 | Acc: 63.785% \n",
      "[epoch:21, iter:10080] Loss: 1.009 | Acc: 63.763% \n",
      "[epoch:21, iter:10081] Loss: 1.008 | Acc: 63.827% \n",
      "[epoch:21, iter:10082] Loss: 1.005 | Acc: 63.927% \n",
      "[epoch:21, iter:10083] Loss: 1.004 | Acc: 63.988% \n",
      "[epoch:21, iter:10084] Loss: 1.004 | Acc: 64.036% \n",
      "[epoch:21, iter:10085] Loss: 1.003 | Acc: 64.082% \n",
      "[epoch:21, iter:10086] Loss: 1.003 | Acc: 64.047% \n",
      "[epoch:21, iter:10087] Loss: 1.001 | Acc: 64.115% \n",
      "[epoch:21, iter:10088] Loss: 0.999 | Acc: 64.136% \n",
      "[epoch:21, iter:10089] Loss: 0.999 | Acc: 64.180% \n",
      "[epoch:21, iter:10090] Loss: 0.999 | Acc: 64.144% \n",
      "[epoch:21, iter:10091] Loss: 1.000 | Acc: 64.099% \n",
      "[epoch:21, iter:10092] Loss: 1.000 | Acc: 64.141% \n",
      "[epoch:21, iter:10093] Loss: 1.000 | Acc: 64.140% \n",
      "[epoch:21, iter:10094] Loss: 1.002 | Acc: 64.106% \n",
      "[epoch:21, iter:10095] Loss: 1.002 | Acc: 64.116% \n",
      "[epoch:21, iter:10096] Loss: 1.001 | Acc: 64.146% \n",
      "[epoch:21, iter:10097] Loss: 1.002 | Acc: 64.144% \n",
      "[epoch:21, iter:10098] Loss: 1.002 | Acc: 64.133% \n",
      "[epoch:21, iter:10099] Loss: 1.000 | Acc: 64.182% \n",
      "[epoch:21, iter:10100] Loss: 0.999 | Acc: 64.220% \n",
      "[epoch:21, iter:10101] Loss: 0.999 | Acc: 64.228% \n",
      "[epoch:21, iter:10102] Loss: 0.999 | Acc: 64.216% \n",
      "[epoch:21, iter:10103] Loss: 0.998 | Acc: 64.311% \n",
      "[epoch:21, iter:10104] Loss: 0.999 | Acc: 64.212% \n",
      "[epoch:21, iter:10105] Loss: 0.998 | Acc: 64.238% \n",
      "[epoch:21, iter:10106] Loss: 0.997 | Acc: 64.255% \n",
      "[epoch:21, iter:10107] Loss: 0.997 | Acc: 64.243% \n",
      "[epoch:21, iter:10108] Loss: 0.997 | Acc: 64.259% \n",
      "[epoch:21, iter:10109] Loss: 0.998 | Acc: 64.257% \n",
      "[epoch:21, iter:10110] Loss: 0.998 | Acc: 64.218% \n",
      "[epoch:21, iter:10111] Loss: 0.999 | Acc: 64.207% \n",
      "[epoch:21, iter:10112] Loss: 0.998 | Acc: 64.179% \n",
      "[epoch:21, iter:10113] Loss: 0.999 | Acc: 64.150% \n",
      "[epoch:21, iter:10114] Loss: 0.997 | Acc: 64.228% \n",
      "[epoch:21, iter:10115] Loss: 0.997 | Acc: 64.243% \n",
      "[epoch:21, iter:10116] Loss: 0.997 | Acc: 64.233% \n",
      "[epoch:21, iter:10117] Loss: 0.996 | Acc: 64.291% \n",
      "[epoch:21, iter:10118] Loss: 0.994 | Acc: 64.373% \n",
      "[epoch:21, iter:10119] Loss: 0.994 | Acc: 64.412% \n",
      "[epoch:21, iter:10120] Loss: 0.994 | Acc: 64.392% \n",
      "[epoch:21, iter:10121] Loss: 0.993 | Acc: 64.421% \n",
      "[epoch:21, iter:10122] Loss: 0.994 | Acc: 64.426% \n",
      "[epoch:21, iter:10123] Loss: 0.994 | Acc: 64.439% \n",
      "[epoch:21, iter:10124] Loss: 0.994 | Acc: 64.419% \n",
      "[epoch:21, iter:10125] Loss: 0.994 | Acc: 64.400% \n",
      "[epoch:21, iter:10126] Loss: 0.996 | Acc: 64.373% \n",
      "[epoch:21, iter:10127] Loss: 0.998 | Acc: 64.315% \n",
      "[epoch:21, iter:10128] Loss: 0.999 | Acc: 64.266% \n",
      "[epoch:21, iter:10129] Loss: 0.999 | Acc: 64.279% \n",
      "[epoch:21, iter:10130] Loss: 0.999 | Acc: 64.254% \n",
      "[epoch:21, iter:10131] Loss: 0.999 | Acc: 64.214% \n",
      "[epoch:21, iter:10132] Loss: 0.998 | Acc: 64.273% \n",
      "[epoch:21, iter:10133] Loss: 0.998 | Acc: 64.301% \n",
      "[epoch:21, iter:10134] Loss: 0.997 | Acc: 64.328% \n",
      "[epoch:21, iter:10135] Loss: 0.996 | Acc: 64.422% \n",
      "[epoch:21, iter:10136] Loss: 0.995 | Acc: 64.426% \n",
      "[epoch:21, iter:10137] Loss: 0.995 | Acc: 64.445% \n",
      "[epoch:21, iter:10138] Loss: 0.994 | Acc: 64.493% \n",
      "[epoch:21, iter:10139] Loss: 0.995 | Acc: 64.482% \n",
      "[epoch:21, iter:10140] Loss: 0.994 | Acc: 64.521% \n",
      "[epoch:21, iter:10141] Loss: 0.994 | Acc: 64.525% \n",
      "[epoch:21, iter:10142] Loss: 0.994 | Acc: 64.535% \n",
      "[epoch:21, iter:10143] Loss: 0.993 | Acc: 64.552% \n",
      "[epoch:21, iter:10144] Loss: 0.993 | Acc: 64.562% \n",
      "[epoch:21, iter:10145] Loss: 0.994 | Acc: 64.538% \n",
      "[epoch:21, iter:10146] Loss: 0.994 | Acc: 64.500% \n",
      "[epoch:21, iter:10147] Loss: 0.994 | Acc: 64.537% \n",
      "[epoch:21, iter:10148] Loss: 0.993 | Acc: 64.574% \n",
      "[epoch:21, iter:10149] Loss: 0.993 | Acc: 64.564% \n",
      "[epoch:21, iter:10150] Loss: 0.992 | Acc: 64.567% \n",
      "[epoch:21, iter:10151] Loss: 0.992 | Acc: 64.596% \n",
      "[epoch:21, iter:10152] Loss: 0.993 | Acc: 64.566% \n",
      "[epoch:21, iter:10153] Loss: 0.993 | Acc: 64.562% \n",
      "[epoch:21, iter:10154] Loss: 0.992 | Acc: 64.617% \n",
      "[epoch:21, iter:10155] Loss: 0.992 | Acc: 64.606% \n",
      "[epoch:21, iter:10156] Loss: 0.991 | Acc: 64.596% \n",
      "[epoch:21, iter:10157] Loss: 0.992 | Acc: 64.573% \n",
      "[epoch:21, iter:10158] Loss: 0.992 | Acc: 64.601% \n",
      "[epoch:21, iter:10159] Loss: 0.990 | Acc: 64.667% \n",
      "[epoch:21, iter:10160] Loss: 0.990 | Acc: 64.644% \n",
      "[epoch:21, iter:10161] Loss: 0.990 | Acc: 64.646% \n",
      "[epoch:21, iter:10162] Loss: 0.989 | Acc: 64.691% \n",
      "[epoch:21, iter:10163] Loss: 0.989 | Acc: 64.681% \n",
      "[epoch:21, iter:10164] Loss: 0.989 | Acc: 64.683% \n",
      "[epoch:21, iter:10165] Loss: 0.989 | Acc: 64.685% \n",
      "[epoch:21, iter:10166] Loss: 0.989 | Acc: 64.687% \n",
      "[epoch:21, iter:10167] Loss: 0.989 | Acc: 64.701% \n",
      "[epoch:21, iter:10168] Loss: 0.989 | Acc: 64.732% \n",
      "[epoch:21, iter:10169] Loss: 0.987 | Acc: 64.787% \n",
      "[epoch:21, iter:10170] Loss: 0.987 | Acc: 64.818% \n",
      "[epoch:21, iter:10171] Loss: 0.987 | Acc: 64.825% \n",
      "[epoch:21, iter:10172] Loss: 0.987 | Acc: 64.802% \n",
      "[epoch:21, iter:10173] Loss: 0.987 | Acc: 64.792% \n",
      "[epoch:21, iter:10174] Loss: 0.987 | Acc: 64.782% \n",
      "[epoch:21, iter:10175] Loss: 0.987 | Acc: 64.766% \n",
      "[epoch:21, iter:10176] Loss: 0.987 | Acc: 64.790% \n",
      "[epoch:21, iter:10177] Loss: 0.988 | Acc: 64.768% \n",
      "[epoch:21, iter:10178] Loss: 0.987 | Acc: 64.770% \n",
      "[epoch:21, iter:10179] Loss: 0.988 | Acc: 64.765% \n",
      "[epoch:21, iter:10180] Loss: 0.989 | Acc: 64.739% \n",
      "[epoch:21, iter:10181] Loss: 0.990 | Acc: 64.696% \n",
      "[epoch:21, iter:10182] Loss: 0.989 | Acc: 64.681% \n",
      "[epoch:21, iter:10183] Loss: 0.989 | Acc: 64.667% \n",
      "[epoch:21, iter:10184] Loss: 0.989 | Acc: 64.679% \n",
      "[epoch:21, iter:10185] Loss: 0.988 | Acc: 64.697% \n",
      "[epoch:21, iter:10186] Loss: 0.987 | Acc: 64.753% \n",
      "[epoch:21, iter:10187] Loss: 0.986 | Acc: 64.775% \n",
      "[epoch:21, iter:10188] Loss: 0.986 | Acc: 64.766% \n",
      "[epoch:21, iter:10189] Loss: 0.987 | Acc: 64.757% \n",
      "[epoch:21, iter:10190] Loss: 0.987 | Acc: 64.742% \n",
      "[epoch:21, iter:10191] Loss: 0.987 | Acc: 64.749% \n",
      "[epoch:21, iter:10192] Loss: 0.986 | Acc: 64.776% \n",
      "[epoch:21, iter:10193] Loss: 0.986 | Acc: 64.772% \n",
      "[epoch:21, iter:10194] Loss: 0.987 | Acc: 64.747% \n",
      "[epoch:21, iter:10195] Loss: 0.988 | Acc: 64.708% \n",
      "[epoch:21, iter:10196] Loss: 0.988 | Acc: 64.704% \n",
      "[epoch:21, iter:10197] Loss: 0.988 | Acc: 64.660% \n",
      "[epoch:21, iter:10198] Loss: 0.987 | Acc: 64.727% \n",
      "[epoch:21, iter:10199] Loss: 0.987 | Acc: 64.739% \n",
      "[epoch:21, iter:10200] Loss: 0.990 | Acc: 64.685% \n",
      "[epoch:21, iter:10201] Loss: 0.990 | Acc: 64.687% \n",
      "[epoch:21, iter:10202] Loss: 0.990 | Acc: 64.713% \n",
      "[epoch:21, iter:10203] Loss: 0.989 | Acc: 64.744% \n",
      "[epoch:21, iter:10204] Loss: 0.989 | Acc: 64.730% \n",
      "[epoch:21, iter:10205] Loss: 0.989 | Acc: 64.741% \n",
      "[epoch:21, iter:10206] Loss: 0.988 | Acc: 64.762% \n",
      "[epoch:21, iter:10207] Loss: 0.989 | Acc: 64.749% \n",
      "[epoch:21, iter:10208] Loss: 0.989 | Acc: 64.750% \n",
      "[epoch:21, iter:10209] Loss: 0.989 | Acc: 64.761% \n",
      "[epoch:21, iter:10210] Loss: 0.989 | Acc: 64.752% \n",
      "[epoch:21, iter:10211] Loss: 0.988 | Acc: 64.768% \n",
      "[epoch:21, iter:10212] Loss: 0.988 | Acc: 64.774% \n",
      "[epoch:21, iter:10213] Loss: 0.988 | Acc: 64.784% \n",
      "[epoch:21, iter:10214] Loss: 0.988 | Acc: 64.794% \n",
      "[epoch:21, iter:10215] Loss: 0.988 | Acc: 64.795% \n",
      "[epoch:21, iter:10216] Loss: 0.988 | Acc: 64.778% \n",
      "[epoch:21, iter:10217] Loss: 0.987 | Acc: 64.811% \n",
      "[epoch:21, iter:10218] Loss: 0.986 | Acc: 64.872% \n",
      "[epoch:21, iter:10219] Loss: 0.986 | Acc: 64.877% \n",
      "[epoch:21, iter:10220] Loss: 0.986 | Acc: 64.886% \n",
      "[epoch:21, iter:10221] Loss: 0.985 | Acc: 64.896% \n",
      "[epoch:21, iter:10222] Loss: 0.985 | Acc: 64.887% \n",
      "[epoch:21, iter:10223] Loss: 0.984 | Acc: 64.888% \n",
      "[epoch:21, iter:10224] Loss: 0.984 | Acc: 64.875% \n",
      "[epoch:21, iter:10225] Loss: 0.984 | Acc: 64.907% \n",
      "[epoch:21, iter:10226] Loss: 0.984 | Acc: 64.889% \n",
      "[epoch:21, iter:10227] Loss: 0.984 | Acc: 64.859% \n",
      "[epoch:21, iter:10228] Loss: 0.985 | Acc: 64.868% \n",
      "[epoch:21, iter:10229] Loss: 0.983 | Acc: 64.908% \n",
      "[epoch:21, iter:10230] Loss: 0.983 | Acc: 64.900% \n",
      "[epoch:21, iter:10231] Loss: 0.983 | Acc: 64.913% \n",
      "[epoch:21, iter:10232] Loss: 0.983 | Acc: 64.892% \n",
      "[epoch:21, iter:10233] Loss: 0.984 | Acc: 64.871% \n",
      "[epoch:21, iter:10234] Loss: 0.984 | Acc: 64.876% \n",
      "[epoch:21, iter:10235] Loss: 0.984 | Acc: 64.885% \n",
      "[epoch:21, iter:10236] Loss: 0.984 | Acc: 64.852% \n",
      "[epoch:21, iter:10237] Loss: 0.985 | Acc: 64.835% \n",
      "[epoch:21, iter:10238] Loss: 0.984 | Acc: 64.832% \n",
      "[epoch:21, iter:10239] Loss: 0.984 | Acc: 64.833% \n",
      "[epoch:21, iter:10240] Loss: 0.985 | Acc: 64.808% \n",
      "[epoch:21, iter:10241] Loss: 0.985 | Acc: 64.813% \n",
      "[epoch:21, iter:10242] Loss: 0.984 | Acc: 64.843% \n",
      "[epoch:21, iter:10243] Loss: 0.984 | Acc: 64.844% \n",
      "[epoch:21, iter:10244] Loss: 0.984 | Acc: 64.857% \n",
      "[epoch:21, iter:10245] Loss: 0.983 | Acc: 64.853% \n",
      "[epoch:21, iter:10246] Loss: 0.983 | Acc: 64.870% \n",
      "[epoch:21, iter:10247] Loss: 0.983 | Acc: 64.870% \n",
      "[epoch:21, iter:10248] Loss: 0.982 | Acc: 64.891% \n",
      "[epoch:21, iter:10249] Loss: 0.983 | Acc: 64.827% \n",
      "[epoch:21, iter:10250] Loss: 0.983 | Acc: 64.836% \n",
      "[epoch:21, iter:10251] Loss: 0.983 | Acc: 64.845% \n",
      "[epoch:21, iter:10252] Loss: 0.983 | Acc: 64.821% \n",
      "[epoch:21, iter:10253] Loss: 0.983 | Acc: 64.818% \n",
      "[epoch:21, iter:10254] Loss: 0.983 | Acc: 64.835% \n",
      "[epoch:21, iter:10255] Loss: 0.983 | Acc: 64.831% \n",
      "[epoch:21, iter:10256] Loss: 0.982 | Acc: 64.859% \n",
      "[epoch:21, iter:10257] Loss: 0.982 | Acc: 64.860% \n",
      "[epoch:21, iter:10258] Loss: 0.982 | Acc: 64.864% \n",
      "[epoch:21, iter:10259] Loss: 0.982 | Acc: 64.853% \n",
      "[epoch:21, iter:10260] Loss: 0.983 | Acc: 64.862% \n",
      "[epoch:21, iter:10261] Loss: 0.984 | Acc: 64.835% \n",
      "[epoch:21, iter:10262] Loss: 0.983 | Acc: 64.859% \n",
      "[epoch:21, iter:10263] Loss: 0.983 | Acc: 64.863% \n",
      "[epoch:21, iter:10264] Loss: 0.982 | Acc: 64.875% \n",
      "[epoch:21, iter:10265] Loss: 0.982 | Acc: 64.857% \n",
      "[epoch:21, iter:10266] Loss: 0.983 | Acc: 64.853% \n",
      "[epoch:21, iter:10267] Loss: 0.982 | Acc: 64.850% \n",
      "[epoch:21, iter:10268] Loss: 0.982 | Acc: 64.851% \n",
      "[epoch:21, iter:10269] Loss: 0.981 | Acc: 64.848% \n",
      "[epoch:21, iter:10270] Loss: 0.981 | Acc: 64.863% \n",
      "[epoch:21, iter:10271] Loss: 0.981 | Acc: 64.871% \n",
      "[epoch:21, iter:10272] Loss: 0.980 | Acc: 64.886% \n",
      "[epoch:21, iter:10273] Loss: 0.981 | Acc: 64.857% \n",
      "[epoch:21, iter:10274] Loss: 0.981 | Acc: 64.865% \n",
      "[epoch:21, iter:10275] Loss: 0.981 | Acc: 64.858% \n",
      "[epoch:21, iter:10276] Loss: 0.981 | Acc: 64.851% \n",
      "[epoch:21, iter:10277] Loss: 0.981 | Acc: 64.852% \n",
      "[epoch:21, iter:10278] Loss: 0.981 | Acc: 64.849% \n",
      "[epoch:21, iter:10279] Loss: 0.980 | Acc: 64.864% \n",
      "[epoch:21, iter:10280] Loss: 0.980 | Acc: 64.871% \n",
      "[epoch:21, iter:10281] Loss: 0.980 | Acc: 64.890% \n",
      "[epoch:21, iter:10282] Loss: 0.980 | Acc: 64.869% \n",
      "[epoch:21, iter:10283] Loss: 0.980 | Acc: 64.866% \n",
      "[epoch:21, iter:10284] Loss: 0.980 | Acc: 64.877% \n",
      "[epoch:21, iter:10285] Loss: 0.979 | Acc: 64.870% \n",
      "[epoch:21, iter:10286] Loss: 0.979 | Acc: 64.871% \n",
      "[epoch:21, iter:10287] Loss: 0.979 | Acc: 64.885% \n",
      "[epoch:21, iter:10288] Loss: 0.979 | Acc: 64.885% \n",
      "[epoch:21, iter:10289] Loss: 0.979 | Acc: 64.910% \n",
      "[epoch:21, iter:10290] Loss: 0.978 | Acc: 64.931% \n",
      "[epoch:21, iter:10291] Loss: 0.979 | Acc: 64.928% \n",
      "[epoch:21, iter:10292] Loss: 0.979 | Acc: 64.925% \n",
      "[epoch:21, iter:10293] Loss: 0.979 | Acc: 64.932% \n",
      "[epoch:21, iter:10294] Loss: 0.979 | Acc: 64.942% \n",
      "[epoch:21, iter:10295] Loss: 0.978 | Acc: 64.966% \n",
      "[epoch:21, iter:10296] Loss: 0.978 | Acc: 64.963% \n",
      "[epoch:21, iter:10297] Loss: 0.978 | Acc: 64.963% \n",
      "[epoch:21, iter:10298] Loss: 0.977 | Acc: 64.966% \n",
      "[epoch:21, iter:10299] Loss: 0.977 | Acc: 64.963% \n",
      "[epoch:21, iter:10300] Loss: 0.977 | Acc: 64.973% \n",
      "[epoch:21, iter:10301] Loss: 0.977 | Acc: 64.990% \n",
      "[epoch:21, iter:10302] Loss: 0.977 | Acc: 65.000% \n",
      "[epoch:21, iter:10303] Loss: 0.977 | Acc: 64.997% \n",
      "[epoch:21, iter:10304] Loss: 0.977 | Acc: 65.007% \n",
      "[epoch:21, iter:10305] Loss: 0.977 | Acc: 64.987% \n",
      "[epoch:21, iter:10306] Loss: 0.977 | Acc: 64.993% \n",
      "[epoch:21, iter:10307] Loss: 0.977 | Acc: 64.997% \n",
      "[epoch:21, iter:10308] Loss: 0.977 | Acc: 64.994% \n",
      "[epoch:21, iter:10309] Loss: 0.977 | Acc: 64.987% \n",
      "[epoch:21, iter:10310] Loss: 0.977 | Acc: 65.010% \n",
      "[epoch:21, iter:10311] Loss: 0.976 | Acc: 65.035% \n",
      "[epoch:21, iter:10312] Loss: 0.977 | Acc: 65.010% \n",
      "[epoch:21, iter:10313] Loss: 0.977 | Acc: 65.022% \n",
      "[epoch:21, iter:10314] Loss: 0.977 | Acc: 65.045% \n",
      "[epoch:21, iter:10315] Loss: 0.977 | Acc: 65.032% \n",
      "[epoch:21, iter:10316] Loss: 0.976 | Acc: 65.038% \n",
      "[epoch:21, iter:10317] Loss: 0.976 | Acc: 65.038% \n",
      "[epoch:21, iter:10318] Loss: 0.976 | Acc: 65.047% \n",
      "[epoch:21, iter:10319] Loss: 0.976 | Acc: 65.053% \n",
      "[epoch:21, iter:10320] Loss: 0.976 | Acc: 65.044% \n",
      "[epoch:21, iter:10321] Loss: 0.976 | Acc: 65.028% \n",
      "[epoch:21, iter:10322] Loss: 0.976 | Acc: 65.034% \n",
      "[epoch:21, iter:10323] Loss: 0.976 | Acc: 65.056% \n",
      "[epoch:21, iter:10324] Loss: 0.976 | Acc: 65.043% \n",
      "[epoch:21, iter:10325] Loss: 0.976 | Acc: 65.034% \n",
      "[epoch:21, iter:10326] Loss: 0.976 | Acc: 65.034% \n",
      "[epoch:21, iter:10327] Loss: 0.976 | Acc: 65.034% \n",
      "[epoch:21, iter:10328] Loss: 0.976 | Acc: 65.037% \n",
      "[epoch:21, iter:10329] Loss: 0.976 | Acc: 65.030% \n",
      "[epoch:21, iter:10330] Loss: 0.976 | Acc: 65.045% \n",
      "[epoch:21, iter:10331] Loss: 0.975 | Acc: 65.060% \n",
      "[epoch:21, iter:10332] Loss: 0.975 | Acc: 65.066% \n",
      "[epoch:21, iter:10333] Loss: 0.975 | Acc: 65.069% \n",
      "[epoch:21, iter:10334] Loss: 0.975 | Acc: 65.072% \n",
      "[epoch:21, iter:10335] Loss: 0.975 | Acc: 65.075% \n",
      "[epoch:21, iter:10336] Loss: 0.974 | Acc: 65.077% \n",
      "[epoch:21, iter:10337] Loss: 0.974 | Acc: 65.095% \n",
      "[epoch:21, iter:10338] Loss: 0.974 | Acc: 65.101% \n",
      "[epoch:21, iter:10339] Loss: 0.974 | Acc: 65.121% \n",
      "[epoch:21, iter:10340] Loss: 0.974 | Acc: 65.118% \n",
      "[epoch:21, iter:10341] Loss: 0.974 | Acc: 65.103% \n",
      "[epoch:21, iter:10342] Loss: 0.974 | Acc: 65.111% \n",
      "[epoch:21, iter:10343] Loss: 0.974 | Acc: 65.111% \n",
      "[epoch:21, iter:10344] Loss: 0.974 | Acc: 65.116% \n",
      "[epoch:21, iter:10345] Loss: 0.974 | Acc: 65.139% \n",
      "[epoch:21, iter:10346] Loss: 0.974 | Acc: 65.142% \n",
      "[epoch:21, iter:10347] Loss: 0.974 | Acc: 65.141% \n",
      "[epoch:21, iter:10348] Loss: 0.974 | Acc: 65.135% \n",
      "[epoch:21, iter:10349] Loss: 0.974 | Acc: 65.112% \n",
      "[epoch:21, iter:10350] Loss: 0.974 | Acc: 65.103% \n",
      "[epoch:21, iter:10351] Loss: 0.974 | Acc: 65.114% \n",
      "[epoch:21, iter:10352] Loss: 0.974 | Acc: 65.105% \n",
      "[epoch:21, iter:10353] Loss: 0.974 | Acc: 65.116% \n",
      "[epoch:21, iter:10354] Loss: 0.974 | Acc: 65.110% \n",
      "[epoch:21, iter:10355] Loss: 0.973 | Acc: 65.118% \n",
      "[epoch:21, iter:10356] Loss: 0.973 | Acc: 65.126% \n",
      "[epoch:21, iter:10357] Loss: 0.973 | Acc: 65.123% \n",
      "[epoch:21, iter:10358] Loss: 0.973 | Acc: 65.123% \n",
      "[epoch:21, iter:10359] Loss: 0.973 | Acc: 65.162% \n",
      "[epoch:21, iter:10360] Loss: 0.972 | Acc: 65.169% \n",
      "[epoch:21, iter:10361] Loss: 0.973 | Acc: 65.158% \n",
      "[epoch:21, iter:10362] Loss: 0.973 | Acc: 65.141% \n",
      "[epoch:21, iter:10363] Loss: 0.973 | Acc: 65.132% \n",
      "[epoch:21, iter:10364] Loss: 0.973 | Acc: 65.140% \n",
      "[epoch:21, iter:10365] Loss: 0.973 | Acc: 65.159% \n",
      "[epoch:21, iter:10366] Loss: 0.973 | Acc: 65.161% \n",
      "[epoch:21, iter:10367] Loss: 0.973 | Acc: 65.136% \n",
      "[epoch:21, iter:10368] Loss: 0.973 | Acc: 65.111% \n",
      "[epoch:21, iter:10369] Loss: 0.974 | Acc: 65.100% \n",
      "[epoch:21, iter:10370] Loss: 0.974 | Acc: 65.095% \n",
      "[epoch:21, iter:10371] Loss: 0.974 | Acc: 65.097% \n",
      "[epoch:21, iter:10372] Loss: 0.974 | Acc: 65.094% \n",
      "[epoch:21, iter:10373] Loss: 0.974 | Acc: 65.097% \n",
      "[epoch:21, iter:10374] Loss: 0.973 | Acc: 65.094% \n",
      "[epoch:21, iter:10375] Loss: 0.973 | Acc: 65.104% \n",
      "[epoch:21, iter:10376] Loss: 0.973 | Acc: 65.144% \n",
      "[epoch:21, iter:10377] Loss: 0.973 | Acc: 65.119% \n",
      "[epoch:21, iter:10378] Loss: 0.973 | Acc: 65.116% \n",
      "[epoch:21, iter:10379] Loss: 0.972 | Acc: 65.153% \n",
      "[epoch:21, iter:10380] Loss: 0.972 | Acc: 65.161% \n",
      "[epoch:21, iter:10381] Loss: 0.972 | Acc: 65.157% \n",
      "[epoch:21, iter:10382] Loss: 0.971 | Acc: 65.178% \n",
      "[epoch:21, iter:10383] Loss: 0.971 | Acc: 65.188% \n",
      "[epoch:21, iter:10384] Loss: 0.971 | Acc: 65.190% \n",
      "[epoch:21, iter:10385] Loss: 0.971 | Acc: 65.200% \n",
      "[epoch:21, iter:10386] Loss: 0.971 | Acc: 65.184% \n",
      "[epoch:21, iter:10387] Loss: 0.970 | Acc: 65.196% \n",
      "[epoch:21, iter:10388] Loss: 0.971 | Acc: 65.178% \n",
      "[epoch:21, iter:10389] Loss: 0.971 | Acc: 65.180% \n",
      "[epoch:21, iter:10390] Loss: 0.970 | Acc: 65.197% \n",
      "[epoch:21, iter:10391] Loss: 0.970 | Acc: 65.205% \n",
      "[epoch:21, iter:10392] Loss: 0.970 | Acc: 65.207% \n",
      "[epoch:21, iter:10393] Loss: 0.970 | Acc: 65.224% \n",
      "[epoch:21, iter:10394] Loss: 0.970 | Acc: 65.228% \n",
      "[epoch:21, iter:10395] Loss: 0.970 | Acc: 65.223% \n",
      "[epoch:21, iter:10396] Loss: 0.970 | Acc: 65.230% \n",
      "[epoch:21, iter:10397] Loss: 0.970 | Acc: 65.237% \n",
      "[epoch:21, iter:10398] Loss: 0.970 | Acc: 65.239% \n",
      "[epoch:21, iter:10399] Loss: 0.970 | Acc: 65.238% \n",
      "[epoch:21, iter:10400] Loss: 0.970 | Acc: 65.230% \n",
      "[epoch:21, iter:10401] Loss: 0.970 | Acc: 65.217% \n",
      "[epoch:21, iter:10402] Loss: 0.970 | Acc: 65.226% \n",
      "[epoch:21, iter:10403] Loss: 0.970 | Acc: 65.231% \n",
      "[epoch:21, iter:10404] Loss: 0.970 | Acc: 65.240% \n",
      "[epoch:21, iter:10405] Loss: 0.970 | Acc: 65.240% \n",
      "[epoch:21, iter:10406] Loss: 0.970 | Acc: 65.236% \n",
      "[epoch:21, iter:10407] Loss: 0.970 | Acc: 65.241% \n",
      "[epoch:21, iter:10408] Loss: 0.970 | Acc: 65.245% \n",
      "[epoch:21, iter:10409] Loss: 0.970 | Acc: 65.249% \n",
      "[epoch:21, iter:10410] Loss: 0.970 | Acc: 65.241% \n",
      "[epoch:21, iter:10411] Loss: 0.969 | Acc: 65.253% \n",
      "[epoch:21, iter:10412] Loss: 0.970 | Acc: 65.240% \n",
      "[epoch:21, iter:10413] Loss: 0.970 | Acc: 65.218% \n",
      "[epoch:21, iter:10414] Loss: 0.970 | Acc: 65.220% \n",
      "[epoch:21, iter:10415] Loss: 0.970 | Acc: 65.219% \n",
      "[epoch:21, iter:10416] Loss: 0.969 | Acc: 65.236% \n",
      "[epoch:21, iter:10417] Loss: 0.970 | Acc: 65.228% \n",
      "[epoch:21, iter:10418] Loss: 0.969 | Acc: 65.237% \n",
      "[epoch:21, iter:10419] Loss: 0.969 | Acc: 65.246% \n",
      "[epoch:21, iter:10420] Loss: 0.969 | Acc: 65.255% \n",
      "[epoch:21, iter:10421] Loss: 0.969 | Acc: 65.266% \n",
      "[epoch:21, iter:10422] Loss: 0.968 | Acc: 65.270% \n",
      "[epoch:21, iter:10423] Loss: 0.968 | Acc: 65.265% \n",
      "[epoch:21, iter:10424] Loss: 0.968 | Acc: 65.267% \n",
      "[epoch:21, iter:10425] Loss: 0.968 | Acc: 65.278% \n",
      "[epoch:21, iter:10426] Loss: 0.967 | Acc: 65.282% \n",
      "[epoch:21, iter:10427] Loss: 0.967 | Acc: 65.274% \n",
      "[epoch:21, iter:10428] Loss: 0.967 | Acc: 65.280% \n",
      "[epoch:21, iter:10429] Loss: 0.966 | Acc: 65.301% \n",
      "[epoch:21, iter:10430] Loss: 0.966 | Acc: 65.307% \n",
      "[epoch:21, iter:10431] Loss: 0.967 | Acc: 65.306% \n",
      "[epoch:21, iter:10432] Loss: 0.966 | Acc: 65.319% \n",
      "[epoch:21, iter:10433] Loss: 0.966 | Acc: 65.323% \n",
      "[epoch:21, iter:10434] Loss: 0.967 | Acc: 65.318% \n",
      "[epoch:21, iter:10435] Loss: 0.966 | Acc: 65.331% \n",
      "[epoch:21, iter:10436] Loss: 0.966 | Acc: 65.335% \n",
      "[epoch:21, iter:10437] Loss: 0.966 | Acc: 65.341% \n",
      "[epoch:21, iter:10438] Loss: 0.966 | Acc: 65.345% \n",
      "[epoch:21, iter:10439] Loss: 0.966 | Acc: 65.339% \n",
      "[epoch:21, iter:10440] Loss: 0.966 | Acc: 65.339% \n",
      "[epoch:21, iter:10441] Loss: 0.966 | Acc: 65.333% \n",
      "[epoch:21, iter:10442] Loss: 0.965 | Acc: 65.362% \n",
      "[epoch:21, iter:10443] Loss: 0.966 | Acc: 65.345% \n",
      "[epoch:21, iter:10444] Loss: 0.966 | Acc: 65.356% \n",
      "[epoch:21, iter:10445] Loss: 0.965 | Acc: 65.362% \n",
      "[epoch:21, iter:10446] Loss: 0.965 | Acc: 65.374% \n",
      "[epoch:21, iter:10447] Loss: 0.964 | Acc: 65.398% \n",
      "[epoch:21, iter:10448] Loss: 0.965 | Acc: 65.397% \n",
      "[epoch:21, iter:10449] Loss: 0.965 | Acc: 65.390% \n",
      "[epoch:21, iter:10450] Loss: 0.964 | Acc: 65.413% \n",
      "[epoch:21, iter:10451] Loss: 0.964 | Acc: 65.419% \n",
      "[epoch:21, iter:10452] Loss: 0.964 | Acc: 65.416% \n",
      "[epoch:21, iter:10453] Loss: 0.964 | Acc: 65.428% \n",
      "[epoch:21, iter:10454] Loss: 0.964 | Acc: 65.421% \n",
      "[epoch:21, iter:10455] Loss: 0.964 | Acc: 65.429% \n",
      "[epoch:21, iter:10456] Loss: 0.964 | Acc: 65.417% \n",
      "[epoch:21, iter:10457] Loss: 0.964 | Acc: 65.411% \n",
      "[epoch:21, iter:10458] Loss: 0.964 | Acc: 65.428% \n",
      "[epoch:21, iter:10459] Loss: 0.964 | Acc: 65.434% \n",
      "[epoch:21, iter:10460] Loss: 0.964 | Acc: 65.433% \n",
      "[epoch:21, iter:10461] Loss: 0.964 | Acc: 65.427% \n",
      "[epoch:21, iter:10462] Loss: 0.964 | Acc: 65.426% \n",
      "[epoch:21, iter:10463] Loss: 0.964 | Acc: 65.428% \n",
      "[epoch:21, iter:10464] Loss: 0.964 | Acc: 65.448% \n",
      "[epoch:21, iter:10465] Loss: 0.963 | Acc: 65.460% \n",
      "[epoch:21, iter:10466] Loss: 0.963 | Acc: 65.459% \n",
      "[epoch:21, iter:10467] Loss: 0.963 | Acc: 65.456% \n",
      "[epoch:21, iter:10468] Loss: 0.963 | Acc: 65.479% \n",
      "[epoch:21, iter:10469] Loss: 0.963 | Acc: 65.469% \n",
      "[epoch:21, iter:10470] Loss: 0.963 | Acc: 65.479% \n",
      "[epoch:21, iter:10471] Loss: 0.963 | Acc: 65.473% \n",
      "[epoch:21, iter:10472] Loss: 0.963 | Acc: 65.485% \n",
      "[epoch:21, iter:10473] Loss: 0.963 | Acc: 65.486% \n",
      "[epoch:21, iter:10474] Loss: 0.963 | Acc: 65.494% \n",
      "[epoch:21, iter:10475] Loss: 0.963 | Acc: 65.501% \n",
      "[epoch:21, iter:10476] Loss: 0.963 | Acc: 65.506% \n",
      "[epoch:21, iter:10477] Loss: 0.962 | Acc: 65.522% \n",
      "[epoch:21, iter:10478] Loss: 0.962 | Acc: 65.548% \n",
      "[epoch:21, iter:10479] Loss: 0.962 | Acc: 65.551% \n",
      "[epoch:21, iter:10480] Loss: 0.962 | Acc: 65.550% \n",
      "[epoch:21, iter:10481] Loss: 0.962 | Acc: 65.547% \n",
      "[epoch:21, iter:10482] Loss: 0.962 | Acc: 65.550% \n",
      "[epoch:21, iter:10483] Loss: 0.962 | Acc: 65.553% \n",
      "[epoch:21, iter:10484] Loss: 0.961 | Acc: 65.568% \n",
      "[epoch:21, iter:10485] Loss: 0.962 | Acc: 65.553% \n",
      "[epoch:21, iter:10486] Loss: 0.962 | Acc: 65.543% \n",
      "[epoch:21, iter:10487] Loss: 0.962 | Acc: 65.546% \n",
      "[epoch:21, iter:10488] Loss: 0.961 | Acc: 65.561% \n",
      "[epoch:21, iter:10489] Loss: 0.961 | Acc: 65.564% \n",
      "[epoch:21, iter:10490] Loss: 0.962 | Acc: 65.563% \n",
      "[epoch:21, iter:10491] Loss: 0.962 | Acc: 65.566% \n",
      "[epoch:21, iter:10492] Loss: 0.961 | Acc: 65.585% \n",
      "[epoch:21, iter:10493] Loss: 0.962 | Acc: 65.572% \n",
      "[epoch:21, iter:10494] Loss: 0.962 | Acc: 65.557% \n",
      "[epoch:21, iter:10495] Loss: 0.961 | Acc: 65.574% \n",
      "[epoch:21, iter:10496] Loss: 0.961 | Acc: 65.560% \n",
      "[epoch:21, iter:10497] Loss: 0.962 | Acc: 65.569% \n",
      "[epoch:21, iter:10498] Loss: 0.962 | Acc: 65.572% \n",
      "[epoch:21, iter:10499] Loss: 0.962 | Acc: 65.583% \n",
      "[epoch:21, iter:10500] Loss: 0.962 | Acc: 65.590% \n",
      "Waiting Test...\n",
      "Test's ac is: 63.150%\n",
      "\n",
      "Epoch: 22\n",
      "[epoch:22, iter:10501] Loss: 0.929 | Acc: 70.000% \n",
      "[epoch:22, iter:10502] Loss: 0.899 | Acc: 69.000% \n",
      "[epoch:22, iter:10503] Loss: 0.880 | Acc: 67.667% \n",
      "[epoch:22, iter:10504] Loss: 0.880 | Acc: 68.250% \n",
      "[epoch:22, iter:10505] Loss: 0.876 | Acc: 68.000% \n",
      "[epoch:22, iter:10506] Loss: 0.877 | Acc: 67.333% \n",
      "[epoch:22, iter:10507] Loss: 0.859 | Acc: 68.000% \n",
      "[epoch:22, iter:10508] Loss: 0.890 | Acc: 67.125% \n",
      "[epoch:22, iter:10509] Loss: 0.896 | Acc: 67.222% \n",
      "[epoch:22, iter:10510] Loss: 0.893 | Acc: 66.600% \n",
      "[epoch:22, iter:10511] Loss: 0.904 | Acc: 66.091% \n",
      "[epoch:22, iter:10512] Loss: 0.927 | Acc: 65.500% \n",
      "[epoch:22, iter:10513] Loss: 0.929 | Acc: 65.692% \n",
      "[epoch:22, iter:10514] Loss: 0.925 | Acc: 66.000% \n",
      "[epoch:22, iter:10515] Loss: 0.925 | Acc: 66.133% \n",
      "[epoch:22, iter:10516] Loss: 0.922 | Acc: 66.375% \n",
      "[epoch:22, iter:10517] Loss: 0.917 | Acc: 66.647% \n",
      "[epoch:22, iter:10518] Loss: 0.919 | Acc: 66.667% \n",
      "[epoch:22, iter:10519] Loss: 0.919 | Acc: 66.526% \n",
      "[epoch:22, iter:10520] Loss: 0.917 | Acc: 66.650% \n",
      "[epoch:22, iter:10521] Loss: 0.914 | Acc: 66.571% \n",
      "[epoch:22, iter:10522] Loss: 0.911 | Acc: 66.864% \n",
      "[epoch:22, iter:10523] Loss: 0.902 | Acc: 67.435% \n",
      "[epoch:22, iter:10524] Loss: 0.901 | Acc: 67.417% \n",
      "[epoch:22, iter:10525] Loss: 0.908 | Acc: 67.240% \n",
      "[epoch:22, iter:10526] Loss: 0.910 | Acc: 67.038% \n",
      "[epoch:22, iter:10527] Loss: 0.911 | Acc: 67.037% \n",
      "[epoch:22, iter:10528] Loss: 0.909 | Acc: 67.071% \n",
      "[epoch:22, iter:10529] Loss: 0.906 | Acc: 67.241% \n",
      "[epoch:22, iter:10530] Loss: 0.904 | Acc: 67.400% \n",
      "[epoch:22, iter:10531] Loss: 0.904 | Acc: 67.419% \n",
      "[epoch:22, iter:10532] Loss: 0.906 | Acc: 67.375% \n",
      "[epoch:22, iter:10533] Loss: 0.906 | Acc: 67.303% \n",
      "[epoch:22, iter:10534] Loss: 0.906 | Acc: 67.382% \n",
      "[epoch:22, iter:10535] Loss: 0.904 | Acc: 67.429% \n",
      "[epoch:22, iter:10536] Loss: 0.906 | Acc: 67.306% \n",
      "[epoch:22, iter:10537] Loss: 0.908 | Acc: 67.297% \n",
      "[epoch:22, iter:10538] Loss: 0.909 | Acc: 67.263% \n",
      "[epoch:22, iter:10539] Loss: 0.909 | Acc: 67.385% \n",
      "[epoch:22, iter:10540] Loss: 0.912 | Acc: 67.325% \n",
      "[epoch:22, iter:10541] Loss: 0.915 | Acc: 67.268% \n",
      "[epoch:22, iter:10542] Loss: 0.913 | Acc: 67.262% \n",
      "[epoch:22, iter:10543] Loss: 0.912 | Acc: 67.140% \n",
      "[epoch:22, iter:10544] Loss: 0.915 | Acc: 67.045% \n",
      "[epoch:22, iter:10545] Loss: 0.919 | Acc: 66.889% \n",
      "[epoch:22, iter:10546] Loss: 0.919 | Acc: 66.913% \n",
      "[epoch:22, iter:10547] Loss: 0.919 | Acc: 66.872% \n",
      "[epoch:22, iter:10548] Loss: 0.923 | Acc: 66.729% \n",
      "[epoch:22, iter:10549] Loss: 0.923 | Acc: 66.673% \n",
      "[epoch:22, iter:10550] Loss: 0.921 | Acc: 66.700% \n",
      "[epoch:22, iter:10551] Loss: 0.921 | Acc: 66.647% \n",
      "[epoch:22, iter:10552] Loss: 0.921 | Acc: 66.712% \n",
      "[epoch:22, iter:10553] Loss: 0.921 | Acc: 66.660% \n",
      "[epoch:22, iter:10554] Loss: 0.920 | Acc: 66.796% \n",
      "[epoch:22, iter:10555] Loss: 0.919 | Acc: 66.764% \n",
      "[epoch:22, iter:10556] Loss: 0.920 | Acc: 66.750% \n",
      "[epoch:22, iter:10557] Loss: 0.921 | Acc: 66.684% \n",
      "[epoch:22, iter:10558] Loss: 0.922 | Acc: 66.672% \n",
      "[epoch:22, iter:10559] Loss: 0.921 | Acc: 66.627% \n",
      "[epoch:22, iter:10560] Loss: 0.922 | Acc: 66.533% \n",
      "[epoch:22, iter:10561] Loss: 0.924 | Acc: 66.541% \n",
      "[epoch:22, iter:10562] Loss: 0.925 | Acc: 66.452% \n",
      "[epoch:22, iter:10563] Loss: 0.928 | Acc: 66.429% \n",
      "[epoch:22, iter:10564] Loss: 0.928 | Acc: 66.500% \n",
      "[epoch:22, iter:10565] Loss: 0.929 | Acc: 66.462% \n",
      "[epoch:22, iter:10566] Loss: 0.930 | Acc: 66.409% \n",
      "[epoch:22, iter:10567] Loss: 0.930 | Acc: 66.358% \n",
      "[epoch:22, iter:10568] Loss: 0.929 | Acc: 66.338% \n",
      "[epoch:22, iter:10569] Loss: 0.930 | Acc: 66.275% \n",
      "[epoch:22, iter:10570] Loss: 0.930 | Acc: 66.229% \n",
      "[epoch:22, iter:10571] Loss: 0.930 | Acc: 66.211% \n",
      "[epoch:22, iter:10572] Loss: 0.930 | Acc: 66.264% \n",
      "[epoch:22, iter:10573] Loss: 0.931 | Acc: 66.247% \n",
      "[epoch:22, iter:10574] Loss: 0.932 | Acc: 66.203% \n",
      "[epoch:22, iter:10575] Loss: 0.933 | Acc: 66.187% \n",
      "[epoch:22, iter:10576] Loss: 0.932 | Acc: 66.237% \n",
      "[epoch:22, iter:10577] Loss: 0.933 | Acc: 66.169% \n",
      "[epoch:22, iter:10578] Loss: 0.932 | Acc: 66.231% \n",
      "[epoch:22, iter:10579] Loss: 0.932 | Acc: 66.177% \n",
      "[epoch:22, iter:10580] Loss: 0.933 | Acc: 66.150% \n",
      "[epoch:22, iter:10581] Loss: 0.932 | Acc: 66.210% \n",
      "[epoch:22, iter:10582] Loss: 0.931 | Acc: 66.207% \n",
      "[epoch:22, iter:10583] Loss: 0.931 | Acc: 66.241% \n",
      "[epoch:22, iter:10584] Loss: 0.933 | Acc: 66.262% \n",
      "[epoch:22, iter:10585] Loss: 0.932 | Acc: 66.353% \n",
      "[epoch:22, iter:10586] Loss: 0.933 | Acc: 66.337% \n",
      "[epoch:22, iter:10587] Loss: 0.932 | Acc: 66.333% \n",
      "[epoch:22, iter:10588] Loss: 0.933 | Acc: 66.261% \n",
      "[epoch:22, iter:10589] Loss: 0.934 | Acc: 66.247% \n",
      "[epoch:22, iter:10590] Loss: 0.934 | Acc: 66.256% \n",
      "[epoch:22, iter:10591] Loss: 0.933 | Acc: 66.319% \n",
      "[epoch:22, iter:10592] Loss: 0.933 | Acc: 66.283% \n",
      "[epoch:22, iter:10593] Loss: 0.933 | Acc: 66.258% \n",
      "[epoch:22, iter:10594] Loss: 0.932 | Acc: 66.255% \n",
      "[epoch:22, iter:10595] Loss: 0.933 | Acc: 66.232% \n",
      "[epoch:22, iter:10596] Loss: 0.935 | Acc: 66.135% \n",
      "[epoch:22, iter:10597] Loss: 0.934 | Acc: 66.175% \n",
      "[epoch:22, iter:10598] Loss: 0.934 | Acc: 66.173% \n",
      "[epoch:22, iter:10599] Loss: 0.934 | Acc: 66.152% \n",
      "[epoch:22, iter:10600] Loss: 0.935 | Acc: 66.080% \n",
      "[epoch:22, iter:10601] Loss: 0.936 | Acc: 66.089% \n",
      "[epoch:22, iter:10602] Loss: 0.936 | Acc: 66.049% \n",
      "[epoch:22, iter:10603] Loss: 0.935 | Acc: 66.087% \n",
      "[epoch:22, iter:10604] Loss: 0.935 | Acc: 66.096% \n",
      "[epoch:22, iter:10605] Loss: 0.936 | Acc: 66.048% \n",
      "[epoch:22, iter:10606] Loss: 0.937 | Acc: 66.028% \n",
      "[epoch:22, iter:10607] Loss: 0.938 | Acc: 65.953% \n",
      "[epoch:22, iter:10608] Loss: 0.940 | Acc: 65.898% \n",
      "[epoch:22, iter:10609] Loss: 0.940 | Acc: 65.899% \n",
      "[epoch:22, iter:10610] Loss: 0.941 | Acc: 65.873% \n",
      "[epoch:22, iter:10611] Loss: 0.941 | Acc: 65.892% \n",
      "[epoch:22, iter:10612] Loss: 0.940 | Acc: 65.893% \n",
      "[epoch:22, iter:10613] Loss: 0.940 | Acc: 65.929% \n",
      "[epoch:22, iter:10614] Loss: 0.941 | Acc: 65.904% \n",
      "[epoch:22, iter:10615] Loss: 0.942 | Acc: 65.922% \n",
      "[epoch:22, iter:10616] Loss: 0.942 | Acc: 65.922% \n",
      "[epoch:22, iter:10617] Loss: 0.943 | Acc: 65.906% \n",
      "[epoch:22, iter:10618] Loss: 0.943 | Acc: 65.907% \n",
      "[epoch:22, iter:10619] Loss: 0.942 | Acc: 65.933% \n",
      "[epoch:22, iter:10620] Loss: 0.942 | Acc: 65.933% \n",
      "[epoch:22, iter:10621] Loss: 0.944 | Acc: 65.843% \n",
      "[epoch:22, iter:10622] Loss: 0.944 | Acc: 65.877% \n",
      "[epoch:22, iter:10623] Loss: 0.943 | Acc: 65.886% \n",
      "[epoch:22, iter:10624] Loss: 0.941 | Acc: 65.976% \n",
      "[epoch:22, iter:10625] Loss: 0.942 | Acc: 65.968% \n",
      "[epoch:22, iter:10626] Loss: 0.942 | Acc: 65.921% \n",
      "[epoch:22, iter:10627] Loss: 0.941 | Acc: 65.953% \n",
      "[epoch:22, iter:10628] Loss: 0.941 | Acc: 65.945% \n",
      "[epoch:22, iter:10629] Loss: 0.941 | Acc: 65.961% \n",
      "[epoch:22, iter:10630] Loss: 0.941 | Acc: 66.008% \n",
      "[epoch:22, iter:10631] Loss: 0.941 | Acc: 66.000% \n",
      "[epoch:22, iter:10632] Loss: 0.940 | Acc: 66.076% \n",
      "[epoch:22, iter:10633] Loss: 0.939 | Acc: 66.090% \n",
      "[epoch:22, iter:10634] Loss: 0.940 | Acc: 66.075% \n",
      "[epoch:22, iter:10635] Loss: 0.940 | Acc: 66.081% \n",
      "[epoch:22, iter:10636] Loss: 0.940 | Acc: 66.066% \n",
      "[epoch:22, iter:10637] Loss: 0.940 | Acc: 66.036% \n",
      "[epoch:22, iter:10638] Loss: 0.940 | Acc: 66.072% \n",
      "[epoch:22, iter:10639] Loss: 0.939 | Acc: 66.108% \n",
      "[epoch:22, iter:10640] Loss: 0.940 | Acc: 66.100% \n",
      "[epoch:22, iter:10641] Loss: 0.939 | Acc: 66.128% \n",
      "[epoch:22, iter:10642] Loss: 0.939 | Acc: 66.120% \n",
      "[epoch:22, iter:10643] Loss: 0.940 | Acc: 66.077% \n",
      "[epoch:22, iter:10644] Loss: 0.939 | Acc: 66.104% \n",
      "[epoch:22, iter:10645] Loss: 0.939 | Acc: 66.152% \n",
      "[epoch:22, iter:10646] Loss: 0.938 | Acc: 66.219% \n",
      "[epoch:22, iter:10647] Loss: 0.938 | Acc: 66.211% \n",
      "[epoch:22, iter:10648] Loss: 0.938 | Acc: 66.223% \n",
      "[epoch:22, iter:10649] Loss: 0.939 | Acc: 66.208% \n",
      "[epoch:22, iter:10650] Loss: 0.939 | Acc: 66.187% \n",
      "[epoch:22, iter:10651] Loss: 0.937 | Acc: 66.265% \n",
      "[epoch:22, iter:10652] Loss: 0.936 | Acc: 66.270% \n",
      "[epoch:22, iter:10653] Loss: 0.936 | Acc: 66.275% \n",
      "[epoch:22, iter:10654] Loss: 0.936 | Acc: 66.273% \n",
      "[epoch:22, iter:10655] Loss: 0.938 | Acc: 66.252% \n",
      "[epoch:22, iter:10656] Loss: 0.938 | Acc: 66.237% \n",
      "[epoch:22, iter:10657] Loss: 0.937 | Acc: 66.274% \n",
      "[epoch:22, iter:10658] Loss: 0.937 | Acc: 66.285% \n",
      "[epoch:22, iter:10659] Loss: 0.937 | Acc: 66.283% \n",
      "[epoch:22, iter:10660] Loss: 0.937 | Acc: 66.275% \n",
      "[epoch:22, iter:10661] Loss: 0.936 | Acc: 66.348% \n",
      "[epoch:22, iter:10662] Loss: 0.936 | Acc: 66.358% \n",
      "[epoch:22, iter:10663] Loss: 0.936 | Acc: 66.380% \n",
      "[epoch:22, iter:10664] Loss: 0.936 | Acc: 66.402% \n",
      "[epoch:22, iter:10665] Loss: 0.934 | Acc: 66.455% \n",
      "[epoch:22, iter:10666] Loss: 0.934 | Acc: 66.476% \n",
      "[epoch:22, iter:10667] Loss: 0.933 | Acc: 66.509% \n",
      "[epoch:22, iter:10668] Loss: 0.933 | Acc: 66.458% \n",
      "[epoch:22, iter:10669] Loss: 0.934 | Acc: 66.432% \n",
      "[epoch:22, iter:10670] Loss: 0.934 | Acc: 66.424% \n",
      "[epoch:22, iter:10671] Loss: 0.934 | Acc: 66.415% \n",
      "[epoch:22, iter:10672] Loss: 0.934 | Acc: 66.436% \n",
      "[epoch:22, iter:10673] Loss: 0.934 | Acc: 66.422% \n",
      "[epoch:22, iter:10674] Loss: 0.933 | Acc: 66.460% \n",
      "[epoch:22, iter:10675] Loss: 0.934 | Acc: 66.434% \n",
      "[epoch:22, iter:10676] Loss: 0.934 | Acc: 66.438% \n",
      "[epoch:22, iter:10677] Loss: 0.934 | Acc: 66.418% \n",
      "[epoch:22, iter:10678] Loss: 0.935 | Acc: 66.399% \n",
      "[epoch:22, iter:10679] Loss: 0.934 | Acc: 66.419% \n",
      "[epoch:22, iter:10680] Loss: 0.934 | Acc: 66.456% \n",
      "[epoch:22, iter:10681] Loss: 0.935 | Acc: 66.431% \n",
      "[epoch:22, iter:10682] Loss: 0.935 | Acc: 66.462% \n",
      "[epoch:22, iter:10683] Loss: 0.935 | Acc: 66.475% \n",
      "[epoch:22, iter:10684] Loss: 0.935 | Acc: 66.505% \n",
      "[epoch:22, iter:10685] Loss: 0.934 | Acc: 66.530% \n",
      "[epoch:22, iter:10686] Loss: 0.935 | Acc: 66.484% \n",
      "[epoch:22, iter:10687] Loss: 0.934 | Acc: 66.492% \n",
      "[epoch:22, iter:10688] Loss: 0.934 | Acc: 66.500% \n",
      "[epoch:22, iter:10689] Loss: 0.934 | Acc: 66.540% \n",
      "[epoch:22, iter:10690] Loss: 0.934 | Acc: 66.537% \n",
      "[epoch:22, iter:10691] Loss: 0.933 | Acc: 66.550% \n",
      "[epoch:22, iter:10692] Loss: 0.933 | Acc: 66.573% \n",
      "[epoch:22, iter:10693] Loss: 0.934 | Acc: 66.534% \n",
      "[epoch:22, iter:10694] Loss: 0.933 | Acc: 66.546% \n",
      "[epoch:22, iter:10695] Loss: 0.933 | Acc: 66.533% \n",
      "[epoch:22, iter:10696] Loss: 0.933 | Acc: 66.510% \n",
      "[epoch:22, iter:10697] Loss: 0.933 | Acc: 66.518% \n",
      "[epoch:22, iter:10698] Loss: 0.932 | Acc: 66.525% \n",
      "[epoch:22, iter:10699] Loss: 0.933 | Acc: 66.503% \n",
      "[epoch:22, iter:10700] Loss: 0.933 | Acc: 66.530% \n",
      "[epoch:22, iter:10701] Loss: 0.933 | Acc: 66.522% \n",
      "[epoch:22, iter:10702] Loss: 0.933 | Acc: 66.525% \n",
      "[epoch:22, iter:10703] Loss: 0.933 | Acc: 66.517% \n",
      "[epoch:22, iter:10704] Loss: 0.934 | Acc: 66.500% \n",
      "[epoch:22, iter:10705] Loss: 0.934 | Acc: 66.517% \n",
      "[epoch:22, iter:10706] Loss: 0.934 | Acc: 66.485% \n",
      "[epoch:22, iter:10707] Loss: 0.934 | Acc: 66.493% \n",
      "[epoch:22, iter:10708] Loss: 0.935 | Acc: 66.457% \n",
      "[epoch:22, iter:10709] Loss: 0.934 | Acc: 66.459% \n",
      "[epoch:22, iter:10710] Loss: 0.934 | Acc: 66.462% \n",
      "[epoch:22, iter:10711] Loss: 0.933 | Acc: 66.474% \n",
      "[epoch:22, iter:10712] Loss: 0.933 | Acc: 66.505% \n",
      "[epoch:22, iter:10713] Loss: 0.933 | Acc: 66.493% \n",
      "[epoch:22, iter:10714] Loss: 0.934 | Acc: 66.467% \n",
      "[epoch:22, iter:10715] Loss: 0.933 | Acc: 66.460% \n",
      "[epoch:22, iter:10716] Loss: 0.932 | Acc: 66.481% \n",
      "[epoch:22, iter:10717] Loss: 0.932 | Acc: 66.456% \n",
      "[epoch:22, iter:10718] Loss: 0.933 | Acc: 66.440% \n",
      "[epoch:22, iter:10719] Loss: 0.933 | Acc: 66.466% \n",
      "[epoch:22, iter:10720] Loss: 0.933 | Acc: 66.455% \n",
      "[epoch:22, iter:10721] Loss: 0.932 | Acc: 66.480% \n",
      "[epoch:22, iter:10722] Loss: 0.932 | Acc: 66.509% \n",
      "[epoch:22, iter:10723] Loss: 0.932 | Acc: 66.520% \n",
      "[epoch:22, iter:10724] Loss: 0.932 | Acc: 66.522% \n",
      "[epoch:22, iter:10725] Loss: 0.932 | Acc: 66.529% \n",
      "[epoch:22, iter:10726] Loss: 0.931 | Acc: 66.522% \n",
      "[epoch:22, iter:10727] Loss: 0.932 | Acc: 66.515% \n",
      "[epoch:22, iter:10728] Loss: 0.931 | Acc: 66.526% \n",
      "[epoch:22, iter:10729] Loss: 0.931 | Acc: 66.533% \n",
      "[epoch:22, iter:10730] Loss: 0.931 | Acc: 66.504% \n",
      "[epoch:22, iter:10731] Loss: 0.930 | Acc: 66.524% \n",
      "[epoch:22, iter:10732] Loss: 0.930 | Acc: 66.522% \n",
      "[epoch:22, iter:10733] Loss: 0.931 | Acc: 66.515% \n",
      "[epoch:22, iter:10734] Loss: 0.931 | Acc: 66.530% \n",
      "[epoch:22, iter:10735] Loss: 0.930 | Acc: 66.540% \n",
      "[epoch:22, iter:10736] Loss: 0.930 | Acc: 66.530% \n",
      "[epoch:22, iter:10737] Loss: 0.930 | Acc: 66.557% \n",
      "[epoch:22, iter:10738] Loss: 0.931 | Acc: 66.529% \n",
      "[epoch:22, iter:10739] Loss: 0.930 | Acc: 66.544% \n",
      "[epoch:22, iter:10740] Loss: 0.930 | Acc: 66.554% \n",
      "[epoch:22, iter:10741] Loss: 0.930 | Acc: 66.577% \n",
      "[epoch:22, iter:10742] Loss: 0.930 | Acc: 66.562% \n",
      "[epoch:22, iter:10743] Loss: 0.930 | Acc: 66.547% \n",
      "[epoch:22, iter:10744] Loss: 0.931 | Acc: 66.541% \n",
      "[epoch:22, iter:10745] Loss: 0.930 | Acc: 66.551% \n",
      "[epoch:22, iter:10746] Loss: 0.931 | Acc: 66.537% \n",
      "[epoch:22, iter:10747] Loss: 0.930 | Acc: 66.551% \n",
      "[epoch:22, iter:10748] Loss: 0.930 | Acc: 66.540% \n",
      "[epoch:22, iter:10749] Loss: 0.930 | Acc: 66.542% \n",
      "[epoch:22, iter:10750] Loss: 0.930 | Acc: 66.536% \n",
      "[epoch:22, iter:10751] Loss: 0.931 | Acc: 66.522% \n",
      "[epoch:22, iter:10752] Loss: 0.931 | Acc: 66.520% \n",
      "[epoch:22, iter:10753] Loss: 0.931 | Acc: 66.518% \n",
      "[epoch:22, iter:10754] Loss: 0.931 | Acc: 66.528% \n",
      "[epoch:22, iter:10755] Loss: 0.931 | Acc: 66.545% \n",
      "[epoch:22, iter:10756] Loss: 0.931 | Acc: 66.531% \n",
      "[epoch:22, iter:10757] Loss: 0.931 | Acc: 66.545% \n",
      "[epoch:22, iter:10758] Loss: 0.931 | Acc: 66.543% \n",
      "[epoch:22, iter:10759] Loss: 0.931 | Acc: 66.533% \n",
      "[epoch:22, iter:10760] Loss: 0.931 | Acc: 66.550% \n",
      "[epoch:22, iter:10761] Loss: 0.931 | Acc: 66.533% \n",
      "[epoch:22, iter:10762] Loss: 0.931 | Acc: 66.519% \n",
      "[epoch:22, iter:10763] Loss: 0.931 | Acc: 66.532% \n",
      "[epoch:22, iter:10764] Loss: 0.931 | Acc: 66.519% \n",
      "[epoch:22, iter:10765] Loss: 0.932 | Acc: 66.506% \n",
      "[epoch:22, iter:10766] Loss: 0.932 | Acc: 66.504% \n",
      "[epoch:22, iter:10767] Loss: 0.932 | Acc: 66.502% \n",
      "[epoch:22, iter:10768] Loss: 0.932 | Acc: 66.507% \n",
      "[epoch:22, iter:10769] Loss: 0.932 | Acc: 66.528% \n",
      "[epoch:22, iter:10770] Loss: 0.932 | Acc: 66.511% \n",
      "[epoch:22, iter:10771] Loss: 0.932 | Acc: 66.509% \n",
      "[epoch:22, iter:10772] Loss: 0.932 | Acc: 66.529% \n",
      "[epoch:22, iter:10773] Loss: 0.932 | Acc: 66.524% \n",
      "[epoch:22, iter:10774] Loss: 0.933 | Acc: 66.507% \n",
      "[epoch:22, iter:10775] Loss: 0.932 | Acc: 66.520% \n",
      "[epoch:22, iter:10776] Loss: 0.932 | Acc: 66.540% \n",
      "[epoch:22, iter:10777] Loss: 0.932 | Acc: 66.520% \n",
      "[epoch:22, iter:10778] Loss: 0.933 | Acc: 66.518% \n",
      "[epoch:22, iter:10779] Loss: 0.933 | Acc: 66.513% \n",
      "[epoch:22, iter:10780] Loss: 0.932 | Acc: 66.554% \n",
      "[epoch:22, iter:10781] Loss: 0.933 | Acc: 66.555% \n",
      "[epoch:22, iter:10782] Loss: 0.933 | Acc: 66.543% \n",
      "[epoch:22, iter:10783] Loss: 0.933 | Acc: 66.548% \n",
      "[epoch:22, iter:10784] Loss: 0.934 | Acc: 66.532% \n",
      "[epoch:22, iter:10785] Loss: 0.933 | Acc: 66.547% \n",
      "[epoch:22, iter:10786] Loss: 0.934 | Acc: 66.538% \n",
      "[epoch:22, iter:10787] Loss: 0.934 | Acc: 66.537% \n",
      "[epoch:22, iter:10788] Loss: 0.934 | Acc: 66.521% \n",
      "[epoch:22, iter:10789] Loss: 0.934 | Acc: 66.502% \n",
      "[epoch:22, iter:10790] Loss: 0.934 | Acc: 66.510% \n",
      "[epoch:22, iter:10791] Loss: 0.934 | Acc: 66.498% \n",
      "[epoch:22, iter:10792] Loss: 0.934 | Acc: 66.507% \n",
      "[epoch:22, iter:10793] Loss: 0.934 | Acc: 66.495% \n",
      "[epoch:22, iter:10794] Loss: 0.934 | Acc: 66.510% \n",
      "[epoch:22, iter:10795] Loss: 0.934 | Acc: 66.498% \n",
      "[epoch:22, iter:10796] Loss: 0.934 | Acc: 66.490% \n",
      "[epoch:22, iter:10797] Loss: 0.933 | Acc: 66.488% \n",
      "[epoch:22, iter:10798] Loss: 0.933 | Acc: 66.503% \n",
      "[epoch:22, iter:10799] Loss: 0.934 | Acc: 66.498% \n",
      "[epoch:22, iter:10800] Loss: 0.933 | Acc: 66.510% \n",
      "[epoch:22, iter:10801] Loss: 0.934 | Acc: 66.482% \n",
      "[epoch:22, iter:10802] Loss: 0.934 | Acc: 66.487% \n",
      "[epoch:22, iter:10803] Loss: 0.933 | Acc: 66.508% \n",
      "[epoch:22, iter:10804] Loss: 0.934 | Acc: 66.500% \n",
      "[epoch:22, iter:10805] Loss: 0.933 | Acc: 66.515% \n",
      "[epoch:22, iter:10806] Loss: 0.933 | Acc: 66.507% \n",
      "[epoch:22, iter:10807] Loss: 0.934 | Acc: 66.485% \n",
      "[epoch:22, iter:10808] Loss: 0.933 | Acc: 66.490% \n",
      "[epoch:22, iter:10809] Loss: 0.934 | Acc: 66.476% \n",
      "[epoch:22, iter:10810] Loss: 0.934 | Acc: 66.465% \n",
      "[epoch:22, iter:10811] Loss: 0.934 | Acc: 66.466% \n",
      "[epoch:22, iter:10812] Loss: 0.933 | Acc: 66.494% \n",
      "[epoch:22, iter:10813] Loss: 0.933 | Acc: 66.498% \n",
      "[epoch:22, iter:10814] Loss: 0.933 | Acc: 66.506% \n",
      "[epoch:22, iter:10815] Loss: 0.933 | Acc: 66.505% \n",
      "[epoch:22, iter:10816] Loss: 0.933 | Acc: 66.513% \n",
      "[epoch:22, iter:10817] Loss: 0.933 | Acc: 66.527% \n",
      "[epoch:22, iter:10818] Loss: 0.932 | Acc: 66.519% \n",
      "[epoch:22, iter:10819] Loss: 0.932 | Acc: 66.502% \n",
      "[epoch:22, iter:10820] Loss: 0.933 | Acc: 66.484% \n",
      "[epoch:22, iter:10821] Loss: 0.933 | Acc: 66.495% \n",
      "[epoch:22, iter:10822] Loss: 0.933 | Acc: 66.503% \n",
      "[epoch:22, iter:10823] Loss: 0.932 | Acc: 66.511% \n",
      "[epoch:22, iter:10824] Loss: 0.932 | Acc: 66.512% \n",
      "[epoch:22, iter:10825] Loss: 0.933 | Acc: 66.517% \n",
      "[epoch:22, iter:10826] Loss: 0.933 | Acc: 66.515% \n",
      "[epoch:22, iter:10827] Loss: 0.933 | Acc: 66.505% \n",
      "[epoch:22, iter:10828] Loss: 0.933 | Acc: 66.497% \n",
      "[epoch:22, iter:10829] Loss: 0.933 | Acc: 66.471% \n",
      "[epoch:22, iter:10830] Loss: 0.933 | Acc: 66.494% \n",
      "[epoch:22, iter:10831] Loss: 0.933 | Acc: 66.498% \n",
      "[epoch:22, iter:10832] Loss: 0.933 | Acc: 66.503% \n",
      "[epoch:22, iter:10833] Loss: 0.932 | Acc: 66.523% \n",
      "[epoch:22, iter:10834] Loss: 0.932 | Acc: 66.530% \n",
      "[epoch:22, iter:10835] Loss: 0.932 | Acc: 66.516% \n",
      "[epoch:22, iter:10836] Loss: 0.933 | Acc: 66.509% \n",
      "[epoch:22, iter:10837] Loss: 0.933 | Acc: 66.504% \n",
      "[epoch:22, iter:10838] Loss: 0.933 | Acc: 66.497% \n",
      "[epoch:22, iter:10839] Loss: 0.932 | Acc: 66.507% \n",
      "[epoch:22, iter:10840] Loss: 0.932 | Acc: 66.512% \n",
      "[epoch:22, iter:10841] Loss: 0.932 | Acc: 66.501% \n",
      "[epoch:22, iter:10842] Loss: 0.932 | Acc: 66.497% \n",
      "[epoch:22, iter:10843] Loss: 0.932 | Acc: 66.499% \n",
      "[epoch:22, iter:10844] Loss: 0.932 | Acc: 66.509% \n",
      "[epoch:22, iter:10845] Loss: 0.932 | Acc: 66.507% \n",
      "[epoch:22, iter:10846] Loss: 0.932 | Acc: 66.494% \n",
      "[epoch:22, iter:10847] Loss: 0.932 | Acc: 66.481% \n",
      "[epoch:22, iter:10848] Loss: 0.932 | Acc: 66.477% \n",
      "[epoch:22, iter:10849] Loss: 0.932 | Acc: 66.479% \n",
      "[epoch:22, iter:10850] Loss: 0.932 | Acc: 66.477% \n",
      "[epoch:22, iter:10851] Loss: 0.932 | Acc: 66.459% \n",
      "[epoch:22, iter:10852] Loss: 0.932 | Acc: 66.457% \n",
      "[epoch:22, iter:10853] Loss: 0.933 | Acc: 66.456% \n",
      "[epoch:22, iter:10854] Loss: 0.933 | Acc: 66.458% \n",
      "[epoch:22, iter:10855] Loss: 0.933 | Acc: 66.448% \n",
      "[epoch:22, iter:10856] Loss: 0.933 | Acc: 66.438% \n",
      "[epoch:22, iter:10857] Loss: 0.934 | Acc: 66.412% \n",
      "[epoch:22, iter:10858] Loss: 0.934 | Acc: 66.411% \n",
      "[epoch:22, iter:10859] Loss: 0.934 | Acc: 66.393% \n",
      "[epoch:22, iter:10860] Loss: 0.934 | Acc: 66.389% \n",
      "[epoch:22, iter:10861] Loss: 0.933 | Acc: 66.418% \n",
      "[epoch:22, iter:10862] Loss: 0.934 | Acc: 66.406% \n",
      "[epoch:22, iter:10863] Loss: 0.933 | Acc: 66.416% \n",
      "[epoch:22, iter:10864] Loss: 0.933 | Acc: 66.426% \n",
      "[epoch:22, iter:10865] Loss: 0.933 | Acc: 66.444% \n",
      "[epoch:22, iter:10866] Loss: 0.933 | Acc: 66.434% \n",
      "[epoch:22, iter:10867] Loss: 0.933 | Acc: 66.428% \n",
      "[epoch:22, iter:10868] Loss: 0.933 | Acc: 66.435% \n",
      "[epoch:22, iter:10869] Loss: 0.933 | Acc: 66.444% \n",
      "[epoch:22, iter:10870] Loss: 0.933 | Acc: 66.435% \n",
      "[epoch:22, iter:10871] Loss: 0.933 | Acc: 66.434% \n",
      "[epoch:22, iter:10872] Loss: 0.933 | Acc: 66.430% \n",
      "[epoch:22, iter:10873] Loss: 0.933 | Acc: 66.413% \n",
      "[epoch:22, iter:10874] Loss: 0.932 | Acc: 66.438% \n",
      "[epoch:22, iter:10875] Loss: 0.932 | Acc: 66.451% \n",
      "[epoch:22, iter:10876] Loss: 0.932 | Acc: 66.452% \n",
      "[epoch:22, iter:10877] Loss: 0.933 | Acc: 66.446% \n",
      "[epoch:22, iter:10878] Loss: 0.933 | Acc: 66.439% \n",
      "[epoch:22, iter:10879] Loss: 0.932 | Acc: 66.438% \n",
      "[epoch:22, iter:10880] Loss: 0.932 | Acc: 66.458% \n",
      "[epoch:22, iter:10881] Loss: 0.932 | Acc: 66.465% \n",
      "[epoch:22, iter:10882] Loss: 0.932 | Acc: 66.487% \n",
      "[epoch:22, iter:10883] Loss: 0.931 | Acc: 66.499% \n",
      "[epoch:22, iter:10884] Loss: 0.932 | Acc: 66.487% \n",
      "[epoch:22, iter:10885] Loss: 0.932 | Acc: 66.496% \n",
      "[epoch:22, iter:10886] Loss: 0.932 | Acc: 66.484% \n",
      "[epoch:22, iter:10887] Loss: 0.932 | Acc: 66.475% \n",
      "[epoch:22, iter:10888] Loss: 0.932 | Acc: 66.466% \n",
      "[epoch:22, iter:10889] Loss: 0.932 | Acc: 66.468% \n",
      "[epoch:22, iter:10890] Loss: 0.931 | Acc: 66.479% \n",
      "[epoch:22, iter:10891] Loss: 0.931 | Acc: 66.488% \n",
      "[epoch:22, iter:10892] Loss: 0.931 | Acc: 66.497% \n",
      "[epoch:22, iter:10893] Loss: 0.931 | Acc: 66.496% \n",
      "[epoch:22, iter:10894] Loss: 0.932 | Acc: 66.477% \n",
      "[epoch:22, iter:10895] Loss: 0.932 | Acc: 66.476% \n",
      "[epoch:22, iter:10896] Loss: 0.932 | Acc: 66.482% \n",
      "[epoch:22, iter:10897] Loss: 0.932 | Acc: 66.484% \n",
      "[epoch:22, iter:10898] Loss: 0.932 | Acc: 66.482% \n",
      "[epoch:22, iter:10899] Loss: 0.932 | Acc: 66.496% \n",
      "[epoch:22, iter:10900] Loss: 0.932 | Acc: 66.480% \n",
      "[epoch:22, iter:10901] Loss: 0.932 | Acc: 66.489% \n",
      "[epoch:22, iter:10902] Loss: 0.932 | Acc: 66.478% \n",
      "[epoch:22, iter:10903] Loss: 0.932 | Acc: 66.481% \n",
      "[epoch:22, iter:10904] Loss: 0.932 | Acc: 66.458% \n",
      "[epoch:22, iter:10905] Loss: 0.932 | Acc: 66.464% \n",
      "[epoch:22, iter:10906] Loss: 0.932 | Acc: 66.468% \n",
      "[epoch:22, iter:10907] Loss: 0.932 | Acc: 66.469% \n",
      "[epoch:22, iter:10908] Loss: 0.932 | Acc: 66.466% \n",
      "[epoch:22, iter:10909] Loss: 0.933 | Acc: 66.430% \n",
      "[epoch:22, iter:10910] Loss: 0.933 | Acc: 66.427% \n",
      "[epoch:22, iter:10911] Loss: 0.932 | Acc: 66.433% \n",
      "[epoch:22, iter:10912] Loss: 0.932 | Acc: 66.442% \n",
      "[epoch:22, iter:10913] Loss: 0.932 | Acc: 66.443% \n",
      "[epoch:22, iter:10914] Loss: 0.932 | Acc: 66.435% \n",
      "[epoch:22, iter:10915] Loss: 0.932 | Acc: 66.455% \n",
      "[epoch:22, iter:10916] Loss: 0.931 | Acc: 66.486% \n",
      "[epoch:22, iter:10917] Loss: 0.931 | Acc: 66.482% \n",
      "[epoch:22, iter:10918] Loss: 0.931 | Acc: 66.486% \n",
      "[epoch:22, iter:10919] Loss: 0.931 | Acc: 66.475% \n",
      "[epoch:22, iter:10920] Loss: 0.931 | Acc: 66.467% \n",
      "[epoch:22, iter:10921] Loss: 0.931 | Acc: 66.473% \n",
      "[epoch:22, iter:10922] Loss: 0.930 | Acc: 66.483% \n",
      "[epoch:22, iter:10923] Loss: 0.930 | Acc: 66.478% \n",
      "[epoch:22, iter:10924] Loss: 0.931 | Acc: 66.474% \n",
      "[epoch:22, iter:10925] Loss: 0.931 | Acc: 66.475% \n",
      "[epoch:22, iter:10926] Loss: 0.931 | Acc: 66.446% \n",
      "[epoch:22, iter:10927] Loss: 0.931 | Acc: 66.440% \n",
      "[epoch:22, iter:10928] Loss: 0.931 | Acc: 66.435% \n",
      "[epoch:22, iter:10929] Loss: 0.932 | Acc: 66.422% \n",
      "[epoch:22, iter:10930] Loss: 0.932 | Acc: 66.409% \n",
      "[epoch:22, iter:10931] Loss: 0.932 | Acc: 66.408% \n",
      "[epoch:22, iter:10932] Loss: 0.932 | Acc: 66.396% \n",
      "[epoch:22, iter:10933] Loss: 0.932 | Acc: 66.390% \n",
      "[epoch:22, iter:10934] Loss: 0.932 | Acc: 66.399% \n",
      "[epoch:22, iter:10935] Loss: 0.932 | Acc: 66.405% \n",
      "[epoch:22, iter:10936] Loss: 0.932 | Acc: 66.397% \n",
      "[epoch:22, iter:10937] Loss: 0.932 | Acc: 66.389% \n",
      "[epoch:22, iter:10938] Loss: 0.933 | Acc: 66.379% \n",
      "[epoch:22, iter:10939] Loss: 0.933 | Acc: 66.369% \n",
      "[epoch:22, iter:10940] Loss: 0.933 | Acc: 66.364% \n",
      "[epoch:22, iter:10941] Loss: 0.933 | Acc: 66.367% \n",
      "[epoch:22, iter:10942] Loss: 0.932 | Acc: 66.378% \n",
      "[epoch:22, iter:10943] Loss: 0.933 | Acc: 66.372% \n",
      "[epoch:22, iter:10944] Loss: 0.932 | Acc: 66.383% \n",
      "[epoch:22, iter:10945] Loss: 0.932 | Acc: 66.389% \n",
      "[epoch:22, iter:10946] Loss: 0.932 | Acc: 66.388% \n",
      "[epoch:22, iter:10947] Loss: 0.932 | Acc: 66.403% \n",
      "[epoch:22, iter:10948] Loss: 0.932 | Acc: 66.397% \n",
      "[epoch:22, iter:10949] Loss: 0.932 | Acc: 66.396% \n",
      "[epoch:22, iter:10950] Loss: 0.932 | Acc: 66.404% \n",
      "[epoch:22, iter:10951] Loss: 0.932 | Acc: 66.417% \n",
      "[epoch:22, iter:10952] Loss: 0.932 | Acc: 66.427% \n",
      "[epoch:22, iter:10953] Loss: 0.932 | Acc: 66.435% \n",
      "[epoch:22, iter:10954] Loss: 0.932 | Acc: 66.436% \n",
      "[epoch:22, iter:10955] Loss: 0.932 | Acc: 66.437% \n",
      "[epoch:22, iter:10956] Loss: 0.932 | Acc: 66.436% \n",
      "[epoch:22, iter:10957] Loss: 0.932 | Acc: 66.442% \n",
      "[epoch:22, iter:10958] Loss: 0.932 | Acc: 66.432% \n",
      "[epoch:22, iter:10959] Loss: 0.932 | Acc: 66.442% \n",
      "[epoch:22, iter:10960] Loss: 0.932 | Acc: 66.446% \n",
      "[epoch:22, iter:10961] Loss: 0.932 | Acc: 66.456% \n",
      "[epoch:22, iter:10962] Loss: 0.932 | Acc: 66.457% \n",
      "[epoch:22, iter:10963] Loss: 0.931 | Acc: 66.473% \n",
      "[epoch:22, iter:10964] Loss: 0.931 | Acc: 66.463% \n",
      "[epoch:22, iter:10965] Loss: 0.931 | Acc: 66.473% \n",
      "[epoch:22, iter:10966] Loss: 0.931 | Acc: 66.479% \n",
      "[epoch:22, iter:10967] Loss: 0.931 | Acc: 66.490% \n",
      "[epoch:22, iter:10968] Loss: 0.931 | Acc: 66.485% \n",
      "[epoch:22, iter:10969] Loss: 0.931 | Acc: 66.486% \n",
      "[epoch:22, iter:10970] Loss: 0.931 | Acc: 66.487% \n",
      "[epoch:22, iter:10971] Loss: 0.931 | Acc: 66.484% \n",
      "[epoch:22, iter:10972] Loss: 0.931 | Acc: 66.487% \n",
      "[epoch:22, iter:10973] Loss: 0.931 | Acc: 66.478% \n",
      "[epoch:22, iter:10974] Loss: 0.931 | Acc: 66.483% \n",
      "[epoch:22, iter:10975] Loss: 0.931 | Acc: 66.503% \n",
      "[epoch:22, iter:10976] Loss: 0.931 | Acc: 66.498% \n",
      "[epoch:22, iter:10977] Loss: 0.931 | Acc: 66.486% \n",
      "[epoch:22, iter:10978] Loss: 0.931 | Acc: 66.471% \n",
      "[epoch:22, iter:10979] Loss: 0.931 | Acc: 66.470% \n",
      "[epoch:22, iter:10980] Loss: 0.931 | Acc: 66.477% \n",
      "[epoch:22, iter:10981] Loss: 0.931 | Acc: 66.478% \n",
      "[epoch:22, iter:10982] Loss: 0.932 | Acc: 66.469% \n",
      "[epoch:22, iter:10983] Loss: 0.932 | Acc: 66.470% \n",
      "[epoch:22, iter:10984] Loss: 0.932 | Acc: 66.471% \n",
      "[epoch:22, iter:10985] Loss: 0.932 | Acc: 66.474% \n",
      "[epoch:22, iter:10986] Loss: 0.931 | Acc: 66.488% \n",
      "[epoch:22, iter:10987] Loss: 0.931 | Acc: 66.499% \n",
      "[epoch:22, iter:10988] Loss: 0.931 | Acc: 66.516% \n",
      "[epoch:22, iter:10989] Loss: 0.931 | Acc: 66.524% \n",
      "[epoch:22, iter:10990] Loss: 0.930 | Acc: 66.531% \n",
      "[epoch:22, iter:10991] Loss: 0.930 | Acc: 66.527% \n",
      "[epoch:22, iter:10992] Loss: 0.930 | Acc: 66.522% \n",
      "[epoch:22, iter:10993] Loss: 0.930 | Acc: 66.531% \n",
      "[epoch:22, iter:10994] Loss: 0.930 | Acc: 66.551% \n",
      "[epoch:22, iter:10995] Loss: 0.930 | Acc: 66.564% \n",
      "[epoch:22, iter:10996] Loss: 0.930 | Acc: 66.577% \n",
      "[epoch:22, iter:10997] Loss: 0.930 | Acc: 66.575% \n",
      "[epoch:22, iter:10998] Loss: 0.930 | Acc: 66.574% \n",
      "[epoch:22, iter:10999] Loss: 0.930 | Acc: 66.567% \n",
      "[epoch:22, iter:11000] Loss: 0.930 | Acc: 66.560% \n",
      "Waiting Test...\n",
      "Test's ac is: 63.480%\n",
      "\n",
      "Epoch: 23\n",
      "[epoch:23, iter:11001] Loss: 0.977 | Acc: 67.000% \n",
      "[epoch:23, iter:11002] Loss: 1.001 | Acc: 66.500% \n",
      "[epoch:23, iter:11003] Loss: 0.970 | Acc: 67.000% \n",
      "[epoch:23, iter:11004] Loss: 1.017 | Acc: 65.750% \n",
      "[epoch:23, iter:11005] Loss: 0.993 | Acc: 66.800% \n",
      "[epoch:23, iter:11006] Loss: 0.995 | Acc: 66.000% \n",
      "[epoch:23, iter:11007] Loss: 0.992 | Acc: 65.714% \n",
      "[epoch:23, iter:11008] Loss: 0.997 | Acc: 64.875% \n",
      "[epoch:23, iter:11009] Loss: 0.990 | Acc: 65.444% \n",
      "[epoch:23, iter:11010] Loss: 0.996 | Acc: 65.100% \n",
      "[epoch:23, iter:11011] Loss: 0.987 | Acc: 65.455% \n",
      "[epoch:23, iter:11012] Loss: 0.984 | Acc: 65.667% \n",
      "[epoch:23, iter:11013] Loss: 0.974 | Acc: 65.923% \n",
      "[epoch:23, iter:11014] Loss: 0.971 | Acc: 65.786% \n",
      "[epoch:23, iter:11015] Loss: 0.973 | Acc: 65.600% \n",
      "[epoch:23, iter:11016] Loss: 0.967 | Acc: 65.938% \n",
      "[epoch:23, iter:11017] Loss: 0.961 | Acc: 66.059% \n",
      "[epoch:23, iter:11018] Loss: 0.957 | Acc: 66.167% \n",
      "[epoch:23, iter:11019] Loss: 0.964 | Acc: 66.158% \n",
      "[epoch:23, iter:11020] Loss: 0.962 | Acc: 65.900% \n",
      "[epoch:23, iter:11021] Loss: 0.961 | Acc: 65.857% \n",
      "[epoch:23, iter:11022] Loss: 0.956 | Acc: 66.136% \n",
      "[epoch:23, iter:11023] Loss: 0.955 | Acc: 66.087% \n",
      "[epoch:23, iter:11024] Loss: 0.957 | Acc: 65.875% \n",
      "[epoch:23, iter:11025] Loss: 0.957 | Acc: 65.920% \n",
      "[epoch:23, iter:11026] Loss: 0.958 | Acc: 66.000% \n",
      "[epoch:23, iter:11027] Loss: 0.960 | Acc: 65.889% \n",
      "[epoch:23, iter:11028] Loss: 0.959 | Acc: 65.929% \n",
      "[epoch:23, iter:11029] Loss: 0.958 | Acc: 65.897% \n",
      "[epoch:23, iter:11030] Loss: 0.955 | Acc: 65.867% \n",
      "[epoch:23, iter:11031] Loss: 0.951 | Acc: 66.032% \n",
      "[epoch:23, iter:11032] Loss: 0.952 | Acc: 66.000% \n",
      "[epoch:23, iter:11033] Loss: 0.946 | Acc: 66.152% \n",
      "[epoch:23, iter:11034] Loss: 0.945 | Acc: 66.147% \n",
      "[epoch:23, iter:11035] Loss: 0.947 | Acc: 66.114% \n",
      "[epoch:23, iter:11036] Loss: 0.948 | Acc: 66.139% \n",
      "[epoch:23, iter:11037] Loss: 0.950 | Acc: 66.135% \n",
      "[epoch:23, iter:11038] Loss: 0.949 | Acc: 66.132% \n",
      "[epoch:23, iter:11039] Loss: 0.948 | Acc: 66.128% \n",
      "[epoch:23, iter:11040] Loss: 0.948 | Acc: 66.075% \n",
      "[epoch:23, iter:11041] Loss: 0.947 | Acc: 66.146% \n",
      "[epoch:23, iter:11042] Loss: 0.949 | Acc: 66.000% \n",
      "[epoch:23, iter:11043] Loss: 0.949 | Acc: 65.884% \n",
      "[epoch:23, iter:11044] Loss: 0.948 | Acc: 65.932% \n",
      "[epoch:23, iter:11045] Loss: 0.946 | Acc: 66.022% \n",
      "[epoch:23, iter:11046] Loss: 0.948 | Acc: 65.978% \n",
      "[epoch:23, iter:11047] Loss: 0.947 | Acc: 66.021% \n",
      "[epoch:23, iter:11048] Loss: 0.946 | Acc: 66.167% \n",
      "[epoch:23, iter:11049] Loss: 0.944 | Acc: 66.245% \n",
      "[epoch:23, iter:11050] Loss: 0.948 | Acc: 66.140% \n",
      "[epoch:23, iter:11051] Loss: 0.946 | Acc: 66.157% \n",
      "[epoch:23, iter:11052] Loss: 0.946 | Acc: 66.115% \n",
      "[epoch:23, iter:11053] Loss: 0.947 | Acc: 66.019% \n",
      "[epoch:23, iter:11054] Loss: 0.948 | Acc: 65.963% \n",
      "[epoch:23, iter:11055] Loss: 0.944 | Acc: 66.073% \n",
      "[epoch:23, iter:11056] Loss: 0.942 | Acc: 66.107% \n",
      "[epoch:23, iter:11057] Loss: 0.943 | Acc: 66.123% \n",
      "[epoch:23, iter:11058] Loss: 0.941 | Acc: 66.172% \n",
      "[epoch:23, iter:11059] Loss: 0.942 | Acc: 66.068% \n",
      "[epoch:23, iter:11060] Loss: 0.940 | Acc: 66.200% \n",
      "[epoch:23, iter:11061] Loss: 0.937 | Acc: 66.311% \n",
      "[epoch:23, iter:11062] Loss: 0.934 | Acc: 66.435% \n",
      "[epoch:23, iter:11063] Loss: 0.932 | Acc: 66.460% \n",
      "[epoch:23, iter:11064] Loss: 0.932 | Acc: 66.500% \n",
      "[epoch:23, iter:11065] Loss: 0.934 | Acc: 66.431% \n",
      "[epoch:23, iter:11066] Loss: 0.937 | Acc: 66.333% \n",
      "[epoch:23, iter:11067] Loss: 0.935 | Acc: 66.388% \n",
      "[epoch:23, iter:11068] Loss: 0.933 | Acc: 66.500% \n",
      "[epoch:23, iter:11069] Loss: 0.930 | Acc: 66.536% \n",
      "[epoch:23, iter:11070] Loss: 0.930 | Acc: 66.529% \n",
      "[epoch:23, iter:11071] Loss: 0.930 | Acc: 66.535% \n",
      "[epoch:23, iter:11072] Loss: 0.930 | Acc: 66.528% \n",
      "[epoch:23, iter:11073] Loss: 0.930 | Acc: 66.507% \n",
      "[epoch:23, iter:11074] Loss: 0.930 | Acc: 66.514% \n",
      "[epoch:23, iter:11075] Loss: 0.930 | Acc: 66.493% \n",
      "[epoch:23, iter:11076] Loss: 0.931 | Acc: 66.526% \n",
      "[epoch:23, iter:11077] Loss: 0.931 | Acc: 66.532% \n",
      "[epoch:23, iter:11078] Loss: 0.929 | Acc: 66.590% \n",
      "[epoch:23, iter:11079] Loss: 0.928 | Acc: 66.633% \n",
      "[epoch:23, iter:11080] Loss: 0.927 | Acc: 66.675% \n",
      "[epoch:23, iter:11081] Loss: 0.926 | Acc: 66.691% \n",
      "[epoch:23, iter:11082] Loss: 0.926 | Acc: 66.683% \n",
      "[epoch:23, iter:11083] Loss: 0.928 | Acc: 66.663% \n",
      "[epoch:23, iter:11084] Loss: 0.929 | Acc: 66.560% \n",
      "[epoch:23, iter:11085] Loss: 0.929 | Acc: 66.588% \n",
      "[epoch:23, iter:11086] Loss: 0.928 | Acc: 66.651% \n",
      "[epoch:23, iter:11087] Loss: 0.929 | Acc: 66.575% \n",
      "[epoch:23, iter:11088] Loss: 0.927 | Acc: 66.682% \n",
      "[epoch:23, iter:11089] Loss: 0.928 | Acc: 66.652% \n",
      "[epoch:23, iter:11090] Loss: 0.929 | Acc: 66.644% \n",
      "[epoch:23, iter:11091] Loss: 0.928 | Acc: 66.659% \n",
      "[epoch:23, iter:11092] Loss: 0.928 | Acc: 66.609% \n",
      "[epoch:23, iter:11093] Loss: 0.929 | Acc: 66.613% \n",
      "[epoch:23, iter:11094] Loss: 0.929 | Acc: 66.543% \n",
      "[epoch:23, iter:11095] Loss: 0.927 | Acc: 66.537% \n",
      "[epoch:23, iter:11096] Loss: 0.930 | Acc: 66.552% \n",
      "[epoch:23, iter:11097] Loss: 0.931 | Acc: 66.567% \n",
      "[epoch:23, iter:11098] Loss: 0.929 | Acc: 66.653% \n",
      "[epoch:23, iter:11099] Loss: 0.928 | Acc: 66.687% \n",
      "[epoch:23, iter:11100] Loss: 0.927 | Acc: 66.700% \n",
      "[epoch:23, iter:11101] Loss: 0.926 | Acc: 66.733% \n",
      "[epoch:23, iter:11102] Loss: 0.926 | Acc: 66.725% \n",
      "[epoch:23, iter:11103] Loss: 0.926 | Acc: 66.728% \n",
      "[epoch:23, iter:11104] Loss: 0.925 | Acc: 66.760% \n",
      "[epoch:23, iter:11105] Loss: 0.923 | Acc: 66.838% \n",
      "[epoch:23, iter:11106] Loss: 0.924 | Acc: 66.840% \n",
      "[epoch:23, iter:11107] Loss: 0.923 | Acc: 66.841% \n",
      "[epoch:23, iter:11108] Loss: 0.923 | Acc: 66.824% \n",
      "[epoch:23, iter:11109] Loss: 0.923 | Acc: 66.844% \n",
      "[epoch:23, iter:11110] Loss: 0.923 | Acc: 66.818% \n",
      "[epoch:23, iter:11111] Loss: 0.922 | Acc: 66.811% \n",
      "[epoch:23, iter:11112] Loss: 0.920 | Acc: 66.893% \n",
      "[epoch:23, iter:11113] Loss: 0.919 | Acc: 66.885% \n",
      "[epoch:23, iter:11114] Loss: 0.919 | Acc: 66.833% \n",
      "[epoch:23, iter:11115] Loss: 0.919 | Acc: 66.835% \n",
      "[epoch:23, iter:11116] Loss: 0.919 | Acc: 66.793% \n",
      "[epoch:23, iter:11117] Loss: 0.919 | Acc: 66.803% \n",
      "[epoch:23, iter:11118] Loss: 0.920 | Acc: 66.788% \n",
      "[epoch:23, iter:11119] Loss: 0.922 | Acc: 66.739% \n",
      "[epoch:23, iter:11120] Loss: 0.924 | Acc: 66.700% \n",
      "[epoch:23, iter:11121] Loss: 0.924 | Acc: 66.727% \n",
      "[epoch:23, iter:11122] Loss: 0.923 | Acc: 66.754% \n",
      "[epoch:23, iter:11123] Loss: 0.922 | Acc: 66.772% \n",
      "[epoch:23, iter:11124] Loss: 0.923 | Acc: 66.774% \n",
      "[epoch:23, iter:11125] Loss: 0.923 | Acc: 66.752% \n",
      "[epoch:23, iter:11126] Loss: 0.922 | Acc: 66.794% \n",
      "[epoch:23, iter:11127] Loss: 0.921 | Acc: 66.795% \n",
      "[epoch:23, iter:11128] Loss: 0.922 | Acc: 66.789% \n",
      "[epoch:23, iter:11129] Loss: 0.922 | Acc: 66.806% \n",
      "[epoch:23, iter:11130] Loss: 0.921 | Acc: 66.838% \n",
      "[epoch:23, iter:11131] Loss: 0.922 | Acc: 66.771% \n",
      "[epoch:23, iter:11132] Loss: 0.923 | Acc: 66.720% \n",
      "[epoch:23, iter:11133] Loss: 0.924 | Acc: 66.699% \n",
      "[epoch:23, iter:11134] Loss: 0.922 | Acc: 66.791% \n",
      "[epoch:23, iter:11135] Loss: 0.923 | Acc: 66.763% \n",
      "[epoch:23, iter:11136] Loss: 0.922 | Acc: 66.779% \n",
      "[epoch:23, iter:11137] Loss: 0.921 | Acc: 66.788% \n",
      "[epoch:23, iter:11138] Loss: 0.922 | Acc: 66.819% \n",
      "[epoch:23, iter:11139] Loss: 0.920 | Acc: 66.892% \n",
      "[epoch:23, iter:11140] Loss: 0.921 | Acc: 66.879% \n",
      "[epoch:23, iter:11141] Loss: 0.920 | Acc: 66.901% \n",
      "[epoch:23, iter:11142] Loss: 0.920 | Acc: 66.930% \n",
      "[epoch:23, iter:11143] Loss: 0.922 | Acc: 66.860% \n",
      "[epoch:23, iter:11144] Loss: 0.922 | Acc: 66.840% \n",
      "[epoch:23, iter:11145] Loss: 0.921 | Acc: 66.841% \n",
      "[epoch:23, iter:11146] Loss: 0.922 | Acc: 66.849% \n",
      "[epoch:23, iter:11147] Loss: 0.920 | Acc: 66.905% \n",
      "[epoch:23, iter:11148] Loss: 0.919 | Acc: 66.919% \n",
      "[epoch:23, iter:11149] Loss: 0.920 | Acc: 66.913% \n",
      "[epoch:23, iter:11150] Loss: 0.919 | Acc: 66.927% \n",
      "[epoch:23, iter:11151] Loss: 0.920 | Acc: 66.927% \n",
      "[epoch:23, iter:11152] Loss: 0.919 | Acc: 66.954% \n",
      "[epoch:23, iter:11153] Loss: 0.920 | Acc: 66.908% \n",
      "[epoch:23, iter:11154] Loss: 0.921 | Acc: 66.877% \n",
      "[epoch:23, iter:11155] Loss: 0.921 | Acc: 66.897% \n",
      "[epoch:23, iter:11156] Loss: 0.922 | Acc: 66.878% \n",
      "[epoch:23, iter:11157] Loss: 0.921 | Acc: 66.879% \n",
      "[epoch:23, iter:11158] Loss: 0.922 | Acc: 66.873% \n",
      "[epoch:23, iter:11159] Loss: 0.922 | Acc: 66.887% \n",
      "[epoch:23, iter:11160] Loss: 0.922 | Acc: 66.906% \n",
      "[epoch:23, iter:11161] Loss: 0.922 | Acc: 66.882% \n",
      "[epoch:23, iter:11162] Loss: 0.923 | Acc: 66.870% \n",
      "[epoch:23, iter:11163] Loss: 0.922 | Acc: 66.883% \n",
      "[epoch:23, iter:11164] Loss: 0.922 | Acc: 66.902% \n",
      "[epoch:23, iter:11165] Loss: 0.921 | Acc: 66.921% \n",
      "[epoch:23, iter:11166] Loss: 0.919 | Acc: 66.988% \n",
      "[epoch:23, iter:11167] Loss: 0.920 | Acc: 66.970% \n",
      "[epoch:23, iter:11168] Loss: 0.919 | Acc: 66.988% \n",
      "[epoch:23, iter:11169] Loss: 0.920 | Acc: 66.935% \n",
      "[epoch:23, iter:11170] Loss: 0.921 | Acc: 66.912% \n",
      "[epoch:23, iter:11171] Loss: 0.920 | Acc: 66.912% \n",
      "[epoch:23, iter:11172] Loss: 0.920 | Acc: 66.930% \n",
      "[epoch:23, iter:11173] Loss: 0.919 | Acc: 66.936% \n",
      "[epoch:23, iter:11174] Loss: 0.919 | Acc: 66.954% \n",
      "[epoch:23, iter:11175] Loss: 0.918 | Acc: 66.983% \n",
      "[epoch:23, iter:11176] Loss: 0.918 | Acc: 66.983% \n",
      "[epoch:23, iter:11177] Loss: 0.919 | Acc: 67.000% \n",
      "[epoch:23, iter:11178] Loss: 0.920 | Acc: 66.966% \n",
      "[epoch:23, iter:11179] Loss: 0.921 | Acc: 66.916% \n",
      "[epoch:23, iter:11180] Loss: 0.922 | Acc: 66.861% \n",
      "[epoch:23, iter:11181] Loss: 0.924 | Acc: 66.812% \n",
      "[epoch:23, iter:11182] Loss: 0.924 | Acc: 66.830% \n",
      "[epoch:23, iter:11183] Loss: 0.924 | Acc: 66.814% \n",
      "[epoch:23, iter:11184] Loss: 0.924 | Acc: 66.821% \n",
      "[epoch:23, iter:11185] Loss: 0.925 | Acc: 66.800% \n",
      "[epoch:23, iter:11186] Loss: 0.924 | Acc: 66.796% \n",
      "[epoch:23, iter:11187] Loss: 0.924 | Acc: 66.797% \n",
      "[epoch:23, iter:11188] Loss: 0.924 | Acc: 66.793% \n",
      "[epoch:23, iter:11189] Loss: 0.924 | Acc: 66.799% \n",
      "[epoch:23, iter:11190] Loss: 0.924 | Acc: 66.795% \n",
      "[epoch:23, iter:11191] Loss: 0.924 | Acc: 66.827% \n",
      "[epoch:23, iter:11192] Loss: 0.923 | Acc: 66.849% \n",
      "[epoch:23, iter:11193] Loss: 0.924 | Acc: 66.808% \n",
      "[epoch:23, iter:11194] Loss: 0.924 | Acc: 66.814% \n",
      "[epoch:23, iter:11195] Loss: 0.925 | Acc: 66.815% \n",
      "[epoch:23, iter:11196] Loss: 0.925 | Acc: 66.816% \n",
      "[epoch:23, iter:11197] Loss: 0.925 | Acc: 66.817% \n",
      "[epoch:23, iter:11198] Loss: 0.926 | Acc: 66.793% \n",
      "[epoch:23, iter:11199] Loss: 0.926 | Acc: 66.784% \n",
      "[epoch:23, iter:11200] Loss: 0.926 | Acc: 66.760% \n",
      "[epoch:23, iter:11201] Loss: 0.927 | Acc: 66.756% \n",
      "[epoch:23, iter:11202] Loss: 0.928 | Acc: 66.752% \n",
      "[epoch:23, iter:11203] Loss: 0.928 | Acc: 66.744% \n",
      "[epoch:23, iter:11204] Loss: 0.927 | Acc: 66.755% \n",
      "[epoch:23, iter:11205] Loss: 0.929 | Acc: 66.707% \n",
      "[epoch:23, iter:11206] Loss: 0.928 | Acc: 66.738% \n",
      "[epoch:23, iter:11207] Loss: 0.928 | Acc: 66.754% \n",
      "[epoch:23, iter:11208] Loss: 0.928 | Acc: 66.750% \n",
      "[epoch:23, iter:11209] Loss: 0.928 | Acc: 66.742% \n",
      "[epoch:23, iter:11210] Loss: 0.928 | Acc: 66.738% \n",
      "[epoch:23, iter:11211] Loss: 0.928 | Acc: 66.758% \n",
      "[epoch:23, iter:11212] Loss: 0.928 | Acc: 66.717% \n",
      "[epoch:23, iter:11213] Loss: 0.929 | Acc: 66.695% \n",
      "[epoch:23, iter:11214] Loss: 0.928 | Acc: 66.743% \n",
      "[epoch:23, iter:11215] Loss: 0.928 | Acc: 66.735% \n",
      "[epoch:23, iter:11216] Loss: 0.927 | Acc: 66.745% \n",
      "[epoch:23, iter:11217] Loss: 0.927 | Acc: 66.737% \n",
      "[epoch:23, iter:11218] Loss: 0.928 | Acc: 66.748% \n",
      "[epoch:23, iter:11219] Loss: 0.928 | Acc: 66.767% \n",
      "[epoch:23, iter:11220] Loss: 0.927 | Acc: 66.782% \n",
      "[epoch:23, iter:11221] Loss: 0.928 | Acc: 66.783% \n",
      "[epoch:23, iter:11222] Loss: 0.928 | Acc: 66.788% \n",
      "[epoch:23, iter:11223] Loss: 0.928 | Acc: 66.780% \n",
      "[epoch:23, iter:11224] Loss: 0.928 | Acc: 66.772% \n",
      "[epoch:23, iter:11225] Loss: 0.929 | Acc: 66.773% \n",
      "[epoch:23, iter:11226] Loss: 0.928 | Acc: 66.757% \n",
      "[epoch:23, iter:11227] Loss: 0.928 | Acc: 66.762% \n",
      "[epoch:23, iter:11228] Loss: 0.929 | Acc: 66.781% \n",
      "[epoch:23, iter:11229] Loss: 0.929 | Acc: 66.760% \n",
      "[epoch:23, iter:11230] Loss: 0.930 | Acc: 66.730% \n",
      "[epoch:23, iter:11231] Loss: 0.930 | Acc: 66.723% \n",
      "[epoch:23, iter:11232] Loss: 0.930 | Acc: 66.720% \n",
      "[epoch:23, iter:11233] Loss: 0.930 | Acc: 66.742% \n",
      "[epoch:23, iter:11234] Loss: 0.930 | Acc: 66.744% \n",
      "[epoch:23, iter:11235] Loss: 0.929 | Acc: 66.770% \n",
      "[epoch:23, iter:11236] Loss: 0.929 | Acc: 66.771% \n",
      "[epoch:23, iter:11237] Loss: 0.929 | Acc: 66.819% \n",
      "[epoch:23, iter:11238] Loss: 0.929 | Acc: 66.786% \n",
      "[epoch:23, iter:11239] Loss: 0.929 | Acc: 66.791% \n",
      "[epoch:23, iter:11240] Loss: 0.929 | Acc: 66.817% \n",
      "[epoch:23, iter:11241] Loss: 0.929 | Acc: 66.788% \n",
      "[epoch:23, iter:11242] Loss: 0.928 | Acc: 66.814% \n",
      "[epoch:23, iter:11243] Loss: 0.929 | Acc: 66.811% \n",
      "[epoch:23, iter:11244] Loss: 0.929 | Acc: 66.803% \n",
      "[epoch:23, iter:11245] Loss: 0.928 | Acc: 66.820% \n",
      "[epoch:23, iter:11246] Loss: 0.928 | Acc: 66.841% \n",
      "[epoch:23, iter:11247] Loss: 0.927 | Acc: 66.870% \n",
      "[epoch:23, iter:11248] Loss: 0.927 | Acc: 66.875% \n",
      "[epoch:23, iter:11249] Loss: 0.926 | Acc: 66.884% \n",
      "[epoch:23, iter:11250] Loss: 0.926 | Acc: 66.880% \n",
      "[epoch:23, iter:11251] Loss: 0.926 | Acc: 66.920% \n",
      "[epoch:23, iter:11252] Loss: 0.926 | Acc: 66.921% \n",
      "[epoch:23, iter:11253] Loss: 0.926 | Acc: 66.913% \n",
      "[epoch:23, iter:11254] Loss: 0.926 | Acc: 66.909% \n",
      "[epoch:23, iter:11255] Loss: 0.927 | Acc: 66.918% \n",
      "[epoch:23, iter:11256] Loss: 0.926 | Acc: 66.922% \n",
      "[epoch:23, iter:11257] Loss: 0.926 | Acc: 66.922% \n",
      "[epoch:23, iter:11258] Loss: 0.926 | Acc: 66.919% \n",
      "[epoch:23, iter:11259] Loss: 0.925 | Acc: 66.961% \n",
      "[epoch:23, iter:11260] Loss: 0.926 | Acc: 66.927% \n",
      "[epoch:23, iter:11261] Loss: 0.926 | Acc: 66.927% \n",
      "[epoch:23, iter:11262] Loss: 0.926 | Acc: 66.912% \n",
      "[epoch:23, iter:11263] Loss: 0.926 | Acc: 66.905% \n",
      "[epoch:23, iter:11264] Loss: 0.926 | Acc: 66.905% \n",
      "[epoch:23, iter:11265] Loss: 0.926 | Acc: 66.891% \n",
      "[epoch:23, iter:11266] Loss: 0.927 | Acc: 66.868% \n",
      "[epoch:23, iter:11267] Loss: 0.926 | Acc: 66.884% \n",
      "[epoch:23, iter:11268] Loss: 0.927 | Acc: 66.858% \n",
      "[epoch:23, iter:11269] Loss: 0.927 | Acc: 66.836% \n",
      "[epoch:23, iter:11270] Loss: 0.926 | Acc: 66.863% \n",
      "[epoch:23, iter:11271] Loss: 0.928 | Acc: 66.815% \n",
      "[epoch:23, iter:11272] Loss: 0.928 | Acc: 66.838% \n",
      "[epoch:23, iter:11273] Loss: 0.928 | Acc: 66.846% \n",
      "[epoch:23, iter:11274] Loss: 0.928 | Acc: 66.828% \n",
      "[epoch:23, iter:11275] Loss: 0.927 | Acc: 66.844% \n",
      "[epoch:23, iter:11276] Loss: 0.929 | Acc: 66.797% \n",
      "[epoch:23, iter:11277] Loss: 0.928 | Acc: 66.791% \n",
      "[epoch:23, iter:11278] Loss: 0.928 | Acc: 66.820% \n",
      "[epoch:23, iter:11279] Loss: 0.927 | Acc: 66.832% \n",
      "[epoch:23, iter:11280] Loss: 0.927 | Acc: 66.825% \n",
      "[epoch:23, iter:11281] Loss: 0.927 | Acc: 66.854% \n",
      "[epoch:23, iter:11282] Loss: 0.927 | Acc: 66.851% \n",
      "[epoch:23, iter:11283] Loss: 0.927 | Acc: 66.869% \n",
      "[epoch:23, iter:11284] Loss: 0.926 | Acc: 66.891% \n",
      "[epoch:23, iter:11285] Loss: 0.926 | Acc: 66.870% \n",
      "[epoch:23, iter:11286] Loss: 0.926 | Acc: 66.871% \n",
      "[epoch:23, iter:11287] Loss: 0.926 | Acc: 66.868% \n",
      "[epoch:23, iter:11288] Loss: 0.926 | Acc: 66.882% \n",
      "[epoch:23, iter:11289] Loss: 0.926 | Acc: 66.886% \n",
      "[epoch:23, iter:11290] Loss: 0.926 | Acc: 66.890% \n",
      "[epoch:23, iter:11291] Loss: 0.925 | Acc: 66.911% \n",
      "[epoch:23, iter:11292] Loss: 0.926 | Acc: 66.887% \n",
      "[epoch:23, iter:11293] Loss: 0.925 | Acc: 66.908% \n",
      "[epoch:23, iter:11294] Loss: 0.924 | Acc: 66.925% \n",
      "[epoch:23, iter:11295] Loss: 0.926 | Acc: 66.871% \n",
      "[epoch:23, iter:11296] Loss: 0.926 | Acc: 66.834% \n",
      "[epoch:23, iter:11297] Loss: 0.926 | Acc: 66.852% \n",
      "[epoch:23, iter:11298] Loss: 0.926 | Acc: 66.856% \n",
      "[epoch:23, iter:11299] Loss: 0.925 | Acc: 66.876% \n",
      "[epoch:23, iter:11300] Loss: 0.925 | Acc: 66.873% \n",
      "[epoch:23, iter:11301] Loss: 0.925 | Acc: 66.887% \n",
      "[epoch:23, iter:11302] Loss: 0.925 | Acc: 66.871% \n",
      "[epoch:23, iter:11303] Loss: 0.924 | Acc: 66.911% \n",
      "[epoch:23, iter:11304] Loss: 0.925 | Acc: 66.905% \n",
      "[epoch:23, iter:11305] Loss: 0.925 | Acc: 66.898% \n",
      "[epoch:23, iter:11306] Loss: 0.925 | Acc: 66.889% \n",
      "[epoch:23, iter:11307] Loss: 0.925 | Acc: 66.883% \n",
      "[epoch:23, iter:11308] Loss: 0.925 | Acc: 66.883% \n",
      "[epoch:23, iter:11309] Loss: 0.925 | Acc: 66.887% \n",
      "[epoch:23, iter:11310] Loss: 0.925 | Acc: 66.881% \n",
      "[epoch:23, iter:11311] Loss: 0.924 | Acc: 66.894% \n",
      "[epoch:23, iter:11312] Loss: 0.926 | Acc: 66.862% \n",
      "[epoch:23, iter:11313] Loss: 0.926 | Acc: 66.863% \n",
      "[epoch:23, iter:11314] Loss: 0.926 | Acc: 66.838% \n",
      "[epoch:23, iter:11315] Loss: 0.926 | Acc: 66.857% \n",
      "[epoch:23, iter:11316] Loss: 0.926 | Acc: 66.829% \n",
      "[epoch:23, iter:11317] Loss: 0.926 | Acc: 66.836% \n",
      "[epoch:23, iter:11318] Loss: 0.926 | Acc: 66.849% \n",
      "[epoch:23, iter:11319] Loss: 0.926 | Acc: 66.834% \n",
      "[epoch:23, iter:11320] Loss: 0.926 | Acc: 66.844% \n",
      "[epoch:23, iter:11321] Loss: 0.927 | Acc: 66.822% \n",
      "[epoch:23, iter:11322] Loss: 0.926 | Acc: 66.829% \n",
      "[epoch:23, iter:11323] Loss: 0.926 | Acc: 66.830% \n",
      "[epoch:23, iter:11324] Loss: 0.926 | Acc: 66.815% \n",
      "[epoch:23, iter:11325] Loss: 0.927 | Acc: 66.822% \n",
      "[epoch:23, iter:11326] Loss: 0.927 | Acc: 66.834% \n",
      "[epoch:23, iter:11327] Loss: 0.926 | Acc: 66.832% \n",
      "[epoch:23, iter:11328] Loss: 0.926 | Acc: 66.838% \n",
      "[epoch:23, iter:11329] Loss: 0.926 | Acc: 66.842% \n",
      "[epoch:23, iter:11330] Loss: 0.925 | Acc: 66.845% \n",
      "[epoch:23, iter:11331] Loss: 0.926 | Acc: 66.825% \n",
      "[epoch:23, iter:11332] Loss: 0.925 | Acc: 66.837% \n",
      "[epoch:23, iter:11333] Loss: 0.926 | Acc: 66.817% \n",
      "[epoch:23, iter:11334] Loss: 0.925 | Acc: 66.826% \n",
      "[epoch:23, iter:11335] Loss: 0.925 | Acc: 66.824% \n",
      "[epoch:23, iter:11336] Loss: 0.925 | Acc: 66.833% \n",
      "[epoch:23, iter:11337] Loss: 0.925 | Acc: 66.840% \n",
      "[epoch:23, iter:11338] Loss: 0.925 | Acc: 66.861% \n",
      "[epoch:23, iter:11339] Loss: 0.925 | Acc: 66.861% \n",
      "[epoch:23, iter:11340] Loss: 0.925 | Acc: 66.835% \n",
      "[epoch:23, iter:11341] Loss: 0.925 | Acc: 66.836% \n",
      "[epoch:23, iter:11342] Loss: 0.926 | Acc: 66.836% \n",
      "[epoch:23, iter:11343] Loss: 0.926 | Acc: 66.845% \n",
      "[epoch:23, iter:11344] Loss: 0.926 | Acc: 66.831% \n",
      "[epoch:23, iter:11345] Loss: 0.926 | Acc: 66.826% \n",
      "[epoch:23, iter:11346] Loss: 0.925 | Acc: 66.855% \n",
      "[epoch:23, iter:11347] Loss: 0.925 | Acc: 66.865% \n",
      "[epoch:23, iter:11348] Loss: 0.925 | Acc: 66.862% \n",
      "[epoch:23, iter:11349] Loss: 0.925 | Acc: 66.874% \n",
      "[epoch:23, iter:11350] Loss: 0.925 | Acc: 66.860% \n",
      "[epoch:23, iter:11351] Loss: 0.925 | Acc: 66.855% \n",
      "[epoch:23, iter:11352] Loss: 0.926 | Acc: 66.838% \n",
      "[epoch:23, iter:11353] Loss: 0.926 | Acc: 66.841% \n",
      "[epoch:23, iter:11354] Loss: 0.926 | Acc: 66.839% \n",
      "[epoch:23, iter:11355] Loss: 0.926 | Acc: 66.834% \n",
      "[epoch:23, iter:11356] Loss: 0.925 | Acc: 66.848% \n",
      "[epoch:23, iter:11357] Loss: 0.926 | Acc: 66.840% \n",
      "[epoch:23, iter:11358] Loss: 0.925 | Acc: 66.838% \n",
      "[epoch:23, iter:11359] Loss: 0.926 | Acc: 66.830% \n",
      "[epoch:23, iter:11360] Loss: 0.926 | Acc: 66.847% \n",
      "[epoch:23, iter:11361] Loss: 0.926 | Acc: 66.806% \n",
      "[epoch:23, iter:11362] Loss: 0.926 | Acc: 66.812% \n",
      "[epoch:23, iter:11363] Loss: 0.926 | Acc: 66.813% \n",
      "[epoch:23, iter:11364] Loss: 0.925 | Acc: 66.827% \n",
      "[epoch:23, iter:11365] Loss: 0.924 | Acc: 66.847% \n",
      "[epoch:23, iter:11366] Loss: 0.924 | Acc: 66.839% \n",
      "[epoch:23, iter:11367] Loss: 0.924 | Acc: 66.864% \n",
      "[epoch:23, iter:11368] Loss: 0.924 | Acc: 66.851% \n",
      "[epoch:23, iter:11369] Loss: 0.924 | Acc: 66.870% \n",
      "[epoch:23, iter:11370] Loss: 0.923 | Acc: 66.884% \n",
      "[epoch:23, iter:11371] Loss: 0.923 | Acc: 66.892% \n",
      "[epoch:23, iter:11372] Loss: 0.923 | Acc: 66.898% \n",
      "[epoch:23, iter:11373] Loss: 0.923 | Acc: 66.885% \n",
      "[epoch:23, iter:11374] Loss: 0.923 | Acc: 66.885% \n",
      "[epoch:23, iter:11375] Loss: 0.922 | Acc: 66.891% \n",
      "[epoch:23, iter:11376] Loss: 0.923 | Acc: 66.894% \n",
      "[epoch:23, iter:11377] Loss: 0.922 | Acc: 66.899% \n",
      "[epoch:23, iter:11378] Loss: 0.923 | Acc: 66.897% \n",
      "[epoch:23, iter:11379] Loss: 0.923 | Acc: 66.905% \n",
      "[epoch:23, iter:11380] Loss: 0.922 | Acc: 66.926% \n",
      "[epoch:23, iter:11381] Loss: 0.922 | Acc: 66.932% \n",
      "[epoch:23, iter:11382] Loss: 0.922 | Acc: 66.921% \n",
      "[epoch:23, iter:11383] Loss: 0.922 | Acc: 66.916% \n",
      "[epoch:23, iter:11384] Loss: 0.922 | Acc: 66.904% \n",
      "[epoch:23, iter:11385] Loss: 0.922 | Acc: 66.914% \n",
      "[epoch:23, iter:11386] Loss: 0.922 | Acc: 66.927% \n",
      "[epoch:23, iter:11387] Loss: 0.921 | Acc: 66.943% \n",
      "[epoch:23, iter:11388] Loss: 0.921 | Acc: 66.959% \n",
      "[epoch:23, iter:11389] Loss: 0.921 | Acc: 66.979% \n",
      "[epoch:23, iter:11390] Loss: 0.921 | Acc: 66.990% \n",
      "[epoch:23, iter:11391] Loss: 0.921 | Acc: 66.997% \n",
      "[epoch:23, iter:11392] Loss: 0.921 | Acc: 66.992% \n",
      "[epoch:23, iter:11393] Loss: 0.921 | Acc: 66.992% \n",
      "[epoch:23, iter:11394] Loss: 0.921 | Acc: 66.980% \n",
      "[epoch:23, iter:11395] Loss: 0.921 | Acc: 66.987% \n",
      "[epoch:23, iter:11396] Loss: 0.921 | Acc: 66.985% \n",
      "[epoch:23, iter:11397] Loss: 0.921 | Acc: 66.990% \n",
      "[epoch:23, iter:11398] Loss: 0.921 | Acc: 66.985% \n",
      "[epoch:23, iter:11399] Loss: 0.921 | Acc: 66.985% \n",
      "[epoch:23, iter:11400] Loss: 0.921 | Acc: 66.990% \n",
      "[epoch:23, iter:11401] Loss: 0.921 | Acc: 66.993% \n",
      "[epoch:23, iter:11402] Loss: 0.921 | Acc: 66.983% \n",
      "[epoch:23, iter:11403] Loss: 0.921 | Acc: 66.978% \n",
      "[epoch:23, iter:11404] Loss: 0.921 | Acc: 66.965% \n",
      "[epoch:23, iter:11405] Loss: 0.922 | Acc: 66.953% \n",
      "[epoch:23, iter:11406] Loss: 0.922 | Acc: 66.958% \n",
      "[epoch:23, iter:11407] Loss: 0.922 | Acc: 66.946% \n",
      "[epoch:23, iter:11408] Loss: 0.922 | Acc: 66.931% \n",
      "[epoch:23, iter:11409] Loss: 0.922 | Acc: 66.951% \n",
      "[epoch:23, iter:11410] Loss: 0.922 | Acc: 66.941% \n",
      "[epoch:23, iter:11411] Loss: 0.922 | Acc: 66.944% \n",
      "[epoch:23, iter:11412] Loss: 0.922 | Acc: 66.934% \n",
      "[epoch:23, iter:11413] Loss: 0.922 | Acc: 66.930% \n",
      "[epoch:23, iter:11414] Loss: 0.922 | Acc: 66.913% \n",
      "[epoch:23, iter:11415] Loss: 0.922 | Acc: 66.918% \n",
      "[epoch:23, iter:11416] Loss: 0.922 | Acc: 66.918% \n",
      "[epoch:23, iter:11417] Loss: 0.922 | Acc: 66.935% \n",
      "[epoch:23, iter:11418] Loss: 0.922 | Acc: 66.952% \n",
      "[epoch:23, iter:11419] Loss: 0.922 | Acc: 66.964% \n",
      "[epoch:23, iter:11420] Loss: 0.922 | Acc: 66.962% \n",
      "[epoch:23, iter:11421] Loss: 0.922 | Acc: 66.962% \n",
      "[epoch:23, iter:11422] Loss: 0.922 | Acc: 66.945% \n",
      "[epoch:23, iter:11423] Loss: 0.923 | Acc: 66.936% \n",
      "[epoch:23, iter:11424] Loss: 0.923 | Acc: 66.932% \n",
      "[epoch:23, iter:11425] Loss: 0.923 | Acc: 66.946% \n",
      "[epoch:23, iter:11426] Loss: 0.922 | Acc: 66.955% \n",
      "[epoch:23, iter:11427] Loss: 0.923 | Acc: 66.953% \n",
      "[epoch:23, iter:11428] Loss: 0.923 | Acc: 66.951% \n",
      "[epoch:23, iter:11429] Loss: 0.923 | Acc: 66.946% \n",
      "[epoch:23, iter:11430] Loss: 0.923 | Acc: 66.947% \n",
      "[epoch:23, iter:11431] Loss: 0.922 | Acc: 66.965% \n",
      "[epoch:23, iter:11432] Loss: 0.922 | Acc: 66.961% \n",
      "[epoch:23, iter:11433] Loss: 0.922 | Acc: 66.963% \n",
      "[epoch:23, iter:11434] Loss: 0.923 | Acc: 66.959% \n",
      "[epoch:23, iter:11435] Loss: 0.923 | Acc: 66.961% \n",
      "[epoch:23, iter:11436] Loss: 0.922 | Acc: 66.979% \n",
      "[epoch:23, iter:11437] Loss: 0.922 | Acc: 66.977% \n",
      "[epoch:23, iter:11438] Loss: 0.923 | Acc: 66.984% \n",
      "[epoch:23, iter:11439] Loss: 0.923 | Acc: 66.954% \n",
      "[epoch:23, iter:11440] Loss: 0.923 | Acc: 66.968% \n",
      "[epoch:23, iter:11441] Loss: 0.923 | Acc: 66.984% \n",
      "[epoch:23, iter:11442] Loss: 0.923 | Acc: 66.980% \n",
      "[epoch:23, iter:11443] Loss: 0.922 | Acc: 66.986% \n",
      "[epoch:23, iter:11444] Loss: 0.922 | Acc: 66.991% \n",
      "[epoch:23, iter:11445] Loss: 0.922 | Acc: 67.000% \n",
      "[epoch:23, iter:11446] Loss: 0.922 | Acc: 66.998% \n",
      "[epoch:23, iter:11447] Loss: 0.922 | Acc: 67.011% \n",
      "[epoch:23, iter:11448] Loss: 0.922 | Acc: 67.004% \n",
      "[epoch:23, iter:11449] Loss: 0.922 | Acc: 66.996% \n",
      "[epoch:23, iter:11450] Loss: 0.922 | Acc: 67.016% \n",
      "[epoch:23, iter:11451] Loss: 0.922 | Acc: 67.009% \n",
      "[epoch:23, iter:11452] Loss: 0.922 | Acc: 67.000% \n",
      "[epoch:23, iter:11453] Loss: 0.922 | Acc: 66.985% \n",
      "[epoch:23, iter:11454] Loss: 0.922 | Acc: 67.002% \n",
      "[epoch:23, iter:11455] Loss: 0.922 | Acc: 67.000% \n",
      "[epoch:23, iter:11456] Loss: 0.922 | Acc: 66.998% \n",
      "[epoch:23, iter:11457] Loss: 0.922 | Acc: 66.989% \n",
      "[epoch:23, iter:11458] Loss: 0.921 | Acc: 66.996% \n",
      "[epoch:23, iter:11459] Loss: 0.921 | Acc: 67.002% \n",
      "[epoch:23, iter:11460] Loss: 0.921 | Acc: 67.009% \n",
      "[epoch:23, iter:11461] Loss: 0.921 | Acc: 67.017% \n",
      "[epoch:23, iter:11462] Loss: 0.921 | Acc: 67.030% \n",
      "[epoch:23, iter:11463] Loss: 0.920 | Acc: 67.037% \n",
      "[epoch:23, iter:11464] Loss: 0.920 | Acc: 67.041% \n",
      "[epoch:23, iter:11465] Loss: 0.920 | Acc: 67.039% \n",
      "[epoch:23, iter:11466] Loss: 0.921 | Acc: 67.028% \n",
      "[epoch:23, iter:11467] Loss: 0.921 | Acc: 67.013% \n",
      "[epoch:23, iter:11468] Loss: 0.921 | Acc: 67.017% \n",
      "[epoch:23, iter:11469] Loss: 0.921 | Acc: 67.028% \n",
      "[epoch:23, iter:11470] Loss: 0.921 | Acc: 67.032% \n",
      "[epoch:23, iter:11471] Loss: 0.921 | Acc: 67.034% \n",
      "[epoch:23, iter:11472] Loss: 0.920 | Acc: 67.034% \n",
      "[epoch:23, iter:11473] Loss: 0.920 | Acc: 67.034% \n",
      "[epoch:23, iter:11474] Loss: 0.920 | Acc: 67.021% \n",
      "[epoch:23, iter:11475] Loss: 0.920 | Acc: 67.006% \n",
      "[epoch:23, iter:11476] Loss: 0.921 | Acc: 67.004% \n",
      "[epoch:23, iter:11477] Loss: 0.921 | Acc: 66.992% \n",
      "[epoch:23, iter:11478] Loss: 0.921 | Acc: 66.992% \n",
      "[epoch:23, iter:11479] Loss: 0.920 | Acc: 67.002% \n",
      "[epoch:23, iter:11480] Loss: 0.920 | Acc: 67.008% \n",
      "[epoch:23, iter:11481] Loss: 0.920 | Acc: 67.006% \n",
      "[epoch:23, iter:11482] Loss: 0.920 | Acc: 67.023% \n",
      "[epoch:23, iter:11483] Loss: 0.920 | Acc: 67.002% \n",
      "[epoch:23, iter:11484] Loss: 0.920 | Acc: 67.004% \n",
      "[epoch:23, iter:11485] Loss: 0.920 | Acc: 66.998% \n",
      "[epoch:23, iter:11486] Loss: 0.920 | Acc: 66.998% \n",
      "[epoch:23, iter:11487] Loss: 0.920 | Acc: 66.998% \n",
      "[epoch:23, iter:11488] Loss: 0.920 | Acc: 67.014% \n",
      "[epoch:23, iter:11489] Loss: 0.920 | Acc: 67.006% \n",
      "[epoch:23, iter:11490] Loss: 0.920 | Acc: 67.027% \n",
      "[epoch:23, iter:11491] Loss: 0.920 | Acc: 67.033% \n",
      "[epoch:23, iter:11492] Loss: 0.920 | Acc: 67.028% \n",
      "[epoch:23, iter:11493] Loss: 0.920 | Acc: 67.034% \n",
      "[epoch:23, iter:11494] Loss: 0.919 | Acc: 67.055% \n",
      "[epoch:23, iter:11495] Loss: 0.919 | Acc: 67.048% \n",
      "[epoch:23, iter:11496] Loss: 0.919 | Acc: 67.040% \n",
      "[epoch:23, iter:11497] Loss: 0.920 | Acc: 67.020% \n",
      "[epoch:23, iter:11498] Loss: 0.920 | Acc: 67.014% \n",
      "[epoch:23, iter:11499] Loss: 0.920 | Acc: 67.012% \n",
      "[epoch:23, iter:11500] Loss: 0.920 | Acc: 67.004% \n",
      "Waiting Test...\n",
      "Test's ac is: 64.200%\n",
      "\n",
      "Epoch: 24\n",
      "[epoch:24, iter:11501] Loss: 1.096 | Acc: 61.000% \n",
      "[epoch:24, iter:11502] Loss: 1.077 | Acc: 61.000% \n",
      "[epoch:24, iter:11503] Loss: 1.034 | Acc: 63.000% \n",
      "[epoch:24, iter:11504] Loss: 1.002 | Acc: 64.000% \n",
      "[epoch:24, iter:11505] Loss: 0.988 | Acc: 64.800% \n",
      "[epoch:24, iter:11506] Loss: 0.997 | Acc: 64.333% \n",
      "[epoch:24, iter:11507] Loss: 0.972 | Acc: 65.143% \n",
      "[epoch:24, iter:11508] Loss: 0.958 | Acc: 65.875% \n",
      "[epoch:24, iter:11509] Loss: 0.936 | Acc: 66.556% \n",
      "[epoch:24, iter:11510] Loss: 0.939 | Acc: 66.900% \n",
      "[epoch:24, iter:11511] Loss: 0.954 | Acc: 66.636% \n",
      "[epoch:24, iter:11512] Loss: 0.942 | Acc: 66.750% \n",
      "[epoch:24, iter:11513] Loss: 0.938 | Acc: 67.000% \n",
      "[epoch:24, iter:11514] Loss: 0.942 | Acc: 66.929% \n",
      "[epoch:24, iter:11515] Loss: 0.939 | Acc: 66.667% \n",
      "[epoch:24, iter:11516] Loss: 0.934 | Acc: 66.688% \n",
      "[epoch:24, iter:11517] Loss: 0.931 | Acc: 66.941% \n",
      "[epoch:24, iter:11518] Loss: 0.932 | Acc: 66.833% \n",
      "[epoch:24, iter:11519] Loss: 0.930 | Acc: 67.158% \n",
      "[epoch:24, iter:11520] Loss: 0.928 | Acc: 67.350% \n",
      "[epoch:24, iter:11521] Loss: 0.929 | Acc: 67.429% \n",
      "[epoch:24, iter:11522] Loss: 0.924 | Acc: 67.818% \n",
      "[epoch:24, iter:11523] Loss: 0.929 | Acc: 67.565% \n",
      "[epoch:24, iter:11524] Loss: 0.931 | Acc: 67.458% \n",
      "[epoch:24, iter:11525] Loss: 0.935 | Acc: 67.440% \n",
      "[epoch:24, iter:11526] Loss: 0.936 | Acc: 67.385% \n",
      "[epoch:24, iter:11527] Loss: 0.931 | Acc: 67.481% \n",
      "[epoch:24, iter:11528] Loss: 0.934 | Acc: 67.286% \n",
      "[epoch:24, iter:11529] Loss: 0.931 | Acc: 67.379% \n",
      "[epoch:24, iter:11530] Loss: 0.926 | Acc: 67.400% \n",
      "[epoch:24, iter:11531] Loss: 0.928 | Acc: 67.355% \n",
      "[epoch:24, iter:11532] Loss: 0.926 | Acc: 67.469% \n",
      "[epoch:24, iter:11533] Loss: 0.925 | Acc: 67.576% \n",
      "[epoch:24, iter:11534] Loss: 0.923 | Acc: 67.529% \n",
      "[epoch:24, iter:11535] Loss: 0.924 | Acc: 67.429% \n",
      "[epoch:24, iter:11536] Loss: 0.921 | Acc: 67.472% \n",
      "[epoch:24, iter:11537] Loss: 0.922 | Acc: 67.486% \n",
      "[epoch:24, iter:11538] Loss: 0.926 | Acc: 67.447% \n",
      "[epoch:24, iter:11539] Loss: 0.926 | Acc: 67.436% \n",
      "[epoch:24, iter:11540] Loss: 0.927 | Acc: 67.425% \n",
      "[epoch:24, iter:11541] Loss: 0.925 | Acc: 67.463% \n",
      "[epoch:24, iter:11542] Loss: 0.925 | Acc: 67.405% \n",
      "[epoch:24, iter:11543] Loss: 0.922 | Acc: 67.465% \n",
      "[epoch:24, iter:11544] Loss: 0.924 | Acc: 67.409% \n",
      "[epoch:24, iter:11545] Loss: 0.922 | Acc: 67.422% \n",
      "[epoch:24, iter:11546] Loss: 0.920 | Acc: 67.478% \n",
      "[epoch:24, iter:11547] Loss: 0.920 | Acc: 67.404% \n",
      "[epoch:24, iter:11548] Loss: 0.918 | Acc: 67.479% \n",
      "[epoch:24, iter:11549] Loss: 0.920 | Acc: 67.408% \n",
      "[epoch:24, iter:11550] Loss: 0.919 | Acc: 67.340% \n",
      "[epoch:24, iter:11551] Loss: 0.919 | Acc: 67.412% \n",
      "[epoch:24, iter:11552] Loss: 0.916 | Acc: 67.519% \n",
      "[epoch:24, iter:11553] Loss: 0.912 | Acc: 67.623% \n",
      "[epoch:24, iter:11554] Loss: 0.911 | Acc: 67.648% \n",
      "[epoch:24, iter:11555] Loss: 0.912 | Acc: 67.636% \n",
      "[epoch:24, iter:11556] Loss: 0.911 | Acc: 67.714% \n",
      "[epoch:24, iter:11557] Loss: 0.911 | Acc: 67.579% \n",
      "[epoch:24, iter:11558] Loss: 0.915 | Acc: 67.466% \n",
      "[epoch:24, iter:11559] Loss: 0.917 | Acc: 67.525% \n",
      "[epoch:24, iter:11560] Loss: 0.914 | Acc: 67.633% \n",
      "[epoch:24, iter:11561] Loss: 0.915 | Acc: 67.639% \n",
      "[epoch:24, iter:11562] Loss: 0.914 | Acc: 67.694% \n",
      "[epoch:24, iter:11563] Loss: 0.909 | Acc: 67.905% \n",
      "[epoch:24, iter:11564] Loss: 0.909 | Acc: 67.938% \n",
      "[epoch:24, iter:11565] Loss: 0.910 | Acc: 67.862% \n",
      "[epoch:24, iter:11566] Loss: 0.911 | Acc: 67.864% \n",
      "[epoch:24, iter:11567] Loss: 0.911 | Acc: 67.836% \n",
      "[epoch:24, iter:11568] Loss: 0.912 | Acc: 67.750% \n",
      "[epoch:24, iter:11569] Loss: 0.912 | Acc: 67.681% \n",
      "[epoch:24, iter:11570] Loss: 0.914 | Acc: 67.643% \n",
      "[epoch:24, iter:11571] Loss: 0.913 | Acc: 67.634% \n",
      "[epoch:24, iter:11572] Loss: 0.913 | Acc: 67.708% \n",
      "[epoch:24, iter:11573] Loss: 0.914 | Acc: 67.616% \n",
      "[epoch:24, iter:11574] Loss: 0.914 | Acc: 67.635% \n",
      "[epoch:24, iter:11575] Loss: 0.916 | Acc: 67.560% \n",
      "[epoch:24, iter:11576] Loss: 0.916 | Acc: 67.553% \n",
      "[epoch:24, iter:11577] Loss: 0.916 | Acc: 67.532% \n",
      "[epoch:24, iter:11578] Loss: 0.914 | Acc: 67.577% \n",
      "[epoch:24, iter:11579] Loss: 0.915 | Acc: 67.506% \n",
      "[epoch:24, iter:11580] Loss: 0.915 | Acc: 67.463% \n",
      "[epoch:24, iter:11581] Loss: 0.914 | Acc: 67.556% \n",
      "[epoch:24, iter:11582] Loss: 0.914 | Acc: 67.573% \n",
      "[epoch:24, iter:11583] Loss: 0.913 | Acc: 67.639% \n",
      "[epoch:24, iter:11584] Loss: 0.913 | Acc: 67.595% \n",
      "[epoch:24, iter:11585] Loss: 0.912 | Acc: 67.624% \n",
      "[epoch:24, iter:11586] Loss: 0.913 | Acc: 67.593% \n",
      "[epoch:24, iter:11587] Loss: 0.913 | Acc: 67.598% \n",
      "[epoch:24, iter:11588] Loss: 0.913 | Acc: 67.545% \n",
      "[epoch:24, iter:11589] Loss: 0.914 | Acc: 67.494% \n",
      "[epoch:24, iter:11590] Loss: 0.916 | Acc: 67.478% \n",
      "[epoch:24, iter:11591] Loss: 0.916 | Acc: 67.473% \n",
      "[epoch:24, iter:11592] Loss: 0.915 | Acc: 67.489% \n",
      "[epoch:24, iter:11593] Loss: 0.914 | Acc: 67.484% \n",
      "[epoch:24, iter:11594] Loss: 0.915 | Acc: 67.383% \n",
      "[epoch:24, iter:11595] Loss: 0.915 | Acc: 67.337% \n",
      "[epoch:24, iter:11596] Loss: 0.913 | Acc: 67.375% \n",
      "[epoch:24, iter:11597] Loss: 0.913 | Acc: 67.423% \n",
      "[epoch:24, iter:11598] Loss: 0.912 | Acc: 67.469% \n",
      "[epoch:24, iter:11599] Loss: 0.912 | Acc: 67.384% \n",
      "[epoch:24, iter:11600] Loss: 0.911 | Acc: 67.390% \n",
      "[epoch:24, iter:11601] Loss: 0.910 | Acc: 67.386% \n",
      "[epoch:24, iter:11602] Loss: 0.909 | Acc: 67.402% \n",
      "[epoch:24, iter:11603] Loss: 0.909 | Acc: 67.388% \n",
      "[epoch:24, iter:11604] Loss: 0.909 | Acc: 67.385% \n",
      "[epoch:24, iter:11605] Loss: 0.909 | Acc: 67.362% \n",
      "[epoch:24, iter:11606] Loss: 0.909 | Acc: 67.349% \n",
      "[epoch:24, iter:11607] Loss: 0.910 | Acc: 67.355% \n",
      "[epoch:24, iter:11608] Loss: 0.909 | Acc: 67.343% \n",
      "[epoch:24, iter:11609] Loss: 0.910 | Acc: 67.275% \n",
      "[epoch:24, iter:11610] Loss: 0.910 | Acc: 67.318% \n",
      "[epoch:24, iter:11611] Loss: 0.910 | Acc: 67.288% \n",
      "[epoch:24, iter:11612] Loss: 0.911 | Acc: 67.259% \n",
      "[epoch:24, iter:11613] Loss: 0.909 | Acc: 67.310% \n",
      "[epoch:24, iter:11614] Loss: 0.911 | Acc: 67.211% \n",
      "[epoch:24, iter:11615] Loss: 0.910 | Acc: 67.252% \n",
      "[epoch:24, iter:11616] Loss: 0.911 | Acc: 67.250% \n",
      "[epoch:24, iter:11617] Loss: 0.911 | Acc: 67.265% \n",
      "[epoch:24, iter:11618] Loss: 0.911 | Acc: 67.263% \n",
      "[epoch:24, iter:11619] Loss: 0.912 | Acc: 67.235% \n",
      "[epoch:24, iter:11620] Loss: 0.912 | Acc: 67.225% \n",
      "[epoch:24, iter:11621] Loss: 0.913 | Acc: 67.215% \n",
      "[epoch:24, iter:11622] Loss: 0.912 | Acc: 67.221% \n",
      "[epoch:24, iter:11623] Loss: 0.911 | Acc: 67.236% \n",
      "[epoch:24, iter:11624] Loss: 0.912 | Acc: 67.202% \n",
      "[epoch:24, iter:11625] Loss: 0.911 | Acc: 67.248% \n",
      "[epoch:24, iter:11626] Loss: 0.911 | Acc: 67.254% \n",
      "[epoch:24, iter:11627] Loss: 0.911 | Acc: 67.268% \n",
      "[epoch:24, iter:11628] Loss: 0.912 | Acc: 67.234% \n",
      "[epoch:24, iter:11629] Loss: 0.911 | Acc: 67.302% \n",
      "[epoch:24, iter:11630] Loss: 0.909 | Acc: 67.362% \n",
      "[epoch:24, iter:11631] Loss: 0.909 | Acc: 67.382% \n",
      "[epoch:24, iter:11632] Loss: 0.909 | Acc: 67.341% \n",
      "[epoch:24, iter:11633] Loss: 0.910 | Acc: 67.338% \n",
      "[epoch:24, iter:11634] Loss: 0.909 | Acc: 67.373% \n",
      "[epoch:24, iter:11635] Loss: 0.909 | Acc: 67.400% \n",
      "[epoch:24, iter:11636] Loss: 0.909 | Acc: 67.463% \n",
      "[epoch:24, iter:11637] Loss: 0.908 | Acc: 67.445% \n",
      "[epoch:24, iter:11638] Loss: 0.909 | Acc: 67.435% \n",
      "[epoch:24, iter:11639] Loss: 0.908 | Acc: 67.453% \n",
      "[epoch:24, iter:11640] Loss: 0.907 | Acc: 67.486% \n",
      "[epoch:24, iter:11641] Loss: 0.907 | Acc: 67.504% \n",
      "[epoch:24, iter:11642] Loss: 0.908 | Acc: 67.486% \n",
      "[epoch:24, iter:11643] Loss: 0.907 | Acc: 67.476% \n",
      "[epoch:24, iter:11644] Loss: 0.907 | Acc: 67.486% \n",
      "[epoch:24, iter:11645] Loss: 0.908 | Acc: 67.490% \n",
      "[epoch:24, iter:11646] Loss: 0.907 | Acc: 67.486% \n",
      "[epoch:24, iter:11647] Loss: 0.907 | Acc: 67.524% \n",
      "[epoch:24, iter:11648] Loss: 0.907 | Acc: 67.507% \n",
      "[epoch:24, iter:11649] Loss: 0.906 | Acc: 67.557% \n",
      "[epoch:24, iter:11650] Loss: 0.906 | Acc: 67.553% \n",
      "[epoch:24, iter:11651] Loss: 0.906 | Acc: 67.556% \n",
      "[epoch:24, iter:11652] Loss: 0.905 | Acc: 67.572% \n",
      "[epoch:24, iter:11653] Loss: 0.905 | Acc: 67.582% \n",
      "[epoch:24, iter:11654] Loss: 0.906 | Acc: 67.578% \n",
      "[epoch:24, iter:11655] Loss: 0.906 | Acc: 67.561% \n",
      "[epoch:24, iter:11656] Loss: 0.906 | Acc: 67.564% \n",
      "[epoch:24, iter:11657] Loss: 0.905 | Acc: 67.586% \n",
      "[epoch:24, iter:11658] Loss: 0.907 | Acc: 67.532% \n",
      "[epoch:24, iter:11659] Loss: 0.908 | Acc: 67.491% \n",
      "[epoch:24, iter:11660] Loss: 0.908 | Acc: 67.531% \n",
      "[epoch:24, iter:11661] Loss: 0.907 | Acc: 67.565% \n",
      "[epoch:24, iter:11662] Loss: 0.907 | Acc: 67.562% \n",
      "[epoch:24, iter:11663] Loss: 0.906 | Acc: 67.589% \n",
      "[epoch:24, iter:11664] Loss: 0.905 | Acc: 67.616% \n",
      "[epoch:24, iter:11665] Loss: 0.907 | Acc: 67.558% \n",
      "[epoch:24, iter:11666] Loss: 0.907 | Acc: 67.560% \n",
      "[epoch:24, iter:11667] Loss: 0.907 | Acc: 67.551% \n",
      "[epoch:24, iter:11668] Loss: 0.907 | Acc: 67.565% \n",
      "[epoch:24, iter:11669] Loss: 0.909 | Acc: 67.521% \n",
      "[epoch:24, iter:11670] Loss: 0.908 | Acc: 67.529% \n",
      "[epoch:24, iter:11671] Loss: 0.908 | Acc: 67.532% \n",
      "[epoch:24, iter:11672] Loss: 0.908 | Acc: 67.517% \n",
      "[epoch:24, iter:11673] Loss: 0.907 | Acc: 67.572% \n",
      "[epoch:24, iter:11674] Loss: 0.907 | Acc: 67.575% \n",
      "[epoch:24, iter:11675] Loss: 0.906 | Acc: 67.583% \n",
      "[epoch:24, iter:11676] Loss: 0.906 | Acc: 67.568% \n",
      "[epoch:24, iter:11677] Loss: 0.908 | Acc: 67.531% \n",
      "[epoch:24, iter:11678] Loss: 0.908 | Acc: 67.506% \n",
      "[epoch:24, iter:11679] Loss: 0.907 | Acc: 67.520% \n",
      "[epoch:24, iter:11680] Loss: 0.907 | Acc: 67.511% \n",
      "[epoch:24, iter:11681] Loss: 0.906 | Acc: 67.503% \n",
      "[epoch:24, iter:11682] Loss: 0.906 | Acc: 67.522% \n",
      "[epoch:24, iter:11683] Loss: 0.906 | Acc: 67.541% \n",
      "[epoch:24, iter:11684] Loss: 0.906 | Acc: 67.560% \n",
      "[epoch:24, iter:11685] Loss: 0.905 | Acc: 67.562% \n",
      "[epoch:24, iter:11686] Loss: 0.905 | Acc: 67.548% \n",
      "[epoch:24, iter:11687] Loss: 0.904 | Acc: 67.604% \n",
      "[epoch:24, iter:11688] Loss: 0.904 | Acc: 67.596% \n",
      "[epoch:24, iter:11689] Loss: 0.903 | Acc: 67.640% \n",
      "[epoch:24, iter:11690] Loss: 0.903 | Acc: 67.626% \n",
      "[epoch:24, iter:11691] Loss: 0.903 | Acc: 67.613% \n",
      "[epoch:24, iter:11692] Loss: 0.902 | Acc: 67.672% \n",
      "[epoch:24, iter:11693] Loss: 0.902 | Acc: 67.674% \n",
      "[epoch:24, iter:11694] Loss: 0.902 | Acc: 67.670% \n",
      "[epoch:24, iter:11695] Loss: 0.901 | Acc: 67.697% \n",
      "[epoch:24, iter:11696] Loss: 0.901 | Acc: 67.714% \n",
      "[epoch:24, iter:11697] Loss: 0.901 | Acc: 67.736% \n",
      "[epoch:24, iter:11698] Loss: 0.900 | Acc: 67.778% \n",
      "[epoch:24, iter:11699] Loss: 0.900 | Acc: 67.764% \n",
      "[epoch:24, iter:11700] Loss: 0.901 | Acc: 67.770% \n",
      "[epoch:24, iter:11701] Loss: 0.901 | Acc: 67.801% \n",
      "[epoch:24, iter:11702] Loss: 0.901 | Acc: 67.782% \n",
      "[epoch:24, iter:11703] Loss: 0.900 | Acc: 67.803% \n",
      "[epoch:24, iter:11704] Loss: 0.900 | Acc: 67.799% \n",
      "[epoch:24, iter:11705] Loss: 0.900 | Acc: 67.815% \n",
      "[epoch:24, iter:11706] Loss: 0.901 | Acc: 67.743% \n",
      "[epoch:24, iter:11707] Loss: 0.901 | Acc: 67.787% \n",
      "[epoch:24, iter:11708] Loss: 0.901 | Acc: 67.760% \n",
      "[epoch:24, iter:11709] Loss: 0.902 | Acc: 67.746% \n",
      "[epoch:24, iter:11710] Loss: 0.902 | Acc: 67.719% \n",
      "[epoch:24, iter:11711] Loss: 0.901 | Acc: 67.754% \n",
      "[epoch:24, iter:11712] Loss: 0.901 | Acc: 67.764% \n",
      "[epoch:24, iter:11713] Loss: 0.901 | Acc: 67.761% \n",
      "[epoch:24, iter:11714] Loss: 0.901 | Acc: 67.738% \n",
      "[epoch:24, iter:11715] Loss: 0.901 | Acc: 67.777% \n",
      "[epoch:24, iter:11716] Loss: 0.901 | Acc: 67.787% \n",
      "[epoch:24, iter:11717] Loss: 0.900 | Acc: 67.834% \n",
      "[epoch:24, iter:11718] Loss: 0.899 | Acc: 67.826% \n",
      "[epoch:24, iter:11719] Loss: 0.899 | Acc: 67.817% \n",
      "[epoch:24, iter:11720] Loss: 0.900 | Acc: 67.764% \n",
      "[epoch:24, iter:11721] Loss: 0.900 | Acc: 67.769% \n",
      "[epoch:24, iter:11722] Loss: 0.899 | Acc: 67.761% \n",
      "[epoch:24, iter:11723] Loss: 0.899 | Acc: 67.785% \n",
      "[epoch:24, iter:11724] Loss: 0.900 | Acc: 67.750% \n",
      "[epoch:24, iter:11725] Loss: 0.900 | Acc: 67.756% \n",
      "[epoch:24, iter:11726] Loss: 0.901 | Acc: 67.739% \n",
      "[epoch:24, iter:11727] Loss: 0.901 | Acc: 67.744% \n",
      "[epoch:24, iter:11728] Loss: 0.900 | Acc: 67.759% \n",
      "[epoch:24, iter:11729] Loss: 0.901 | Acc: 67.707% \n",
      "[epoch:24, iter:11730] Loss: 0.901 | Acc: 67.709% \n",
      "[epoch:24, iter:11731] Loss: 0.901 | Acc: 67.710% \n",
      "[epoch:24, iter:11732] Loss: 0.902 | Acc: 67.711% \n",
      "[epoch:24, iter:11733] Loss: 0.903 | Acc: 67.665% \n",
      "[epoch:24, iter:11734] Loss: 0.902 | Acc: 67.692% \n",
      "[epoch:24, iter:11735] Loss: 0.902 | Acc: 67.677% \n",
      "[epoch:24, iter:11736] Loss: 0.903 | Acc: 67.669% \n",
      "[epoch:24, iter:11737] Loss: 0.904 | Acc: 67.646% \n",
      "[epoch:24, iter:11738] Loss: 0.904 | Acc: 67.639% \n",
      "[epoch:24, iter:11739] Loss: 0.904 | Acc: 67.632% \n",
      "[epoch:24, iter:11740] Loss: 0.905 | Acc: 67.608% \n",
      "[epoch:24, iter:11741] Loss: 0.905 | Acc: 67.598% \n",
      "[epoch:24, iter:11742] Loss: 0.904 | Acc: 67.628% \n",
      "[epoch:24, iter:11743] Loss: 0.904 | Acc: 67.634% \n",
      "[epoch:24, iter:11744] Loss: 0.904 | Acc: 67.643% \n",
      "[epoch:24, iter:11745] Loss: 0.904 | Acc: 67.637% \n",
      "[epoch:24, iter:11746] Loss: 0.904 | Acc: 67.638% \n",
      "[epoch:24, iter:11747] Loss: 0.905 | Acc: 67.640% \n",
      "[epoch:24, iter:11748] Loss: 0.905 | Acc: 67.641% \n",
      "[epoch:24, iter:11749] Loss: 0.905 | Acc: 67.639% \n",
      "[epoch:24, iter:11750] Loss: 0.906 | Acc: 67.628% \n",
      "[epoch:24, iter:11751] Loss: 0.905 | Acc: 67.657% \n",
      "[epoch:24, iter:11752] Loss: 0.905 | Acc: 67.651% \n",
      "[epoch:24, iter:11753] Loss: 0.905 | Acc: 67.621% \n",
      "[epoch:24, iter:11754] Loss: 0.906 | Acc: 67.618% \n",
      "[epoch:24, iter:11755] Loss: 0.906 | Acc: 67.596% \n",
      "[epoch:24, iter:11756] Loss: 0.905 | Acc: 67.602% \n",
      "[epoch:24, iter:11757] Loss: 0.905 | Acc: 67.599% \n",
      "[epoch:24, iter:11758] Loss: 0.905 | Acc: 67.605% \n",
      "[epoch:24, iter:11759] Loss: 0.905 | Acc: 67.618% \n",
      "[epoch:24, iter:11760] Loss: 0.905 | Acc: 67.615% \n",
      "[epoch:24, iter:11761] Loss: 0.906 | Acc: 67.609% \n",
      "[epoch:24, iter:11762] Loss: 0.907 | Acc: 67.584% \n",
      "[epoch:24, iter:11763] Loss: 0.907 | Acc: 67.586% \n",
      "[epoch:24, iter:11764] Loss: 0.906 | Acc: 67.617% \n",
      "[epoch:24, iter:11765] Loss: 0.906 | Acc: 67.611% \n",
      "[epoch:24, iter:11766] Loss: 0.906 | Acc: 67.598% \n",
      "[epoch:24, iter:11767] Loss: 0.907 | Acc: 67.596% \n",
      "[epoch:24, iter:11768] Loss: 0.906 | Acc: 67.608% \n",
      "[epoch:24, iter:11769] Loss: 0.907 | Acc: 67.576% \n",
      "[epoch:24, iter:11770] Loss: 0.907 | Acc: 67.596% \n",
      "[epoch:24, iter:11771] Loss: 0.906 | Acc: 67.605% \n",
      "[epoch:24, iter:11772] Loss: 0.906 | Acc: 67.607% \n",
      "[epoch:24, iter:11773] Loss: 0.906 | Acc: 67.615% \n",
      "[epoch:24, iter:11774] Loss: 0.906 | Acc: 67.620% \n",
      "[epoch:24, iter:11775] Loss: 0.906 | Acc: 67.625% \n",
      "[epoch:24, iter:11776] Loss: 0.906 | Acc: 67.616% \n",
      "[epoch:24, iter:11777] Loss: 0.906 | Acc: 67.603% \n",
      "[epoch:24, iter:11778] Loss: 0.906 | Acc: 67.604% \n",
      "[epoch:24, iter:11779] Loss: 0.906 | Acc: 67.591% \n",
      "[epoch:24, iter:11780] Loss: 0.906 | Acc: 67.589% \n",
      "[epoch:24, iter:11781] Loss: 0.906 | Acc: 67.609% \n",
      "[epoch:24, iter:11782] Loss: 0.907 | Acc: 67.571% \n",
      "[epoch:24, iter:11783] Loss: 0.907 | Acc: 67.569% \n",
      "[epoch:24, iter:11784] Loss: 0.907 | Acc: 67.570% \n",
      "[epoch:24, iter:11785] Loss: 0.906 | Acc: 67.579% \n",
      "[epoch:24, iter:11786] Loss: 0.907 | Acc: 67.563% \n",
      "[epoch:24, iter:11787] Loss: 0.907 | Acc: 67.561% \n",
      "[epoch:24, iter:11788] Loss: 0.907 | Acc: 67.531% \n",
      "[epoch:24, iter:11789] Loss: 0.907 | Acc: 67.557% \n",
      "[epoch:24, iter:11790] Loss: 0.907 | Acc: 67.555% \n",
      "[epoch:24, iter:11791] Loss: 0.907 | Acc: 67.581% \n",
      "[epoch:24, iter:11792] Loss: 0.906 | Acc: 67.568% \n",
      "[epoch:24, iter:11793] Loss: 0.906 | Acc: 67.567% \n",
      "[epoch:24, iter:11794] Loss: 0.906 | Acc: 67.571% \n",
      "[epoch:24, iter:11795] Loss: 0.906 | Acc: 67.586% \n",
      "[epoch:24, iter:11796] Loss: 0.905 | Acc: 67.591% \n",
      "[epoch:24, iter:11797] Loss: 0.906 | Acc: 67.599% \n",
      "[epoch:24, iter:11798] Loss: 0.906 | Acc: 67.604% \n",
      "[epoch:24, iter:11799] Loss: 0.906 | Acc: 67.582% \n",
      "[epoch:24, iter:11800] Loss: 0.905 | Acc: 67.603% \n",
      "[epoch:24, iter:11801] Loss: 0.905 | Acc: 67.598% \n",
      "[epoch:24, iter:11802] Loss: 0.906 | Acc: 67.593% \n",
      "[epoch:24, iter:11803] Loss: 0.906 | Acc: 67.597% \n",
      "[epoch:24, iter:11804] Loss: 0.907 | Acc: 67.576% \n",
      "[epoch:24, iter:11805] Loss: 0.907 | Acc: 67.577% \n",
      "[epoch:24, iter:11806] Loss: 0.908 | Acc: 67.539% \n",
      "[epoch:24, iter:11807] Loss: 0.908 | Acc: 67.550% \n",
      "[epoch:24, iter:11808] Loss: 0.907 | Acc: 67.532% \n",
      "[epoch:24, iter:11809] Loss: 0.908 | Acc: 67.498% \n",
      "[epoch:24, iter:11810] Loss: 0.908 | Acc: 67.506% \n",
      "[epoch:24, iter:11811] Loss: 0.908 | Acc: 67.511% \n",
      "[epoch:24, iter:11812] Loss: 0.909 | Acc: 67.465% \n",
      "[epoch:24, iter:11813] Loss: 0.909 | Acc: 67.460% \n",
      "[epoch:24, iter:11814] Loss: 0.909 | Acc: 67.462% \n",
      "[epoch:24, iter:11815] Loss: 0.909 | Acc: 67.460% \n",
      "[epoch:24, iter:11816] Loss: 0.909 | Acc: 67.459% \n",
      "[epoch:24, iter:11817] Loss: 0.909 | Acc: 67.445% \n",
      "[epoch:24, iter:11818] Loss: 0.910 | Acc: 67.418% \n",
      "[epoch:24, iter:11819] Loss: 0.910 | Acc: 67.420% \n",
      "[epoch:24, iter:11820] Loss: 0.910 | Acc: 67.412% \n",
      "[epoch:24, iter:11821] Loss: 0.910 | Acc: 67.414% \n",
      "[epoch:24, iter:11822] Loss: 0.910 | Acc: 67.416% \n",
      "[epoch:24, iter:11823] Loss: 0.910 | Acc: 67.406% \n",
      "[epoch:24, iter:11824] Loss: 0.909 | Acc: 67.435% \n",
      "[epoch:24, iter:11825] Loss: 0.909 | Acc: 67.446% \n",
      "[epoch:24, iter:11826] Loss: 0.909 | Acc: 67.442% \n",
      "[epoch:24, iter:11827] Loss: 0.909 | Acc: 67.428% \n",
      "[epoch:24, iter:11828] Loss: 0.910 | Acc: 67.409% \n",
      "[epoch:24, iter:11829] Loss: 0.910 | Acc: 67.419% \n",
      "[epoch:24, iter:11830] Loss: 0.910 | Acc: 67.427% \n",
      "[epoch:24, iter:11831] Loss: 0.909 | Acc: 67.450% \n",
      "[epoch:24, iter:11832] Loss: 0.909 | Acc: 67.452% \n",
      "[epoch:24, iter:11833] Loss: 0.909 | Acc: 67.447% \n",
      "[epoch:24, iter:11834] Loss: 0.909 | Acc: 67.446% \n",
      "[epoch:24, iter:11835] Loss: 0.909 | Acc: 67.451% \n",
      "[epoch:24, iter:11836] Loss: 0.909 | Acc: 67.461% \n",
      "[epoch:24, iter:11837] Loss: 0.909 | Acc: 67.469% \n",
      "[epoch:24, iter:11838] Loss: 0.908 | Acc: 67.473% \n",
      "[epoch:24, iter:11839] Loss: 0.908 | Acc: 67.481% \n",
      "[epoch:24, iter:11840] Loss: 0.908 | Acc: 67.491% \n",
      "[epoch:24, iter:11841] Loss: 0.908 | Acc: 67.493% \n",
      "[epoch:24, iter:11842] Loss: 0.908 | Acc: 67.485% \n",
      "[epoch:24, iter:11843] Loss: 0.907 | Acc: 67.525% \n",
      "[epoch:24, iter:11844] Loss: 0.907 | Acc: 67.526% \n",
      "[epoch:24, iter:11845] Loss: 0.908 | Acc: 67.499% \n",
      "[epoch:24, iter:11846] Loss: 0.908 | Acc: 67.491% \n",
      "[epoch:24, iter:11847] Loss: 0.908 | Acc: 67.481% \n",
      "[epoch:24, iter:11848] Loss: 0.908 | Acc: 67.483% \n",
      "[epoch:24, iter:11849] Loss: 0.908 | Acc: 67.504% \n",
      "[epoch:24, iter:11850] Loss: 0.908 | Acc: 67.497% \n",
      "[epoch:24, iter:11851] Loss: 0.907 | Acc: 67.536% \n",
      "[epoch:24, iter:11852] Loss: 0.907 | Acc: 67.537% \n",
      "[epoch:24, iter:11853] Loss: 0.907 | Acc: 67.533% \n",
      "[epoch:24, iter:11854] Loss: 0.907 | Acc: 67.517% \n",
      "[epoch:24, iter:11855] Loss: 0.908 | Acc: 67.507% \n",
      "[epoch:24, iter:11856] Loss: 0.907 | Acc: 67.531% \n",
      "[epoch:24, iter:11857] Loss: 0.908 | Acc: 67.518% \n",
      "[epoch:24, iter:11858] Loss: 0.908 | Acc: 67.514% \n",
      "[epoch:24, iter:11859] Loss: 0.908 | Acc: 67.499% \n",
      "[epoch:24, iter:11860] Loss: 0.908 | Acc: 67.508% \n",
      "[epoch:24, iter:11861] Loss: 0.909 | Acc: 67.476% \n",
      "[epoch:24, iter:11862] Loss: 0.908 | Acc: 67.503% \n",
      "[epoch:24, iter:11863] Loss: 0.908 | Acc: 67.526% \n",
      "[epoch:24, iter:11864] Loss: 0.907 | Acc: 67.549% \n",
      "[epoch:24, iter:11865] Loss: 0.907 | Acc: 67.545% \n",
      "[epoch:24, iter:11866] Loss: 0.907 | Acc: 67.563% \n",
      "[epoch:24, iter:11867] Loss: 0.907 | Acc: 67.553% \n",
      "[epoch:24, iter:11868] Loss: 0.907 | Acc: 67.554% \n",
      "[epoch:24, iter:11869] Loss: 0.907 | Acc: 67.558% \n",
      "[epoch:24, iter:11870] Loss: 0.907 | Acc: 67.562% \n",
      "[epoch:24, iter:11871] Loss: 0.907 | Acc: 67.580% \n",
      "[epoch:24, iter:11872] Loss: 0.907 | Acc: 67.573% \n",
      "[epoch:24, iter:11873] Loss: 0.907 | Acc: 67.566% \n",
      "[epoch:24, iter:11874] Loss: 0.907 | Acc: 67.564% \n",
      "[epoch:24, iter:11875] Loss: 0.907 | Acc: 67.571% \n",
      "[epoch:24, iter:11876] Loss: 0.906 | Acc: 67.577% \n",
      "[epoch:24, iter:11877] Loss: 0.906 | Acc: 67.570% \n",
      "[epoch:24, iter:11878] Loss: 0.906 | Acc: 67.579% \n",
      "[epoch:24, iter:11879] Loss: 0.906 | Acc: 67.583% \n",
      "[epoch:24, iter:11880] Loss: 0.906 | Acc: 67.587% \n",
      "[epoch:24, iter:11881] Loss: 0.906 | Acc: 67.598% \n",
      "[epoch:24, iter:11882] Loss: 0.906 | Acc: 67.599% \n",
      "[epoch:24, iter:11883] Loss: 0.905 | Acc: 67.614% \n",
      "[epoch:24, iter:11884] Loss: 0.905 | Acc: 67.612% \n",
      "[epoch:24, iter:11885] Loss: 0.906 | Acc: 67.579% \n",
      "[epoch:24, iter:11886] Loss: 0.906 | Acc: 67.580% \n",
      "[epoch:24, iter:11887] Loss: 0.906 | Acc: 67.584% \n",
      "[epoch:24, iter:11888] Loss: 0.906 | Acc: 67.585% \n",
      "[epoch:24, iter:11889] Loss: 0.906 | Acc: 67.576% \n",
      "[epoch:24, iter:11890] Loss: 0.906 | Acc: 67.577% \n",
      "[epoch:24, iter:11891] Loss: 0.906 | Acc: 67.575% \n",
      "[epoch:24, iter:11892] Loss: 0.906 | Acc: 67.597% \n",
      "[epoch:24, iter:11893] Loss: 0.905 | Acc: 67.618% \n",
      "[epoch:24, iter:11894] Loss: 0.905 | Acc: 67.612% \n",
      "[epoch:24, iter:11895] Loss: 0.906 | Acc: 67.603% \n",
      "[epoch:24, iter:11896] Loss: 0.906 | Acc: 67.624% \n",
      "[epoch:24, iter:11897] Loss: 0.906 | Acc: 67.622% \n",
      "[epoch:24, iter:11898] Loss: 0.906 | Acc: 67.616% \n",
      "[epoch:24, iter:11899] Loss: 0.906 | Acc: 67.619% \n",
      "[epoch:24, iter:11900] Loss: 0.906 | Acc: 67.632% \n",
      "[epoch:24, iter:11901] Loss: 0.906 | Acc: 67.643% \n",
      "[epoch:24, iter:11902] Loss: 0.906 | Acc: 67.639% \n",
      "[epoch:24, iter:11903] Loss: 0.906 | Acc: 67.633% \n",
      "[epoch:24, iter:11904] Loss: 0.906 | Acc: 67.639% \n",
      "[epoch:24, iter:11905] Loss: 0.906 | Acc: 67.617% \n",
      "[epoch:24, iter:11906] Loss: 0.907 | Acc: 67.616% \n",
      "[epoch:24, iter:11907] Loss: 0.907 | Acc: 67.612% \n",
      "[epoch:24, iter:11908] Loss: 0.907 | Acc: 67.600% \n",
      "[epoch:24, iter:11909] Loss: 0.907 | Acc: 67.599% \n",
      "[epoch:24, iter:11910] Loss: 0.907 | Acc: 67.588% \n",
      "[epoch:24, iter:11911] Loss: 0.907 | Acc: 67.611% \n",
      "[epoch:24, iter:11912] Loss: 0.907 | Acc: 67.604% \n",
      "[epoch:24, iter:11913] Loss: 0.907 | Acc: 67.613% \n",
      "[epoch:24, iter:11914] Loss: 0.907 | Acc: 67.609% \n",
      "[epoch:24, iter:11915] Loss: 0.907 | Acc: 67.634% \n",
      "[epoch:24, iter:11916] Loss: 0.907 | Acc: 67.644% \n",
      "[epoch:24, iter:11917] Loss: 0.906 | Acc: 67.652% \n",
      "[epoch:24, iter:11918] Loss: 0.907 | Acc: 67.648% \n",
      "[epoch:24, iter:11919] Loss: 0.907 | Acc: 67.635% \n",
      "[epoch:24, iter:11920] Loss: 0.907 | Acc: 67.629% \n",
      "[epoch:24, iter:11921] Loss: 0.907 | Acc: 67.610% \n",
      "[epoch:24, iter:11922] Loss: 0.907 | Acc: 67.600% \n",
      "[epoch:24, iter:11923] Loss: 0.907 | Acc: 67.593% \n",
      "[epoch:24, iter:11924] Loss: 0.907 | Acc: 67.590% \n",
      "[epoch:24, iter:11925] Loss: 0.907 | Acc: 67.598% \n",
      "[epoch:24, iter:11926] Loss: 0.907 | Acc: 67.606% \n",
      "[epoch:24, iter:11927] Loss: 0.907 | Acc: 67.614% \n",
      "[epoch:24, iter:11928] Loss: 0.907 | Acc: 67.614% \n",
      "[epoch:24, iter:11929] Loss: 0.907 | Acc: 67.604% \n",
      "[epoch:24, iter:11930] Loss: 0.907 | Acc: 67.612% \n",
      "[epoch:24, iter:11931] Loss: 0.907 | Acc: 67.624% \n",
      "[epoch:24, iter:11932] Loss: 0.907 | Acc: 67.609% \n",
      "[epoch:24, iter:11933] Loss: 0.907 | Acc: 67.614% \n",
      "[epoch:24, iter:11934] Loss: 0.907 | Acc: 67.627% \n",
      "[epoch:24, iter:11935] Loss: 0.907 | Acc: 67.598% \n",
      "[epoch:24, iter:11936] Loss: 0.907 | Acc: 67.601% \n",
      "[epoch:24, iter:11937] Loss: 0.907 | Acc: 67.622% \n",
      "[epoch:24, iter:11938] Loss: 0.907 | Acc: 67.632% \n",
      "[epoch:24, iter:11939] Loss: 0.906 | Acc: 67.651% \n",
      "[epoch:24, iter:11940] Loss: 0.906 | Acc: 67.650% \n",
      "[epoch:24, iter:11941] Loss: 0.906 | Acc: 67.644% \n",
      "[epoch:24, iter:11942] Loss: 0.906 | Acc: 67.643% \n",
      "[epoch:24, iter:11943] Loss: 0.906 | Acc: 67.639% \n",
      "[epoch:24, iter:11944] Loss: 0.906 | Acc: 67.626% \n",
      "[epoch:24, iter:11945] Loss: 0.906 | Acc: 67.616% \n",
      "[epoch:24, iter:11946] Loss: 0.906 | Acc: 67.621% \n",
      "[epoch:24, iter:11947] Loss: 0.906 | Acc: 67.613% \n",
      "[epoch:24, iter:11948] Loss: 0.906 | Acc: 67.627% \n",
      "[epoch:24, iter:11949] Loss: 0.906 | Acc: 67.624% \n",
      "[epoch:24, iter:11950] Loss: 0.906 | Acc: 67.620% \n",
      "[epoch:24, iter:11951] Loss: 0.907 | Acc: 67.601% \n",
      "[epoch:24, iter:11952] Loss: 0.906 | Acc: 67.619% \n",
      "[epoch:24, iter:11953] Loss: 0.906 | Acc: 67.631% \n",
      "[epoch:24, iter:11954] Loss: 0.906 | Acc: 67.632% \n",
      "[epoch:24, iter:11955] Loss: 0.906 | Acc: 67.640% \n",
      "[epoch:24, iter:11956] Loss: 0.906 | Acc: 67.618% \n",
      "[epoch:24, iter:11957] Loss: 0.906 | Acc: 67.615% \n",
      "[epoch:24, iter:11958] Loss: 0.905 | Acc: 67.616% \n",
      "[epoch:24, iter:11959] Loss: 0.905 | Acc: 67.623% \n",
      "[epoch:24, iter:11960] Loss: 0.905 | Acc: 67.626% \n",
      "[epoch:24, iter:11961] Loss: 0.905 | Acc: 67.623% \n",
      "[epoch:24, iter:11962] Loss: 0.905 | Acc: 67.621% \n",
      "[epoch:24, iter:11963] Loss: 0.905 | Acc: 67.629% \n",
      "[epoch:24, iter:11964] Loss: 0.904 | Acc: 67.627% \n",
      "[epoch:24, iter:11965] Loss: 0.904 | Acc: 67.634% \n",
      "[epoch:24, iter:11966] Loss: 0.904 | Acc: 67.644% \n",
      "[epoch:24, iter:11967] Loss: 0.904 | Acc: 67.630% \n",
      "[epoch:24, iter:11968] Loss: 0.904 | Acc: 67.632% \n",
      "[epoch:24, iter:11969] Loss: 0.904 | Acc: 67.618% \n",
      "[epoch:24, iter:11970] Loss: 0.904 | Acc: 67.628% \n",
      "[epoch:24, iter:11971] Loss: 0.904 | Acc: 67.622% \n",
      "[epoch:24, iter:11972] Loss: 0.904 | Acc: 67.623% \n",
      "[epoch:24, iter:11973] Loss: 0.905 | Acc: 67.617% \n",
      "[epoch:24, iter:11974] Loss: 0.905 | Acc: 67.610% \n",
      "[epoch:24, iter:11975] Loss: 0.905 | Acc: 67.604% \n",
      "[epoch:24, iter:11976] Loss: 0.905 | Acc: 67.605% \n",
      "[epoch:24, iter:11977] Loss: 0.905 | Acc: 67.602% \n",
      "[epoch:24, iter:11978] Loss: 0.905 | Acc: 67.611% \n",
      "[epoch:24, iter:11979] Loss: 0.905 | Acc: 67.601% \n",
      "[epoch:24, iter:11980] Loss: 0.905 | Acc: 67.610% \n",
      "[epoch:24, iter:11981] Loss: 0.905 | Acc: 67.617% \n",
      "[epoch:24, iter:11982] Loss: 0.905 | Acc: 67.618% \n",
      "[epoch:24, iter:11983] Loss: 0.905 | Acc: 67.615% \n",
      "[epoch:24, iter:11984] Loss: 0.905 | Acc: 67.605% \n",
      "[epoch:24, iter:11985] Loss: 0.905 | Acc: 67.608% \n",
      "[epoch:24, iter:11986] Loss: 0.905 | Acc: 67.611% \n",
      "[epoch:24, iter:11987] Loss: 0.904 | Acc: 67.637% \n",
      "[epoch:24, iter:11988] Loss: 0.904 | Acc: 67.660% \n",
      "[epoch:24, iter:11989] Loss: 0.904 | Acc: 67.646% \n",
      "[epoch:24, iter:11990] Loss: 0.904 | Acc: 67.665% \n",
      "[epoch:24, iter:11991] Loss: 0.904 | Acc: 67.656% \n",
      "[epoch:24, iter:11992] Loss: 0.905 | Acc: 67.646% \n",
      "[epoch:24, iter:11993] Loss: 0.905 | Acc: 67.639% \n",
      "[epoch:24, iter:11994] Loss: 0.904 | Acc: 67.636% \n",
      "[epoch:24, iter:11995] Loss: 0.904 | Acc: 67.636% \n",
      "[epoch:24, iter:11996] Loss: 0.904 | Acc: 67.641% \n",
      "[epoch:24, iter:11997] Loss: 0.904 | Acc: 67.646% \n",
      "[epoch:24, iter:11998] Loss: 0.904 | Acc: 67.651% \n",
      "[epoch:24, iter:11999] Loss: 0.904 | Acc: 67.659% \n",
      "[epoch:24, iter:12000] Loss: 0.904 | Acc: 67.658% \n",
      "Waiting Test...\n",
      "Test's ac is: 65.050%\n",
      "\n",
      "Epoch: 25\n",
      "[epoch:25, iter:12001] Loss: 0.771 | Acc: 75.000% \n",
      "[epoch:25, iter:12002] Loss: 0.748 | Acc: 76.000% \n",
      "[epoch:25, iter:12003] Loss: 0.804 | Acc: 71.000% \n",
      "[epoch:25, iter:12004] Loss: 0.877 | Acc: 68.750% \n",
      "[epoch:25, iter:12005] Loss: 0.885 | Acc: 67.400% \n",
      "[epoch:25, iter:12006] Loss: 0.895 | Acc: 67.167% \n",
      "[epoch:25, iter:12007] Loss: 0.900 | Acc: 67.143% \n",
      "[epoch:25, iter:12008] Loss: 0.891 | Acc: 67.375% \n",
      "[epoch:25, iter:12009] Loss: 0.897 | Acc: 66.667% \n",
      "[epoch:25, iter:12010] Loss: 0.905 | Acc: 65.900% \n",
      "[epoch:25, iter:12011] Loss: 0.906 | Acc: 65.636% \n",
      "[epoch:25, iter:12012] Loss: 0.909 | Acc: 65.500% \n",
      "[epoch:25, iter:12013] Loss: 0.908 | Acc: 65.923% \n",
      "[epoch:25, iter:12014] Loss: 0.901 | Acc: 66.357% \n",
      "[epoch:25, iter:12015] Loss: 0.908 | Acc: 66.333% \n",
      "[epoch:25, iter:12016] Loss: 0.899 | Acc: 66.938% \n",
      "[epoch:25, iter:12017] Loss: 0.906 | Acc: 66.706% \n",
      "[epoch:25, iter:12018] Loss: 0.910 | Acc: 66.500% \n",
      "[epoch:25, iter:12019] Loss: 0.911 | Acc: 66.684% \n",
      "[epoch:25, iter:12020] Loss: 0.908 | Acc: 66.850% \n",
      "[epoch:25, iter:12021] Loss: 0.905 | Acc: 66.857% \n",
      "[epoch:25, iter:12022] Loss: 0.900 | Acc: 67.182% \n",
      "[epoch:25, iter:12023] Loss: 0.900 | Acc: 67.174% \n",
      "[epoch:25, iter:12024] Loss: 0.892 | Acc: 67.500% \n",
      "[epoch:25, iter:12025] Loss: 0.889 | Acc: 67.560% \n",
      "[epoch:25, iter:12026] Loss: 0.894 | Acc: 67.308% \n",
      "[epoch:25, iter:12027] Loss: 0.897 | Acc: 67.407% \n",
      "[epoch:25, iter:12028] Loss: 0.895 | Acc: 67.464% \n",
      "[epoch:25, iter:12029] Loss: 0.888 | Acc: 67.862% \n",
      "[epoch:25, iter:12030] Loss: 0.891 | Acc: 67.600% \n",
      "[epoch:25, iter:12031] Loss: 0.887 | Acc: 67.613% \n",
      "[epoch:25, iter:12032] Loss: 0.893 | Acc: 67.594% \n",
      "[epoch:25, iter:12033] Loss: 0.891 | Acc: 67.667% \n",
      "[epoch:25, iter:12034] Loss: 0.889 | Acc: 67.706% \n",
      "[epoch:25, iter:12035] Loss: 0.889 | Acc: 67.571% \n",
      "[epoch:25, iter:12036] Loss: 0.889 | Acc: 67.639% \n",
      "[epoch:25, iter:12037] Loss: 0.894 | Acc: 67.378% \n",
      "[epoch:25, iter:12038] Loss: 0.895 | Acc: 67.342% \n",
      "[epoch:25, iter:12039] Loss: 0.894 | Acc: 67.308% \n",
      "[epoch:25, iter:12040] Loss: 0.897 | Acc: 67.275% \n",
      "[epoch:25, iter:12041] Loss: 0.900 | Acc: 67.244% \n",
      "[epoch:25, iter:12042] Loss: 0.897 | Acc: 67.262% \n",
      "[epoch:25, iter:12043] Loss: 0.898 | Acc: 67.372% \n",
      "[epoch:25, iter:12044] Loss: 0.898 | Acc: 67.318% \n",
      "[epoch:25, iter:12045] Loss: 0.902 | Acc: 67.000% \n",
      "[epoch:25, iter:12046] Loss: 0.903 | Acc: 66.957% \n",
      "[epoch:25, iter:12047] Loss: 0.905 | Acc: 66.979% \n",
      "[epoch:25, iter:12048] Loss: 0.904 | Acc: 67.042% \n",
      "[epoch:25, iter:12049] Loss: 0.905 | Acc: 66.980% \n",
      "[epoch:25, iter:12050] Loss: 0.908 | Acc: 66.940% \n",
      "[epoch:25, iter:12051] Loss: 0.910 | Acc: 66.824% \n",
      "[epoch:25, iter:12052] Loss: 0.910 | Acc: 66.846% \n",
      "[epoch:25, iter:12053] Loss: 0.908 | Acc: 66.887% \n",
      "[epoch:25, iter:12054] Loss: 0.911 | Acc: 66.833% \n",
      "[epoch:25, iter:12055] Loss: 0.911 | Acc: 66.782% \n",
      "[epoch:25, iter:12056] Loss: 0.910 | Acc: 66.714% \n",
      "[epoch:25, iter:12057] Loss: 0.910 | Acc: 66.737% \n",
      "[epoch:25, iter:12058] Loss: 0.907 | Acc: 66.828% \n",
      "[epoch:25, iter:12059] Loss: 0.906 | Acc: 66.814% \n",
      "[epoch:25, iter:12060] Loss: 0.907 | Acc: 66.800% \n",
      "[epoch:25, iter:12061] Loss: 0.908 | Acc: 66.721% \n",
      "[epoch:25, iter:12062] Loss: 0.907 | Acc: 66.694% \n",
      "[epoch:25, iter:12063] Loss: 0.907 | Acc: 66.714% \n",
      "[epoch:25, iter:12064] Loss: 0.905 | Acc: 66.797% \n",
      "[epoch:25, iter:12065] Loss: 0.906 | Acc: 66.738% \n",
      "[epoch:25, iter:12066] Loss: 0.906 | Acc: 66.697% \n",
      "[epoch:25, iter:12067] Loss: 0.906 | Acc: 66.746% \n",
      "[epoch:25, iter:12068] Loss: 0.906 | Acc: 66.721% \n",
      "[epoch:25, iter:12069] Loss: 0.906 | Acc: 66.754% \n",
      "[epoch:25, iter:12070] Loss: 0.907 | Acc: 66.714% \n",
      "[epoch:25, iter:12071] Loss: 0.909 | Acc: 66.746% \n",
      "[epoch:25, iter:12072] Loss: 0.909 | Acc: 66.708% \n",
      "[epoch:25, iter:12073] Loss: 0.911 | Acc: 66.644% \n",
      "[epoch:25, iter:12074] Loss: 0.909 | Acc: 66.770% \n",
      "[epoch:25, iter:12075] Loss: 0.909 | Acc: 66.733% \n",
      "[epoch:25, iter:12076] Loss: 0.909 | Acc: 66.697% \n",
      "[epoch:25, iter:12077] Loss: 0.911 | Acc: 66.649% \n",
      "[epoch:25, iter:12078] Loss: 0.912 | Acc: 66.603% \n",
      "[epoch:25, iter:12079] Loss: 0.911 | Acc: 66.595% \n",
      "[epoch:25, iter:12080] Loss: 0.914 | Acc: 66.575% \n",
      "[epoch:25, iter:12081] Loss: 0.912 | Acc: 66.704% \n",
      "[epoch:25, iter:12082] Loss: 0.912 | Acc: 66.744% \n",
      "[epoch:25, iter:12083] Loss: 0.911 | Acc: 66.771% \n",
      "[epoch:25, iter:12084] Loss: 0.911 | Acc: 66.774% \n",
      "[epoch:25, iter:12085] Loss: 0.910 | Acc: 66.800% \n",
      "[epoch:25, iter:12086] Loss: 0.909 | Acc: 66.791% \n",
      "[epoch:25, iter:12087] Loss: 0.910 | Acc: 66.736% \n",
      "[epoch:25, iter:12088] Loss: 0.912 | Acc: 66.727% \n",
      "[epoch:25, iter:12089] Loss: 0.911 | Acc: 66.719% \n",
      "[epoch:25, iter:12090] Loss: 0.912 | Acc: 66.644% \n",
      "[epoch:25, iter:12091] Loss: 0.911 | Acc: 66.637% \n",
      "[epoch:25, iter:12092] Loss: 0.910 | Acc: 66.674% \n",
      "[epoch:25, iter:12093] Loss: 0.910 | Acc: 66.667% \n",
      "[epoch:25, iter:12094] Loss: 0.908 | Acc: 66.723% \n",
      "[epoch:25, iter:12095] Loss: 0.907 | Acc: 66.821% \n",
      "[epoch:25, iter:12096] Loss: 0.908 | Acc: 66.792% \n",
      "[epoch:25, iter:12097] Loss: 0.907 | Acc: 66.845% \n",
      "[epoch:25, iter:12098] Loss: 0.907 | Acc: 66.816% \n",
      "[epoch:25, iter:12099] Loss: 0.908 | Acc: 66.818% \n",
      "[epoch:25, iter:12100] Loss: 0.906 | Acc: 66.910% \n",
      "[epoch:25, iter:12101] Loss: 0.906 | Acc: 66.931% \n",
      "[epoch:25, iter:12102] Loss: 0.904 | Acc: 67.059% \n",
      "[epoch:25, iter:12103] Loss: 0.905 | Acc: 67.019% \n",
      "[epoch:25, iter:12104] Loss: 0.906 | Acc: 66.981% \n",
      "[epoch:25, iter:12105] Loss: 0.906 | Acc: 67.010% \n",
      "[epoch:25, iter:12106] Loss: 0.904 | Acc: 67.123% \n",
      "[epoch:25, iter:12107] Loss: 0.902 | Acc: 67.196% \n",
      "[epoch:25, iter:12108] Loss: 0.902 | Acc: 67.185% \n",
      "[epoch:25, iter:12109] Loss: 0.902 | Acc: 67.211% \n",
      "[epoch:25, iter:12110] Loss: 0.901 | Acc: 67.245% \n",
      "[epoch:25, iter:12111] Loss: 0.902 | Acc: 67.189% \n",
      "[epoch:25, iter:12112] Loss: 0.902 | Acc: 67.170% \n",
      "[epoch:25, iter:12113] Loss: 0.902 | Acc: 67.212% \n",
      "[epoch:25, iter:12114] Loss: 0.903 | Acc: 67.149% \n",
      "[epoch:25, iter:12115] Loss: 0.903 | Acc: 67.139% \n",
      "[epoch:25, iter:12116] Loss: 0.904 | Acc: 67.172% \n",
      "[epoch:25, iter:12117] Loss: 0.904 | Acc: 67.137% \n",
      "[epoch:25, iter:12118] Loss: 0.903 | Acc: 67.153% \n",
      "[epoch:25, iter:12119] Loss: 0.903 | Acc: 67.193% \n",
      "[epoch:25, iter:12120] Loss: 0.902 | Acc: 67.208% \n",
      "[epoch:25, iter:12121] Loss: 0.903 | Acc: 67.231% \n",
      "[epoch:25, iter:12122] Loss: 0.901 | Acc: 67.311% \n",
      "[epoch:25, iter:12123] Loss: 0.901 | Acc: 67.333% \n",
      "[epoch:25, iter:12124] Loss: 0.901 | Acc: 67.315% \n",
      "[epoch:25, iter:12125] Loss: 0.901 | Acc: 67.296% \n",
      "[epoch:25, iter:12126] Loss: 0.901 | Acc: 67.325% \n",
      "[epoch:25, iter:12127] Loss: 0.901 | Acc: 67.299% \n",
      "[epoch:25, iter:12128] Loss: 0.900 | Acc: 67.336% \n",
      "[epoch:25, iter:12129] Loss: 0.900 | Acc: 67.372% \n",
      "[epoch:25, iter:12130] Loss: 0.899 | Acc: 67.392% \n",
      "[epoch:25, iter:12131] Loss: 0.901 | Acc: 67.359% \n",
      "[epoch:25, iter:12132] Loss: 0.901 | Acc: 67.318% \n",
      "[epoch:25, iter:12133] Loss: 0.901 | Acc: 67.331% \n",
      "[epoch:25, iter:12134] Loss: 0.900 | Acc: 67.343% \n",
      "[epoch:25, iter:12135] Loss: 0.900 | Acc: 67.370% \n",
      "[epoch:25, iter:12136] Loss: 0.899 | Acc: 67.412% \n",
      "[epoch:25, iter:12137] Loss: 0.899 | Acc: 67.445% \n",
      "[epoch:25, iter:12138] Loss: 0.899 | Acc: 67.478% \n",
      "[epoch:25, iter:12139] Loss: 0.898 | Acc: 67.504% \n",
      "[epoch:25, iter:12140] Loss: 0.900 | Acc: 67.457% \n",
      "[epoch:25, iter:12141] Loss: 0.900 | Acc: 67.433% \n",
      "[epoch:25, iter:12142] Loss: 0.900 | Acc: 67.458% \n",
      "[epoch:25, iter:12143] Loss: 0.900 | Acc: 67.462% \n",
      "[epoch:25, iter:12144] Loss: 0.900 | Acc: 67.479% \n",
      "[epoch:25, iter:12145] Loss: 0.899 | Acc: 67.524% \n",
      "[epoch:25, iter:12146] Loss: 0.900 | Acc: 67.541% \n",
      "[epoch:25, iter:12147] Loss: 0.900 | Acc: 67.524% \n",
      "[epoch:25, iter:12148] Loss: 0.900 | Acc: 67.527% \n",
      "[epoch:25, iter:12149] Loss: 0.901 | Acc: 67.544% \n",
      "[epoch:25, iter:12150] Loss: 0.899 | Acc: 67.607% \n",
      "[epoch:25, iter:12151] Loss: 0.900 | Acc: 67.570% \n",
      "[epoch:25, iter:12152] Loss: 0.901 | Acc: 67.546% \n",
      "[epoch:25, iter:12153] Loss: 0.900 | Acc: 67.510% \n",
      "[epoch:25, iter:12154] Loss: 0.902 | Acc: 67.506% \n",
      "[epoch:25, iter:12155] Loss: 0.901 | Acc: 67.529% \n",
      "[epoch:25, iter:12156] Loss: 0.903 | Acc: 67.474% \n",
      "[epoch:25, iter:12157] Loss: 0.903 | Acc: 67.490% \n",
      "[epoch:25, iter:12158] Loss: 0.903 | Acc: 67.468% \n",
      "[epoch:25, iter:12159] Loss: 0.904 | Acc: 67.415% \n",
      "[epoch:25, iter:12160] Loss: 0.903 | Acc: 67.463% \n",
      "[epoch:25, iter:12161] Loss: 0.904 | Acc: 67.441% \n",
      "[epoch:25, iter:12162] Loss: 0.903 | Acc: 67.506% \n",
      "[epoch:25, iter:12163] Loss: 0.903 | Acc: 67.503% \n",
      "[epoch:25, iter:12164] Loss: 0.903 | Acc: 67.543% \n",
      "[epoch:25, iter:12165] Loss: 0.904 | Acc: 67.527% \n",
      "[epoch:25, iter:12166] Loss: 0.904 | Acc: 67.494% \n",
      "[epoch:25, iter:12167] Loss: 0.905 | Acc: 67.479% \n",
      "[epoch:25, iter:12168] Loss: 0.905 | Acc: 67.482% \n",
      "[epoch:25, iter:12169] Loss: 0.904 | Acc: 67.521% \n",
      "[epoch:25, iter:12170] Loss: 0.905 | Acc: 67.524% \n",
      "[epoch:25, iter:12171] Loss: 0.905 | Acc: 67.503% \n",
      "[epoch:25, iter:12172] Loss: 0.906 | Acc: 67.442% \n",
      "[epoch:25, iter:12173] Loss: 0.907 | Acc: 67.428% \n",
      "[epoch:25, iter:12174] Loss: 0.907 | Acc: 67.420% \n",
      "[epoch:25, iter:12175] Loss: 0.907 | Acc: 67.417% \n",
      "[epoch:25, iter:12176] Loss: 0.908 | Acc: 67.403% \n",
      "[epoch:25, iter:12177] Loss: 0.906 | Acc: 67.469% \n",
      "[epoch:25, iter:12178] Loss: 0.906 | Acc: 67.494% \n",
      "[epoch:25, iter:12179] Loss: 0.905 | Acc: 67.536% \n",
      "[epoch:25, iter:12180] Loss: 0.905 | Acc: 67.528% \n",
      "[epoch:25, iter:12181] Loss: 0.905 | Acc: 67.519% \n",
      "[epoch:25, iter:12182] Loss: 0.906 | Acc: 67.478% \n",
      "[epoch:25, iter:12183] Loss: 0.906 | Acc: 67.486% \n",
      "[epoch:25, iter:12184] Loss: 0.906 | Acc: 67.457% \n",
      "[epoch:25, iter:12185] Loss: 0.905 | Acc: 67.470% \n",
      "[epoch:25, iter:12186] Loss: 0.905 | Acc: 67.468% \n",
      "[epoch:25, iter:12187] Loss: 0.905 | Acc: 67.476% \n",
      "[epoch:25, iter:12188] Loss: 0.905 | Acc: 67.463% \n",
      "[epoch:25, iter:12189] Loss: 0.905 | Acc: 67.466% \n",
      "[epoch:25, iter:12190] Loss: 0.905 | Acc: 67.468% \n",
      "[epoch:25, iter:12191] Loss: 0.905 | Acc: 67.482% \n",
      "[epoch:25, iter:12192] Loss: 0.904 | Acc: 67.521% \n",
      "[epoch:25, iter:12193] Loss: 0.904 | Acc: 67.523% \n",
      "[epoch:25, iter:12194] Loss: 0.905 | Acc: 67.485% \n",
      "[epoch:25, iter:12195] Loss: 0.905 | Acc: 67.482% \n",
      "[epoch:25, iter:12196] Loss: 0.906 | Acc: 67.474% \n",
      "[epoch:25, iter:12197] Loss: 0.905 | Acc: 67.477% \n",
      "[epoch:25, iter:12198] Loss: 0.904 | Acc: 67.535% \n",
      "[epoch:25, iter:12199] Loss: 0.903 | Acc: 67.563% \n",
      "[epoch:25, iter:12200] Loss: 0.903 | Acc: 67.575% \n",
      "[epoch:25, iter:12201] Loss: 0.903 | Acc: 67.567% \n",
      "[epoch:25, iter:12202] Loss: 0.903 | Acc: 67.545% \n",
      "[epoch:25, iter:12203] Loss: 0.903 | Acc: 67.571% \n",
      "[epoch:25, iter:12204] Loss: 0.903 | Acc: 67.564% \n",
      "[epoch:25, iter:12205] Loss: 0.904 | Acc: 67.551% \n",
      "[epoch:25, iter:12206] Loss: 0.903 | Acc: 67.573% \n",
      "[epoch:25, iter:12207] Loss: 0.903 | Acc: 67.585% \n",
      "[epoch:25, iter:12208] Loss: 0.903 | Acc: 67.596% \n",
      "[epoch:25, iter:12209] Loss: 0.903 | Acc: 67.565% \n",
      "[epoch:25, iter:12210] Loss: 0.903 | Acc: 67.557% \n",
      "[epoch:25, iter:12211] Loss: 0.904 | Acc: 67.555% \n",
      "[epoch:25, iter:12212] Loss: 0.903 | Acc: 67.561% \n",
      "[epoch:25, iter:12213] Loss: 0.904 | Acc: 67.540% \n",
      "[epoch:25, iter:12214] Loss: 0.903 | Acc: 67.523% \n",
      "[epoch:25, iter:12215] Loss: 0.903 | Acc: 67.535% \n",
      "[epoch:25, iter:12216] Loss: 0.903 | Acc: 67.532% \n",
      "[epoch:25, iter:12217] Loss: 0.903 | Acc: 67.512% \n",
      "[epoch:25, iter:12218] Loss: 0.902 | Acc: 67.523% \n",
      "[epoch:25, iter:12219] Loss: 0.903 | Acc: 67.530% \n",
      "[epoch:25, iter:12220] Loss: 0.902 | Acc: 67.545% \n",
      "[epoch:25, iter:12221] Loss: 0.903 | Acc: 67.502% \n",
      "[epoch:25, iter:12222] Loss: 0.903 | Acc: 67.500% \n",
      "[epoch:25, iter:12223] Loss: 0.903 | Acc: 67.498% \n",
      "[epoch:25, iter:12224] Loss: 0.904 | Acc: 67.464% \n",
      "[epoch:25, iter:12225] Loss: 0.904 | Acc: 67.449% \n",
      "[epoch:25, iter:12226] Loss: 0.903 | Acc: 67.478% \n",
      "[epoch:25, iter:12227] Loss: 0.903 | Acc: 67.480% \n",
      "[epoch:25, iter:12228] Loss: 0.904 | Acc: 67.474% \n",
      "[epoch:25, iter:12229] Loss: 0.903 | Acc: 67.493% \n",
      "[epoch:25, iter:12230] Loss: 0.903 | Acc: 67.496% \n",
      "[epoch:25, iter:12231] Loss: 0.903 | Acc: 67.489% \n",
      "[epoch:25, iter:12232] Loss: 0.904 | Acc: 67.474% \n",
      "[epoch:25, iter:12233] Loss: 0.903 | Acc: 67.489% \n",
      "[epoch:25, iter:12234] Loss: 0.904 | Acc: 67.466% \n",
      "[epoch:25, iter:12235] Loss: 0.904 | Acc: 67.443% \n",
      "[epoch:25, iter:12236] Loss: 0.904 | Acc: 67.445% \n",
      "[epoch:25, iter:12237] Loss: 0.904 | Acc: 67.435% \n",
      "[epoch:25, iter:12238] Loss: 0.904 | Acc: 67.445% \n",
      "[epoch:25, iter:12239] Loss: 0.905 | Acc: 67.452% \n",
      "[epoch:25, iter:12240] Loss: 0.905 | Acc: 67.463% \n",
      "[epoch:25, iter:12241] Loss: 0.905 | Acc: 67.419% \n",
      "[epoch:25, iter:12242] Loss: 0.905 | Acc: 67.430% \n",
      "[epoch:25, iter:12243] Loss: 0.905 | Acc: 67.403% \n",
      "[epoch:25, iter:12244] Loss: 0.906 | Acc: 67.385% \n",
      "[epoch:25, iter:12245] Loss: 0.906 | Acc: 67.363% \n",
      "[epoch:25, iter:12246] Loss: 0.907 | Acc: 67.354% \n",
      "[epoch:25, iter:12247] Loss: 0.906 | Acc: 67.364% \n",
      "[epoch:25, iter:12248] Loss: 0.906 | Acc: 67.363% \n",
      "[epoch:25, iter:12249] Loss: 0.906 | Acc: 67.361% \n",
      "[epoch:25, iter:12250] Loss: 0.905 | Acc: 67.372% \n",
      "[epoch:25, iter:12251] Loss: 0.905 | Acc: 67.378% \n",
      "[epoch:25, iter:12252] Loss: 0.906 | Acc: 67.337% \n",
      "[epoch:25, iter:12253] Loss: 0.906 | Acc: 67.348% \n",
      "[epoch:25, iter:12254] Loss: 0.906 | Acc: 67.315% \n",
      "[epoch:25, iter:12255] Loss: 0.906 | Acc: 67.322% \n",
      "[epoch:25, iter:12256] Loss: 0.905 | Acc: 67.328% \n",
      "[epoch:25, iter:12257] Loss: 0.905 | Acc: 67.374% \n",
      "[epoch:25, iter:12258] Loss: 0.905 | Acc: 67.353% \n",
      "[epoch:25, iter:12259] Loss: 0.904 | Acc: 67.382% \n",
      "[epoch:25, iter:12260] Loss: 0.904 | Acc: 67.400% \n",
      "[epoch:25, iter:12261] Loss: 0.904 | Acc: 67.402% \n",
      "[epoch:25, iter:12262] Loss: 0.903 | Acc: 67.412% \n",
      "[epoch:25, iter:12263] Loss: 0.904 | Acc: 67.373% \n",
      "[epoch:25, iter:12264] Loss: 0.905 | Acc: 67.360% \n",
      "[epoch:25, iter:12265] Loss: 0.904 | Acc: 67.404% \n",
      "[epoch:25, iter:12266] Loss: 0.904 | Acc: 67.429% \n",
      "[epoch:25, iter:12267] Loss: 0.904 | Acc: 67.431% \n",
      "[epoch:25, iter:12268] Loss: 0.903 | Acc: 67.448% \n",
      "[epoch:25, iter:12269] Loss: 0.903 | Acc: 67.480% \n",
      "[epoch:25, iter:12270] Loss: 0.903 | Acc: 67.474% \n",
      "[epoch:25, iter:12271] Loss: 0.902 | Acc: 67.469% \n",
      "[epoch:25, iter:12272] Loss: 0.902 | Acc: 67.463% \n",
      "[epoch:25, iter:12273] Loss: 0.902 | Acc: 67.465% \n",
      "[epoch:25, iter:12274] Loss: 0.902 | Acc: 67.478% \n",
      "[epoch:25, iter:12275] Loss: 0.902 | Acc: 67.495% \n",
      "[epoch:25, iter:12276] Loss: 0.902 | Acc: 67.514% \n",
      "[epoch:25, iter:12277] Loss: 0.902 | Acc: 67.513% \n",
      "[epoch:25, iter:12278] Loss: 0.902 | Acc: 67.518% \n",
      "[epoch:25, iter:12279] Loss: 0.903 | Acc: 67.487% \n",
      "[epoch:25, iter:12280] Loss: 0.903 | Acc: 67.475% \n",
      "[epoch:25, iter:12281] Loss: 0.903 | Acc: 67.470% \n",
      "[epoch:25, iter:12282] Loss: 0.903 | Acc: 67.493% \n",
      "[epoch:25, iter:12283] Loss: 0.903 | Acc: 67.491% \n",
      "[epoch:25, iter:12284] Loss: 0.903 | Acc: 67.482% \n",
      "[epoch:25, iter:12285] Loss: 0.903 | Acc: 67.484% \n",
      "[epoch:25, iter:12286] Loss: 0.903 | Acc: 67.514% \n",
      "[epoch:25, iter:12287] Loss: 0.903 | Acc: 67.502% \n",
      "[epoch:25, iter:12288] Loss: 0.902 | Acc: 67.493% \n",
      "[epoch:25, iter:12289] Loss: 0.902 | Acc: 67.512% \n",
      "[epoch:25, iter:12290] Loss: 0.902 | Acc: 67.517% \n",
      "[epoch:25, iter:12291] Loss: 0.902 | Acc: 67.512% \n",
      "[epoch:25, iter:12292] Loss: 0.903 | Acc: 67.497% \n",
      "[epoch:25, iter:12293] Loss: 0.902 | Acc: 67.512% \n",
      "[epoch:25, iter:12294] Loss: 0.902 | Acc: 67.500% \n",
      "[epoch:25, iter:12295] Loss: 0.903 | Acc: 67.492% \n",
      "[epoch:25, iter:12296] Loss: 0.902 | Acc: 67.507% \n",
      "[epoch:25, iter:12297] Loss: 0.902 | Acc: 67.505% \n",
      "[epoch:25, iter:12298] Loss: 0.902 | Acc: 67.517% \n",
      "[epoch:25, iter:12299] Loss: 0.902 | Acc: 67.532% \n",
      "[epoch:25, iter:12300] Loss: 0.902 | Acc: 67.527% \n",
      "[epoch:25, iter:12301] Loss: 0.902 | Acc: 67.545% \n",
      "[epoch:25, iter:12302] Loss: 0.902 | Acc: 67.523% \n",
      "[epoch:25, iter:12303] Loss: 0.902 | Acc: 67.528% \n",
      "[epoch:25, iter:12304] Loss: 0.901 | Acc: 67.539% \n",
      "[epoch:25, iter:12305] Loss: 0.901 | Acc: 67.548% \n",
      "[epoch:25, iter:12306] Loss: 0.901 | Acc: 67.556% \n",
      "[epoch:25, iter:12307] Loss: 0.901 | Acc: 67.547% \n",
      "[epoch:25, iter:12308] Loss: 0.901 | Acc: 67.526% \n",
      "[epoch:25, iter:12309] Loss: 0.901 | Acc: 67.537% \n",
      "[epoch:25, iter:12310] Loss: 0.901 | Acc: 67.545% \n",
      "[epoch:25, iter:12311] Loss: 0.902 | Acc: 67.534% \n",
      "[epoch:25, iter:12312] Loss: 0.902 | Acc: 67.526% \n",
      "[epoch:25, iter:12313] Loss: 0.901 | Acc: 67.537% \n",
      "[epoch:25, iter:12314] Loss: 0.901 | Acc: 67.538% \n",
      "[epoch:25, iter:12315] Loss: 0.901 | Acc: 67.549% \n",
      "[epoch:25, iter:12316] Loss: 0.901 | Acc: 67.563% \n",
      "[epoch:25, iter:12317] Loss: 0.902 | Acc: 67.530% \n",
      "[epoch:25, iter:12318] Loss: 0.902 | Acc: 67.535% \n",
      "[epoch:25, iter:12319] Loss: 0.902 | Acc: 67.524% \n",
      "[epoch:25, iter:12320] Loss: 0.902 | Acc: 67.516% \n",
      "[epoch:25, iter:12321] Loss: 0.902 | Acc: 67.542% \n",
      "[epoch:25, iter:12322] Loss: 0.903 | Acc: 67.537% \n",
      "[epoch:25, iter:12323] Loss: 0.903 | Acc: 67.539% \n",
      "[epoch:25, iter:12324] Loss: 0.903 | Acc: 67.534% \n",
      "[epoch:25, iter:12325] Loss: 0.903 | Acc: 67.523% \n",
      "[epoch:25, iter:12326] Loss: 0.904 | Acc: 67.518% \n",
      "[epoch:25, iter:12327] Loss: 0.904 | Acc: 67.523% \n",
      "[epoch:25, iter:12328] Loss: 0.903 | Acc: 67.524% \n",
      "[epoch:25, iter:12329] Loss: 0.903 | Acc: 67.520% \n",
      "[epoch:25, iter:12330] Loss: 0.903 | Acc: 67.512% \n",
      "[epoch:25, iter:12331] Loss: 0.903 | Acc: 67.514% \n",
      "[epoch:25, iter:12332] Loss: 0.903 | Acc: 67.542% \n",
      "[epoch:25, iter:12333] Loss: 0.904 | Acc: 67.529% \n",
      "[epoch:25, iter:12334] Loss: 0.903 | Acc: 67.530% \n",
      "[epoch:25, iter:12335] Loss: 0.904 | Acc: 67.537% \n",
      "[epoch:25, iter:12336] Loss: 0.904 | Acc: 67.527% \n",
      "[epoch:25, iter:12337] Loss: 0.904 | Acc: 67.540% \n",
      "[epoch:25, iter:12338] Loss: 0.904 | Acc: 67.521% \n",
      "[epoch:25, iter:12339] Loss: 0.905 | Acc: 67.496% \n",
      "[epoch:25, iter:12340] Loss: 0.905 | Acc: 67.503% \n",
      "[epoch:25, iter:12341] Loss: 0.905 | Acc: 67.501% \n",
      "[epoch:25, iter:12342] Loss: 0.905 | Acc: 67.480% \n",
      "[epoch:25, iter:12343] Loss: 0.906 | Acc: 67.449% \n",
      "[epoch:25, iter:12344] Loss: 0.906 | Acc: 67.480% \n",
      "[epoch:25, iter:12345] Loss: 0.906 | Acc: 67.481% \n",
      "[epoch:25, iter:12346] Loss: 0.906 | Acc: 67.497% \n",
      "[epoch:25, iter:12347] Loss: 0.905 | Acc: 67.527% \n",
      "[epoch:25, iter:12348] Loss: 0.905 | Acc: 67.543% \n",
      "[epoch:25, iter:12349] Loss: 0.905 | Acc: 67.562% \n",
      "[epoch:25, iter:12350] Loss: 0.905 | Acc: 67.569% \n",
      "[epoch:25, iter:12351] Loss: 0.905 | Acc: 67.564% \n",
      "[epoch:25, iter:12352] Loss: 0.905 | Acc: 67.568% \n",
      "[epoch:25, iter:12353] Loss: 0.905 | Acc: 67.581% \n",
      "[epoch:25, iter:12354] Loss: 0.905 | Acc: 67.568% \n",
      "[epoch:25, iter:12355] Loss: 0.905 | Acc: 67.577% \n",
      "[epoch:25, iter:12356] Loss: 0.905 | Acc: 67.576% \n",
      "[epoch:25, iter:12357] Loss: 0.905 | Acc: 67.574% \n",
      "[epoch:25, iter:12358] Loss: 0.905 | Acc: 67.559% \n",
      "[epoch:25, iter:12359] Loss: 0.905 | Acc: 67.560% \n",
      "[epoch:25, iter:12360] Loss: 0.905 | Acc: 67.567% \n",
      "[epoch:25, iter:12361] Loss: 0.904 | Acc: 67.571% \n",
      "[epoch:25, iter:12362] Loss: 0.904 | Acc: 67.572% \n",
      "[epoch:25, iter:12363] Loss: 0.905 | Acc: 67.556% \n",
      "[epoch:25, iter:12364] Loss: 0.905 | Acc: 67.530% \n",
      "[epoch:25, iter:12365] Loss: 0.905 | Acc: 67.534% \n",
      "[epoch:25, iter:12366] Loss: 0.905 | Acc: 67.552% \n",
      "[epoch:25, iter:12367] Loss: 0.905 | Acc: 67.542% \n",
      "[epoch:25, iter:12368] Loss: 0.905 | Acc: 67.557% \n",
      "[epoch:25, iter:12369] Loss: 0.905 | Acc: 67.572% \n",
      "[epoch:25, iter:12370] Loss: 0.904 | Acc: 67.589% \n",
      "[epoch:25, iter:12371] Loss: 0.904 | Acc: 67.601% \n",
      "[epoch:25, iter:12372] Loss: 0.904 | Acc: 67.583% \n",
      "[epoch:25, iter:12373] Loss: 0.904 | Acc: 67.598% \n",
      "[epoch:25, iter:12374] Loss: 0.904 | Acc: 67.615% \n",
      "[epoch:25, iter:12375] Loss: 0.903 | Acc: 67.632% \n",
      "[epoch:25, iter:12376] Loss: 0.903 | Acc: 67.641% \n",
      "[epoch:25, iter:12377] Loss: 0.903 | Acc: 67.653% \n",
      "[epoch:25, iter:12378] Loss: 0.902 | Acc: 67.675% \n",
      "[epoch:25, iter:12379] Loss: 0.902 | Acc: 67.652% \n",
      "[epoch:25, iter:12380] Loss: 0.902 | Acc: 67.679% \n",
      "[epoch:25, iter:12381] Loss: 0.901 | Acc: 67.685% \n",
      "[epoch:25, iter:12382] Loss: 0.901 | Acc: 67.678% \n",
      "[epoch:25, iter:12383] Loss: 0.901 | Acc: 67.674% \n",
      "[epoch:25, iter:12384] Loss: 0.901 | Acc: 67.661% \n",
      "[epoch:25, iter:12385] Loss: 0.901 | Acc: 67.673% \n",
      "[epoch:25, iter:12386] Loss: 0.901 | Acc: 67.676% \n",
      "[epoch:25, iter:12387] Loss: 0.901 | Acc: 67.659% \n",
      "[epoch:25, iter:12388] Loss: 0.901 | Acc: 67.688% \n",
      "[epoch:25, iter:12389] Loss: 0.901 | Acc: 67.692% \n",
      "[epoch:25, iter:12390] Loss: 0.901 | Acc: 67.690% \n",
      "[epoch:25, iter:12391] Loss: 0.901 | Acc: 67.703% \n",
      "[epoch:25, iter:12392] Loss: 0.901 | Acc: 67.689% \n",
      "[epoch:25, iter:12393] Loss: 0.901 | Acc: 67.695% \n",
      "[epoch:25, iter:12394] Loss: 0.901 | Acc: 67.695% \n",
      "[epoch:25, iter:12395] Loss: 0.901 | Acc: 67.719% \n",
      "[epoch:25, iter:12396] Loss: 0.901 | Acc: 67.715% \n",
      "[epoch:25, iter:12397] Loss: 0.901 | Acc: 67.713% \n",
      "[epoch:25, iter:12398] Loss: 0.902 | Acc: 67.671% \n",
      "[epoch:25, iter:12399] Loss: 0.901 | Acc: 67.672% \n",
      "[epoch:25, iter:12400] Loss: 0.901 | Acc: 67.680% \n",
      "[epoch:25, iter:12401] Loss: 0.901 | Acc: 67.676% \n",
      "[epoch:25, iter:12402] Loss: 0.902 | Acc: 67.657% \n",
      "[epoch:25, iter:12403] Loss: 0.902 | Acc: 67.665% \n",
      "[epoch:25, iter:12404] Loss: 0.901 | Acc: 67.686% \n",
      "[epoch:25, iter:12405] Loss: 0.901 | Acc: 67.679% \n",
      "[epoch:25, iter:12406] Loss: 0.901 | Acc: 67.687% \n",
      "[epoch:25, iter:12407] Loss: 0.902 | Acc: 67.668% \n",
      "[epoch:25, iter:12408] Loss: 0.902 | Acc: 67.667% \n",
      "[epoch:25, iter:12409] Loss: 0.902 | Acc: 67.670% \n",
      "[epoch:25, iter:12410] Loss: 0.901 | Acc: 67.688% \n",
      "[epoch:25, iter:12411] Loss: 0.901 | Acc: 67.703% \n",
      "[epoch:25, iter:12412] Loss: 0.901 | Acc: 67.697% \n",
      "[epoch:25, iter:12413] Loss: 0.901 | Acc: 67.688% \n",
      "[epoch:25, iter:12414] Loss: 0.901 | Acc: 67.696% \n",
      "[epoch:25, iter:12415] Loss: 0.901 | Acc: 67.699% \n",
      "[epoch:25, iter:12416] Loss: 0.900 | Acc: 67.704% \n",
      "[epoch:25, iter:12417] Loss: 0.901 | Acc: 67.707% \n",
      "[epoch:25, iter:12418] Loss: 0.901 | Acc: 67.701% \n",
      "[epoch:25, iter:12419] Loss: 0.901 | Acc: 67.695% \n",
      "[epoch:25, iter:12420] Loss: 0.900 | Acc: 67.700% \n",
      "[epoch:25, iter:12421] Loss: 0.900 | Acc: 67.708% \n",
      "[epoch:25, iter:12422] Loss: 0.901 | Acc: 67.709% \n",
      "[epoch:25, iter:12423] Loss: 0.901 | Acc: 67.688% \n",
      "[epoch:25, iter:12424] Loss: 0.901 | Acc: 67.682% \n",
      "[epoch:25, iter:12425] Loss: 0.901 | Acc: 67.682% \n",
      "[epoch:25, iter:12426] Loss: 0.901 | Acc: 67.683% \n",
      "[epoch:25, iter:12427] Loss: 0.902 | Acc: 67.689% \n",
      "[epoch:25, iter:12428] Loss: 0.902 | Acc: 67.687% \n",
      "[epoch:25, iter:12429] Loss: 0.902 | Acc: 67.702% \n",
      "[epoch:25, iter:12430] Loss: 0.902 | Acc: 67.702% \n",
      "[epoch:25, iter:12431] Loss: 0.902 | Acc: 67.712% \n",
      "[epoch:25, iter:12432] Loss: 0.902 | Acc: 67.715% \n",
      "[epoch:25, iter:12433] Loss: 0.902 | Acc: 67.723% \n",
      "[epoch:25, iter:12434] Loss: 0.902 | Acc: 67.728% \n",
      "[epoch:25, iter:12435] Loss: 0.902 | Acc: 67.733% \n",
      "[epoch:25, iter:12436] Loss: 0.901 | Acc: 67.745% \n",
      "[epoch:25, iter:12437] Loss: 0.901 | Acc: 67.744% \n",
      "[epoch:25, iter:12438] Loss: 0.902 | Acc: 67.740% \n",
      "[epoch:25, iter:12439] Loss: 0.902 | Acc: 67.743% \n",
      "[epoch:25, iter:12440] Loss: 0.902 | Acc: 67.748% \n",
      "[epoch:25, iter:12441] Loss: 0.902 | Acc: 67.748% \n",
      "[epoch:25, iter:12442] Loss: 0.902 | Acc: 67.749% \n",
      "[epoch:25, iter:12443] Loss: 0.902 | Acc: 67.756% \n",
      "[epoch:25, iter:12444] Loss: 0.901 | Acc: 67.761% \n",
      "[epoch:25, iter:12445] Loss: 0.901 | Acc: 67.762% \n",
      "[epoch:25, iter:12446] Loss: 0.901 | Acc: 67.776% \n",
      "[epoch:25, iter:12447] Loss: 0.901 | Acc: 67.770% \n",
      "[epoch:25, iter:12448] Loss: 0.901 | Acc: 67.781% \n",
      "[epoch:25, iter:12449] Loss: 0.902 | Acc: 67.757% \n",
      "[epoch:25, iter:12450] Loss: 0.902 | Acc: 67.756% \n",
      "[epoch:25, iter:12451] Loss: 0.902 | Acc: 67.761% \n",
      "[epoch:25, iter:12452] Loss: 0.901 | Acc: 67.759% \n",
      "[epoch:25, iter:12453] Loss: 0.902 | Acc: 67.757% \n",
      "[epoch:25, iter:12454] Loss: 0.901 | Acc: 67.780% \n",
      "[epoch:25, iter:12455] Loss: 0.901 | Acc: 67.791% \n",
      "[epoch:25, iter:12456] Loss: 0.901 | Acc: 67.787% \n",
      "[epoch:25, iter:12457] Loss: 0.901 | Acc: 67.797% \n",
      "[epoch:25, iter:12458] Loss: 0.901 | Acc: 67.790% \n",
      "[epoch:25, iter:12459] Loss: 0.900 | Acc: 67.810% \n",
      "[epoch:25, iter:12460] Loss: 0.901 | Acc: 67.807% \n",
      "[epoch:25, iter:12461] Loss: 0.901 | Acc: 67.794% \n",
      "[epoch:25, iter:12462] Loss: 0.901 | Acc: 67.790% \n",
      "[epoch:25, iter:12463] Loss: 0.901 | Acc: 67.786% \n",
      "[epoch:25, iter:12464] Loss: 0.901 | Acc: 67.789% \n",
      "[epoch:25, iter:12465] Loss: 0.900 | Acc: 67.809% \n",
      "[epoch:25, iter:12466] Loss: 0.900 | Acc: 67.826% \n",
      "[epoch:25, iter:12467] Loss: 0.900 | Acc: 67.833% \n",
      "[epoch:25, iter:12468] Loss: 0.899 | Acc: 67.857% \n",
      "[epoch:25, iter:12469] Loss: 0.899 | Acc: 67.859% \n",
      "[epoch:25, iter:12470] Loss: 0.899 | Acc: 67.857% \n",
      "[epoch:25, iter:12471] Loss: 0.899 | Acc: 67.854% \n",
      "[epoch:25, iter:12472] Loss: 0.899 | Acc: 67.867% \n",
      "[epoch:25, iter:12473] Loss: 0.899 | Acc: 67.863% \n",
      "[epoch:25, iter:12474] Loss: 0.899 | Acc: 67.857% \n",
      "[epoch:25, iter:12475] Loss: 0.899 | Acc: 67.859% \n",
      "[epoch:25, iter:12476] Loss: 0.899 | Acc: 67.866% \n",
      "[epoch:25, iter:12477] Loss: 0.899 | Acc: 67.857% \n",
      "[epoch:25, iter:12478] Loss: 0.899 | Acc: 67.870% \n",
      "[epoch:25, iter:12479] Loss: 0.899 | Acc: 67.864% \n",
      "[epoch:25, iter:12480] Loss: 0.898 | Acc: 67.887% \n",
      "[epoch:25, iter:12481] Loss: 0.898 | Acc: 67.894% \n",
      "[epoch:25, iter:12482] Loss: 0.898 | Acc: 67.890% \n",
      "[epoch:25, iter:12483] Loss: 0.898 | Acc: 67.909% \n",
      "[epoch:25, iter:12484] Loss: 0.898 | Acc: 67.907% \n",
      "[epoch:25, iter:12485] Loss: 0.898 | Acc: 67.905% \n",
      "[epoch:25, iter:12486] Loss: 0.898 | Acc: 67.883% \n",
      "[epoch:25, iter:12487] Loss: 0.898 | Acc: 67.883% \n",
      "[epoch:25, iter:12488] Loss: 0.898 | Acc: 67.875% \n",
      "[epoch:25, iter:12489] Loss: 0.898 | Acc: 67.877% \n",
      "[epoch:25, iter:12490] Loss: 0.898 | Acc: 67.867% \n",
      "[epoch:25, iter:12491] Loss: 0.898 | Acc: 67.864% \n",
      "[epoch:25, iter:12492] Loss: 0.898 | Acc: 67.843% \n",
      "[epoch:25, iter:12493] Loss: 0.898 | Acc: 67.854% \n",
      "[epoch:25, iter:12494] Loss: 0.898 | Acc: 67.850% \n",
      "[epoch:25, iter:12495] Loss: 0.898 | Acc: 67.853% \n",
      "[epoch:25, iter:12496] Loss: 0.899 | Acc: 67.829% \n",
      "[epoch:25, iter:12497] Loss: 0.898 | Acc: 67.835% \n",
      "[epoch:25, iter:12498] Loss: 0.898 | Acc: 67.833% \n",
      "[epoch:25, iter:12499] Loss: 0.898 | Acc: 67.838% \n",
      "[epoch:25, iter:12500] Loss: 0.898 | Acc: 67.848% \n",
      "Waiting Test...\n",
      "Test's ac is: 64.380%\n",
      "\n",
      "Epoch: 26\n",
      "[epoch:26, iter:12501] Loss: 0.950 | Acc: 66.000% \n",
      "[epoch:26, iter:12502] Loss: 0.885 | Acc: 68.500% \n",
      "[epoch:26, iter:12503] Loss: 0.943 | Acc: 67.000% \n",
      "[epoch:26, iter:12504] Loss: 0.968 | Acc: 64.750% \n",
      "[epoch:26, iter:12505] Loss: 0.967 | Acc: 64.600% \n",
      "[epoch:26, iter:12506] Loss: 0.942 | Acc: 65.500% \n",
      "[epoch:26, iter:12507] Loss: 0.938 | Acc: 65.714% \n",
      "[epoch:26, iter:12508] Loss: 0.922 | Acc: 66.500% \n",
      "[epoch:26, iter:12509] Loss: 0.921 | Acc: 66.556% \n",
      "[epoch:26, iter:12510] Loss: 0.926 | Acc: 66.900% \n",
      "[epoch:26, iter:12511] Loss: 0.920 | Acc: 67.273% \n",
      "[epoch:26, iter:12512] Loss: 0.912 | Acc: 68.000% \n",
      "[epoch:26, iter:12513] Loss: 0.924 | Acc: 67.308% \n",
      "[epoch:26, iter:12514] Loss: 0.904 | Acc: 68.143% \n",
      "[epoch:26, iter:12515] Loss: 0.888 | Acc: 68.867% \n",
      "[epoch:26, iter:12516] Loss: 0.901 | Acc: 68.500% \n",
      "[epoch:26, iter:12517] Loss: 0.888 | Acc: 68.941% \n",
      "[epoch:26, iter:12518] Loss: 0.879 | Acc: 69.056% \n",
      "[epoch:26, iter:12519] Loss: 0.886 | Acc: 68.895% \n",
      "[epoch:26, iter:12520] Loss: 0.886 | Acc: 68.900% \n",
      "[epoch:26, iter:12521] Loss: 0.887 | Acc: 68.810% \n",
      "[epoch:26, iter:12522] Loss: 0.882 | Acc: 68.864% \n",
      "[epoch:26, iter:12523] Loss: 0.882 | Acc: 68.870% \n",
      "[epoch:26, iter:12524] Loss: 0.879 | Acc: 69.042% \n",
      "[epoch:26, iter:12525] Loss: 0.877 | Acc: 69.160% \n",
      "[epoch:26, iter:12526] Loss: 0.878 | Acc: 69.154% \n",
      "[epoch:26, iter:12527] Loss: 0.879 | Acc: 69.037% \n",
      "[epoch:26, iter:12528] Loss: 0.887 | Acc: 68.786% \n",
      "[epoch:26, iter:12529] Loss: 0.884 | Acc: 68.759% \n",
      "[epoch:26, iter:12530] Loss: 0.885 | Acc: 68.800% \n",
      "[epoch:26, iter:12531] Loss: 0.880 | Acc: 68.935% \n",
      "[epoch:26, iter:12532] Loss: 0.880 | Acc: 68.875% \n",
      "[epoch:26, iter:12533] Loss: 0.883 | Acc: 68.758% \n",
      "[epoch:26, iter:12534] Loss: 0.882 | Acc: 68.676% \n",
      "[epoch:26, iter:12535] Loss: 0.882 | Acc: 68.686% \n",
      "[epoch:26, iter:12536] Loss: 0.881 | Acc: 68.639% \n",
      "[epoch:26, iter:12537] Loss: 0.882 | Acc: 68.541% \n",
      "[epoch:26, iter:12538] Loss: 0.880 | Acc: 68.474% \n",
      "[epoch:26, iter:12539] Loss: 0.880 | Acc: 68.564% \n",
      "[epoch:26, iter:12540] Loss: 0.882 | Acc: 68.375% \n",
      "[epoch:26, iter:12541] Loss: 0.882 | Acc: 68.244% \n",
      "[epoch:26, iter:12542] Loss: 0.884 | Acc: 68.095% \n",
      "[epoch:26, iter:12543] Loss: 0.887 | Acc: 68.023% \n",
      "[epoch:26, iter:12544] Loss: 0.884 | Acc: 68.227% \n",
      "[epoch:26, iter:12545] Loss: 0.881 | Acc: 68.378% \n",
      "[epoch:26, iter:12546] Loss: 0.884 | Acc: 68.196% \n",
      "[epoch:26, iter:12547] Loss: 0.883 | Acc: 68.213% \n",
      "[epoch:26, iter:12548] Loss: 0.883 | Acc: 68.125% \n",
      "[epoch:26, iter:12549] Loss: 0.887 | Acc: 67.939% \n",
      "[epoch:26, iter:12550] Loss: 0.890 | Acc: 67.800% \n",
      "[epoch:26, iter:12551] Loss: 0.889 | Acc: 67.863% \n",
      "[epoch:26, iter:12552] Loss: 0.888 | Acc: 68.019% \n",
      "[epoch:26, iter:12553] Loss: 0.886 | Acc: 68.000% \n",
      "[epoch:26, iter:12554] Loss: 0.889 | Acc: 67.852% \n",
      "[epoch:26, iter:12555] Loss: 0.888 | Acc: 67.927% \n",
      "[epoch:26, iter:12556] Loss: 0.887 | Acc: 67.982% \n",
      "[epoch:26, iter:12557] Loss: 0.887 | Acc: 67.965% \n",
      "[epoch:26, iter:12558] Loss: 0.890 | Acc: 67.931% \n",
      "[epoch:26, iter:12559] Loss: 0.892 | Acc: 67.814% \n",
      "[epoch:26, iter:12560] Loss: 0.891 | Acc: 67.867% \n",
      "[epoch:26, iter:12561] Loss: 0.889 | Acc: 67.934% \n",
      "[epoch:26, iter:12562] Loss: 0.889 | Acc: 67.919% \n",
      "[epoch:26, iter:12563] Loss: 0.890 | Acc: 67.841% \n",
      "[epoch:26, iter:12564] Loss: 0.886 | Acc: 67.969% \n",
      "[epoch:26, iter:12565] Loss: 0.887 | Acc: 67.862% \n",
      "[epoch:26, iter:12566] Loss: 0.886 | Acc: 67.939% \n",
      "[epoch:26, iter:12567] Loss: 0.888 | Acc: 67.821% \n",
      "[epoch:26, iter:12568] Loss: 0.887 | Acc: 67.926% \n",
      "[epoch:26, iter:12569] Loss: 0.888 | Acc: 67.884% \n",
      "[epoch:26, iter:12570] Loss: 0.886 | Acc: 67.900% \n",
      "[epoch:26, iter:12571] Loss: 0.884 | Acc: 67.901% \n",
      "[epoch:26, iter:12572] Loss: 0.884 | Acc: 67.931% \n",
      "[epoch:26, iter:12573] Loss: 0.885 | Acc: 67.918% \n",
      "[epoch:26, iter:12574] Loss: 0.883 | Acc: 68.000% \n",
      "[epoch:26, iter:12575] Loss: 0.885 | Acc: 67.933% \n",
      "[epoch:26, iter:12576] Loss: 0.886 | Acc: 67.868% \n",
      "[epoch:26, iter:12577] Loss: 0.886 | Acc: 67.831% \n",
      "[epoch:26, iter:12578] Loss: 0.885 | Acc: 67.923% \n",
      "[epoch:26, iter:12579] Loss: 0.886 | Acc: 67.747% \n",
      "[epoch:26, iter:12580] Loss: 0.889 | Acc: 67.662% \n",
      "[epoch:26, iter:12581] Loss: 0.888 | Acc: 67.691% \n",
      "[epoch:26, iter:12582] Loss: 0.888 | Acc: 67.744% \n",
      "[epoch:26, iter:12583] Loss: 0.890 | Acc: 67.723% \n",
      "[epoch:26, iter:12584] Loss: 0.890 | Acc: 67.679% \n",
      "[epoch:26, iter:12585] Loss: 0.890 | Acc: 67.671% \n",
      "[epoch:26, iter:12586] Loss: 0.889 | Acc: 67.733% \n",
      "[epoch:26, iter:12587] Loss: 0.888 | Acc: 67.770% \n",
      "[epoch:26, iter:12588] Loss: 0.886 | Acc: 67.841% \n",
      "[epoch:26, iter:12589] Loss: 0.886 | Acc: 67.888% \n",
      "[epoch:26, iter:12590] Loss: 0.888 | Acc: 67.789% \n",
      "[epoch:26, iter:12591] Loss: 0.888 | Acc: 67.692% \n",
      "[epoch:26, iter:12592] Loss: 0.888 | Acc: 67.707% \n",
      "[epoch:26, iter:12593] Loss: 0.888 | Acc: 67.710% \n",
      "[epoch:26, iter:12594] Loss: 0.889 | Acc: 67.713% \n",
      "[epoch:26, iter:12595] Loss: 0.890 | Acc: 67.632% \n",
      "[epoch:26, iter:12596] Loss: 0.890 | Acc: 67.656% \n",
      "[epoch:26, iter:12597] Loss: 0.891 | Acc: 67.660% \n",
      "[epoch:26, iter:12598] Loss: 0.892 | Acc: 67.684% \n",
      "[epoch:26, iter:12599] Loss: 0.891 | Acc: 67.687% \n",
      "[epoch:26, iter:12600] Loss: 0.890 | Acc: 67.750% \n",
      "[epoch:26, iter:12601] Loss: 0.891 | Acc: 67.723% \n",
      "[epoch:26, iter:12602] Loss: 0.891 | Acc: 67.735% \n",
      "[epoch:26, iter:12603] Loss: 0.895 | Acc: 67.553% \n",
      "[epoch:26, iter:12604] Loss: 0.894 | Acc: 67.596% \n",
      "[epoch:26, iter:12605] Loss: 0.892 | Acc: 67.648% \n",
      "[epoch:26, iter:12606] Loss: 0.892 | Acc: 67.670% \n",
      "[epoch:26, iter:12607] Loss: 0.891 | Acc: 67.692% \n",
      "[epoch:26, iter:12608] Loss: 0.893 | Acc: 67.620% \n",
      "[epoch:26, iter:12609] Loss: 0.894 | Acc: 67.596% \n",
      "[epoch:26, iter:12610] Loss: 0.893 | Acc: 67.627% \n",
      "[epoch:26, iter:12611] Loss: 0.894 | Acc: 67.604% \n",
      "[epoch:26, iter:12612] Loss: 0.894 | Acc: 67.589% \n",
      "[epoch:26, iter:12613] Loss: 0.895 | Acc: 67.566% \n",
      "[epoch:26, iter:12614] Loss: 0.895 | Acc: 67.500% \n",
      "[epoch:26, iter:12615] Loss: 0.896 | Acc: 67.504% \n",
      "[epoch:26, iter:12616] Loss: 0.895 | Acc: 67.552% \n",
      "[epoch:26, iter:12617] Loss: 0.895 | Acc: 67.556% \n",
      "[epoch:26, iter:12618] Loss: 0.896 | Acc: 67.525% \n",
      "[epoch:26, iter:12619] Loss: 0.896 | Acc: 67.496% \n",
      "[epoch:26, iter:12620] Loss: 0.895 | Acc: 67.558% \n",
      "[epoch:26, iter:12621] Loss: 0.894 | Acc: 67.570% \n",
      "[epoch:26, iter:12622] Loss: 0.893 | Acc: 67.574% \n",
      "[epoch:26, iter:12623] Loss: 0.892 | Acc: 67.642% \n",
      "[epoch:26, iter:12624] Loss: 0.892 | Acc: 67.661% \n",
      "[epoch:26, iter:12625] Loss: 0.891 | Acc: 67.648% \n",
      "[epoch:26, iter:12626] Loss: 0.892 | Acc: 67.603% \n",
      "[epoch:26, iter:12627] Loss: 0.891 | Acc: 67.591% \n",
      "[epoch:26, iter:12628] Loss: 0.891 | Acc: 67.594% \n",
      "[epoch:26, iter:12629] Loss: 0.892 | Acc: 67.535% \n",
      "[epoch:26, iter:12630] Loss: 0.891 | Acc: 67.554% \n",
      "[epoch:26, iter:12631] Loss: 0.891 | Acc: 67.542% \n",
      "[epoch:26, iter:12632] Loss: 0.893 | Acc: 67.485% \n",
      "[epoch:26, iter:12633] Loss: 0.892 | Acc: 67.511% \n",
      "[epoch:26, iter:12634] Loss: 0.893 | Acc: 67.463% \n",
      "[epoch:26, iter:12635] Loss: 0.893 | Acc: 67.489% \n",
      "[epoch:26, iter:12636] Loss: 0.894 | Acc: 67.485% \n",
      "[epoch:26, iter:12637] Loss: 0.894 | Acc: 67.489% \n",
      "[epoch:26, iter:12638] Loss: 0.894 | Acc: 67.486% \n",
      "[epoch:26, iter:12639] Loss: 0.893 | Acc: 67.482% \n",
      "[epoch:26, iter:12640] Loss: 0.894 | Acc: 67.471% \n",
      "[epoch:26, iter:12641] Loss: 0.894 | Acc: 67.489% \n",
      "[epoch:26, iter:12642] Loss: 0.894 | Acc: 67.458% \n",
      "[epoch:26, iter:12643] Loss: 0.893 | Acc: 67.517% \n",
      "[epoch:26, iter:12644] Loss: 0.895 | Acc: 67.458% \n",
      "[epoch:26, iter:12645] Loss: 0.896 | Acc: 67.441% \n",
      "[epoch:26, iter:12646] Loss: 0.898 | Acc: 67.370% \n",
      "[epoch:26, iter:12647] Loss: 0.898 | Acc: 67.381% \n",
      "[epoch:26, iter:12648] Loss: 0.899 | Acc: 67.331% \n",
      "[epoch:26, iter:12649] Loss: 0.899 | Acc: 67.329% \n",
      "[epoch:26, iter:12650] Loss: 0.899 | Acc: 67.327% \n",
      "[epoch:26, iter:12651] Loss: 0.898 | Acc: 67.371% \n",
      "[epoch:26, iter:12652] Loss: 0.899 | Acc: 67.342% \n",
      "[epoch:26, iter:12653] Loss: 0.899 | Acc: 67.340% \n",
      "[epoch:26, iter:12654] Loss: 0.900 | Acc: 67.299% \n",
      "[epoch:26, iter:12655] Loss: 0.899 | Acc: 67.323% \n",
      "[epoch:26, iter:12656] Loss: 0.900 | Acc: 67.308% \n",
      "[epoch:26, iter:12657] Loss: 0.899 | Acc: 67.325% \n",
      "[epoch:26, iter:12658] Loss: 0.898 | Acc: 67.380% \n",
      "[epoch:26, iter:12659] Loss: 0.899 | Acc: 67.340% \n",
      "[epoch:26, iter:12660] Loss: 0.899 | Acc: 67.325% \n",
      "[epoch:26, iter:12661] Loss: 0.899 | Acc: 67.323% \n",
      "[epoch:26, iter:12662] Loss: 0.899 | Acc: 67.321% \n",
      "[epoch:26, iter:12663] Loss: 0.898 | Acc: 67.368% \n",
      "[epoch:26, iter:12664] Loss: 0.897 | Acc: 67.378% \n",
      "[epoch:26, iter:12665] Loss: 0.898 | Acc: 67.376% \n",
      "[epoch:26, iter:12666] Loss: 0.899 | Acc: 67.307% \n",
      "[epoch:26, iter:12667] Loss: 0.900 | Acc: 67.299% \n",
      "[epoch:26, iter:12668] Loss: 0.900 | Acc: 67.274% \n",
      "[epoch:26, iter:12669] Loss: 0.900 | Acc: 67.314% \n",
      "[epoch:26, iter:12670] Loss: 0.900 | Acc: 67.353% \n",
      "[epoch:26, iter:12671] Loss: 0.900 | Acc: 67.322% \n",
      "[epoch:26, iter:12672] Loss: 0.900 | Acc: 67.326% \n",
      "[epoch:26, iter:12673] Loss: 0.900 | Acc: 67.358% \n",
      "[epoch:26, iter:12674] Loss: 0.900 | Acc: 67.351% \n",
      "[epoch:26, iter:12675] Loss: 0.899 | Acc: 67.354% \n",
      "[epoch:26, iter:12676] Loss: 0.901 | Acc: 67.335% \n",
      "[epoch:26, iter:12677] Loss: 0.901 | Acc: 67.345% \n",
      "[epoch:26, iter:12678] Loss: 0.901 | Acc: 67.320% \n",
      "[epoch:26, iter:12679] Loss: 0.900 | Acc: 67.296% \n",
      "[epoch:26, iter:12680] Loss: 0.900 | Acc: 67.294% \n",
      "[epoch:26, iter:12681] Loss: 0.901 | Acc: 67.282% \n",
      "[epoch:26, iter:12682] Loss: 0.901 | Acc: 67.275% \n",
      "[epoch:26, iter:12683] Loss: 0.900 | Acc: 67.306% \n",
      "[epoch:26, iter:12684] Loss: 0.899 | Acc: 67.315% \n",
      "[epoch:26, iter:12685] Loss: 0.900 | Acc: 67.303% \n",
      "[epoch:26, iter:12686] Loss: 0.899 | Acc: 67.328% \n",
      "[epoch:26, iter:12687] Loss: 0.899 | Acc: 67.332% \n",
      "[epoch:26, iter:12688] Loss: 0.899 | Acc: 67.330% \n",
      "[epoch:26, iter:12689] Loss: 0.899 | Acc: 67.392% \n",
      "[epoch:26, iter:12690] Loss: 0.899 | Acc: 67.374% \n",
      "[epoch:26, iter:12691] Loss: 0.899 | Acc: 67.377% \n",
      "[epoch:26, iter:12692] Loss: 0.899 | Acc: 67.391% \n",
      "[epoch:26, iter:12693] Loss: 0.899 | Acc: 67.409% \n",
      "[epoch:26, iter:12694] Loss: 0.899 | Acc: 67.412% \n",
      "[epoch:26, iter:12695] Loss: 0.899 | Acc: 67.426% \n",
      "[epoch:26, iter:12696] Loss: 0.898 | Acc: 67.459% \n",
      "[epoch:26, iter:12697] Loss: 0.897 | Acc: 67.497% \n",
      "[epoch:26, iter:12698] Loss: 0.898 | Acc: 67.460% \n",
      "[epoch:26, iter:12699] Loss: 0.897 | Acc: 67.487% \n",
      "[epoch:26, iter:12700] Loss: 0.897 | Acc: 67.485% \n",
      "[epoch:26, iter:12701] Loss: 0.897 | Acc: 67.488% \n",
      "[epoch:26, iter:12702] Loss: 0.896 | Acc: 67.505% \n",
      "[epoch:26, iter:12703] Loss: 0.896 | Acc: 67.537% \n",
      "[epoch:26, iter:12704] Loss: 0.896 | Acc: 67.544% \n",
      "[epoch:26, iter:12705] Loss: 0.895 | Acc: 67.551% \n",
      "[epoch:26, iter:12706] Loss: 0.895 | Acc: 67.568% \n",
      "[epoch:26, iter:12707] Loss: 0.894 | Acc: 67.585% \n",
      "[epoch:26, iter:12708] Loss: 0.894 | Acc: 67.591% \n",
      "[epoch:26, iter:12709] Loss: 0.894 | Acc: 67.565% \n",
      "[epoch:26, iter:12710] Loss: 0.894 | Acc: 67.595% \n",
      "[epoch:26, iter:12711] Loss: 0.894 | Acc: 67.569% \n",
      "[epoch:26, iter:12712] Loss: 0.895 | Acc: 67.542% \n",
      "[epoch:26, iter:12713] Loss: 0.895 | Acc: 67.559% \n",
      "[epoch:26, iter:12714] Loss: 0.895 | Acc: 67.570% \n",
      "[epoch:26, iter:12715] Loss: 0.894 | Acc: 67.563% \n",
      "[epoch:26, iter:12716] Loss: 0.894 | Acc: 67.593% \n",
      "[epoch:26, iter:12717] Loss: 0.894 | Acc: 67.571% \n",
      "[epoch:26, iter:12718] Loss: 0.894 | Acc: 67.573% \n",
      "[epoch:26, iter:12719] Loss: 0.894 | Acc: 67.575% \n",
      "[epoch:26, iter:12720] Loss: 0.893 | Acc: 67.582% \n",
      "[epoch:26, iter:12721] Loss: 0.893 | Acc: 67.566% \n",
      "[epoch:26, iter:12722] Loss: 0.893 | Acc: 67.568% \n",
      "[epoch:26, iter:12723] Loss: 0.894 | Acc: 67.547% \n",
      "[epoch:26, iter:12724] Loss: 0.895 | Acc: 67.509% \n",
      "[epoch:26, iter:12725] Loss: 0.895 | Acc: 67.480% \n",
      "[epoch:26, iter:12726] Loss: 0.896 | Acc: 67.460% \n",
      "[epoch:26, iter:12727] Loss: 0.895 | Acc: 67.485% \n",
      "[epoch:26, iter:12728] Loss: 0.895 | Acc: 67.522% \n",
      "[epoch:26, iter:12729] Loss: 0.894 | Acc: 67.541% \n",
      "[epoch:26, iter:12730] Loss: 0.895 | Acc: 67.530% \n",
      "[epoch:26, iter:12731] Loss: 0.894 | Acc: 67.550% \n",
      "[epoch:26, iter:12732] Loss: 0.895 | Acc: 67.509% \n",
      "[epoch:26, iter:12733] Loss: 0.894 | Acc: 67.519% \n",
      "[epoch:26, iter:12734] Loss: 0.894 | Acc: 67.517% \n",
      "[epoch:26, iter:12735] Loss: 0.894 | Acc: 67.494% \n",
      "[epoch:26, iter:12736] Loss: 0.894 | Acc: 67.470% \n",
      "[epoch:26, iter:12737] Loss: 0.894 | Acc: 67.460% \n",
      "[epoch:26, iter:12738] Loss: 0.894 | Acc: 67.483% \n",
      "[epoch:26, iter:12739] Loss: 0.895 | Acc: 67.473% \n",
      "[epoch:26, iter:12740] Loss: 0.894 | Acc: 67.496% \n",
      "[epoch:26, iter:12741] Loss: 0.894 | Acc: 67.494% \n",
      "[epoch:26, iter:12742] Loss: 0.894 | Acc: 67.512% \n",
      "[epoch:26, iter:12743] Loss: 0.894 | Acc: 67.535% \n",
      "[epoch:26, iter:12744] Loss: 0.894 | Acc: 67.529% \n",
      "[epoch:26, iter:12745] Loss: 0.894 | Acc: 67.522% \n",
      "[epoch:26, iter:12746] Loss: 0.894 | Acc: 67.524% \n",
      "[epoch:26, iter:12747] Loss: 0.894 | Acc: 67.534% \n",
      "[epoch:26, iter:12748] Loss: 0.894 | Acc: 67.532% \n",
      "[epoch:26, iter:12749] Loss: 0.894 | Acc: 67.518% \n",
      "[epoch:26, iter:12750] Loss: 0.894 | Acc: 67.532% \n",
      "[epoch:26, iter:12751] Loss: 0.894 | Acc: 67.530% \n",
      "[epoch:26, iter:12752] Loss: 0.893 | Acc: 67.563% \n",
      "[epoch:26, iter:12753] Loss: 0.894 | Acc: 67.542% \n",
      "[epoch:26, iter:12754] Loss: 0.894 | Acc: 67.539% \n",
      "[epoch:26, iter:12755] Loss: 0.894 | Acc: 67.529% \n",
      "[epoch:26, iter:12756] Loss: 0.894 | Acc: 67.535% \n",
      "[epoch:26, iter:12757] Loss: 0.894 | Acc: 67.560% \n",
      "[epoch:26, iter:12758] Loss: 0.893 | Acc: 67.562% \n",
      "[epoch:26, iter:12759] Loss: 0.893 | Acc: 67.595% \n",
      "[epoch:26, iter:12760] Loss: 0.893 | Acc: 67.588% \n",
      "[epoch:26, iter:12761] Loss: 0.894 | Acc: 67.556% \n",
      "[epoch:26, iter:12762] Loss: 0.893 | Acc: 67.553% \n",
      "[epoch:26, iter:12763] Loss: 0.893 | Acc: 67.574% \n",
      "[epoch:26, iter:12764] Loss: 0.893 | Acc: 67.572% \n",
      "[epoch:26, iter:12765] Loss: 0.893 | Acc: 67.585% \n",
      "[epoch:26, iter:12766] Loss: 0.893 | Acc: 67.568% \n",
      "[epoch:26, iter:12767] Loss: 0.893 | Acc: 67.566% \n",
      "[epoch:26, iter:12768] Loss: 0.893 | Acc: 67.578% \n",
      "[epoch:26, iter:12769] Loss: 0.893 | Acc: 67.587% \n",
      "[epoch:26, iter:12770] Loss: 0.893 | Acc: 67.600% \n",
      "[epoch:26, iter:12771] Loss: 0.892 | Acc: 67.620% \n",
      "[epoch:26, iter:12772] Loss: 0.892 | Acc: 67.614% \n",
      "[epoch:26, iter:12773] Loss: 0.892 | Acc: 67.652% \n",
      "[epoch:26, iter:12774] Loss: 0.892 | Acc: 67.650% \n",
      "[epoch:26, iter:12775] Loss: 0.892 | Acc: 67.651% \n",
      "[epoch:26, iter:12776] Loss: 0.892 | Acc: 67.645% \n",
      "[epoch:26, iter:12777] Loss: 0.892 | Acc: 67.664% \n",
      "[epoch:26, iter:12778] Loss: 0.892 | Acc: 67.655% \n",
      "[epoch:26, iter:12779] Loss: 0.892 | Acc: 67.667% \n",
      "[epoch:26, iter:12780] Loss: 0.892 | Acc: 67.650% \n",
      "[epoch:26, iter:12781] Loss: 0.893 | Acc: 67.630% \n",
      "[epoch:26, iter:12782] Loss: 0.893 | Acc: 67.610% \n",
      "[epoch:26, iter:12783] Loss: 0.893 | Acc: 67.604% \n",
      "[epoch:26, iter:12784] Loss: 0.892 | Acc: 67.623% \n",
      "[epoch:26, iter:12785] Loss: 0.892 | Acc: 67.618% \n",
      "[epoch:26, iter:12786] Loss: 0.892 | Acc: 67.636% \n",
      "[epoch:26, iter:12787] Loss: 0.892 | Acc: 67.631% \n",
      "[epoch:26, iter:12788] Loss: 0.891 | Acc: 67.663% \n",
      "[epoch:26, iter:12789] Loss: 0.891 | Acc: 67.689% \n",
      "[epoch:26, iter:12790] Loss: 0.891 | Acc: 67.707% \n",
      "[epoch:26, iter:12791] Loss: 0.891 | Acc: 67.711% \n",
      "[epoch:26, iter:12792] Loss: 0.891 | Acc: 67.716% \n",
      "[epoch:26, iter:12793] Loss: 0.891 | Acc: 67.734% \n",
      "[epoch:26, iter:12794] Loss: 0.891 | Acc: 67.731% \n",
      "[epoch:26, iter:12795] Loss: 0.891 | Acc: 67.742% \n",
      "[epoch:26, iter:12796] Loss: 0.891 | Acc: 67.716% \n",
      "[epoch:26, iter:12797] Loss: 0.891 | Acc: 67.710% \n",
      "[epoch:26, iter:12798] Loss: 0.891 | Acc: 67.708% \n",
      "[epoch:26, iter:12799] Loss: 0.891 | Acc: 67.672% \n",
      "[epoch:26, iter:12800] Loss: 0.891 | Acc: 67.690% \n",
      "[epoch:26, iter:12801] Loss: 0.891 | Acc: 67.701% \n",
      "[epoch:26, iter:12802] Loss: 0.890 | Acc: 67.715% \n",
      "[epoch:26, iter:12803] Loss: 0.890 | Acc: 67.719% \n",
      "[epoch:26, iter:12804] Loss: 0.891 | Acc: 67.678% \n",
      "[epoch:26, iter:12805] Loss: 0.891 | Acc: 67.689% \n",
      "[epoch:26, iter:12806] Loss: 0.891 | Acc: 67.693% \n",
      "[epoch:26, iter:12807] Loss: 0.891 | Acc: 67.694% \n",
      "[epoch:26, iter:12808] Loss: 0.891 | Acc: 67.711% \n",
      "[epoch:26, iter:12809] Loss: 0.891 | Acc: 67.686% \n",
      "[epoch:26, iter:12810] Loss: 0.891 | Acc: 67.674% \n",
      "[epoch:26, iter:12811] Loss: 0.891 | Acc: 67.678% \n",
      "[epoch:26, iter:12812] Loss: 0.892 | Acc: 67.670% \n",
      "[epoch:26, iter:12813] Loss: 0.892 | Acc: 67.677% \n",
      "[epoch:26, iter:12814] Loss: 0.891 | Acc: 67.691% \n",
      "[epoch:26, iter:12815] Loss: 0.891 | Acc: 67.692% \n",
      "[epoch:26, iter:12816] Loss: 0.891 | Acc: 67.696% \n",
      "[epoch:26, iter:12817] Loss: 0.890 | Acc: 67.729% \n",
      "[epoch:26, iter:12818] Loss: 0.890 | Acc: 67.730% \n",
      "[epoch:26, iter:12819] Loss: 0.890 | Acc: 67.746% \n",
      "[epoch:26, iter:12820] Loss: 0.890 | Acc: 67.719% \n",
      "[epoch:26, iter:12821] Loss: 0.890 | Acc: 67.745% \n",
      "[epoch:26, iter:12822] Loss: 0.890 | Acc: 67.748% \n",
      "[epoch:26, iter:12823] Loss: 0.890 | Acc: 67.749% \n",
      "[epoch:26, iter:12824] Loss: 0.890 | Acc: 67.710% \n",
      "[epoch:26, iter:12825] Loss: 0.890 | Acc: 67.708% \n",
      "[epoch:26, iter:12826] Loss: 0.890 | Acc: 67.706% \n",
      "[epoch:26, iter:12827] Loss: 0.890 | Acc: 67.706% \n",
      "[epoch:26, iter:12828] Loss: 0.890 | Acc: 67.720% \n",
      "[epoch:26, iter:12829] Loss: 0.889 | Acc: 67.717% \n",
      "[epoch:26, iter:12830] Loss: 0.890 | Acc: 67.697% \n",
      "[epoch:26, iter:12831] Loss: 0.889 | Acc: 67.704% \n",
      "[epoch:26, iter:12832] Loss: 0.889 | Acc: 67.720% \n",
      "[epoch:26, iter:12833] Loss: 0.889 | Acc: 67.712% \n",
      "[epoch:26, iter:12834] Loss: 0.889 | Acc: 67.710% \n",
      "[epoch:26, iter:12835] Loss: 0.889 | Acc: 67.710% \n",
      "[epoch:26, iter:12836] Loss: 0.889 | Acc: 67.726% \n",
      "[epoch:26, iter:12837] Loss: 0.889 | Acc: 67.706% \n",
      "[epoch:26, iter:12838] Loss: 0.889 | Acc: 67.710% \n",
      "[epoch:26, iter:12839] Loss: 0.889 | Acc: 67.714% \n",
      "[epoch:26, iter:12840] Loss: 0.889 | Acc: 67.700% \n",
      "[epoch:26, iter:12841] Loss: 0.890 | Acc: 67.680% \n",
      "[epoch:26, iter:12842] Loss: 0.890 | Acc: 67.667% \n",
      "[epoch:26, iter:12843] Loss: 0.890 | Acc: 67.671% \n",
      "[epoch:26, iter:12844] Loss: 0.890 | Acc: 67.654% \n",
      "[epoch:26, iter:12845] Loss: 0.891 | Acc: 67.649% \n",
      "[epoch:26, iter:12846] Loss: 0.891 | Acc: 67.642% \n",
      "[epoch:26, iter:12847] Loss: 0.890 | Acc: 67.648% \n",
      "[epoch:26, iter:12848] Loss: 0.891 | Acc: 67.629% \n",
      "[epoch:26, iter:12849] Loss: 0.891 | Acc: 67.645% \n",
      "[epoch:26, iter:12850] Loss: 0.892 | Acc: 67.634% \n",
      "[epoch:26, iter:12851] Loss: 0.891 | Acc: 67.655% \n",
      "[epoch:26, iter:12852] Loss: 0.891 | Acc: 67.670% \n",
      "[epoch:26, iter:12853] Loss: 0.891 | Acc: 67.680% \n",
      "[epoch:26, iter:12854] Loss: 0.891 | Acc: 67.675% \n",
      "[epoch:26, iter:12855] Loss: 0.891 | Acc: 67.665% \n",
      "[epoch:26, iter:12856] Loss: 0.891 | Acc: 67.669% \n",
      "[epoch:26, iter:12857] Loss: 0.891 | Acc: 67.667% \n",
      "[epoch:26, iter:12858] Loss: 0.891 | Acc: 67.665% \n",
      "[epoch:26, iter:12859] Loss: 0.891 | Acc: 67.666% \n",
      "[epoch:26, iter:12860] Loss: 0.891 | Acc: 67.658% \n",
      "[epoch:26, iter:12861] Loss: 0.891 | Acc: 67.654% \n",
      "[epoch:26, iter:12862] Loss: 0.891 | Acc: 67.677% \n",
      "[epoch:26, iter:12863] Loss: 0.890 | Acc: 67.694% \n",
      "[epoch:26, iter:12864] Loss: 0.890 | Acc: 67.701% \n",
      "[epoch:26, iter:12865] Loss: 0.890 | Acc: 67.718% \n",
      "[epoch:26, iter:12866] Loss: 0.890 | Acc: 67.730% \n",
      "[epoch:26, iter:12867] Loss: 0.889 | Acc: 67.741% \n",
      "[epoch:26, iter:12868] Loss: 0.889 | Acc: 67.731% \n",
      "[epoch:26, iter:12869] Loss: 0.889 | Acc: 67.756% \n",
      "[epoch:26, iter:12870] Loss: 0.889 | Acc: 67.778% \n",
      "[epoch:26, iter:12871] Loss: 0.889 | Acc: 67.787% \n",
      "[epoch:26, iter:12872] Loss: 0.889 | Acc: 67.793% \n",
      "[epoch:26, iter:12873] Loss: 0.889 | Acc: 67.796% \n",
      "[epoch:26, iter:12874] Loss: 0.889 | Acc: 67.813% \n",
      "[epoch:26, iter:12875] Loss: 0.888 | Acc: 67.811% \n",
      "[epoch:26, iter:12876] Loss: 0.888 | Acc: 67.798% \n",
      "[epoch:26, iter:12877] Loss: 0.889 | Acc: 67.782% \n",
      "[epoch:26, iter:12878] Loss: 0.888 | Acc: 67.783% \n",
      "[epoch:26, iter:12879] Loss: 0.888 | Acc: 67.789% \n",
      "[epoch:26, iter:12880] Loss: 0.888 | Acc: 67.782% \n",
      "[epoch:26, iter:12881] Loss: 0.888 | Acc: 67.780% \n",
      "[epoch:26, iter:12882] Loss: 0.888 | Acc: 67.777% \n",
      "[epoch:26, iter:12883] Loss: 0.889 | Acc: 67.778% \n",
      "[epoch:26, iter:12884] Loss: 0.889 | Acc: 67.766% \n",
      "[epoch:26, iter:12885] Loss: 0.889 | Acc: 67.753% \n",
      "[epoch:26, iter:12886] Loss: 0.889 | Acc: 67.751% \n",
      "[epoch:26, iter:12887] Loss: 0.889 | Acc: 67.744% \n",
      "[epoch:26, iter:12888] Loss: 0.888 | Acc: 67.750% \n",
      "[epoch:26, iter:12889] Loss: 0.889 | Acc: 67.756% \n",
      "[epoch:26, iter:12890] Loss: 0.888 | Acc: 67.751% \n",
      "[epoch:26, iter:12891] Loss: 0.888 | Acc: 67.747% \n",
      "[epoch:26, iter:12892] Loss: 0.888 | Acc: 67.747% \n",
      "[epoch:26, iter:12893] Loss: 0.888 | Acc: 67.743% \n",
      "[epoch:26, iter:12894] Loss: 0.888 | Acc: 67.736% \n",
      "[epoch:26, iter:12895] Loss: 0.888 | Acc: 67.754% \n",
      "[epoch:26, iter:12896] Loss: 0.888 | Acc: 67.758% \n",
      "[epoch:26, iter:12897] Loss: 0.888 | Acc: 67.751% \n",
      "[epoch:26, iter:12898] Loss: 0.888 | Acc: 67.736% \n",
      "[epoch:26, iter:12899] Loss: 0.888 | Acc: 67.737% \n",
      "[epoch:26, iter:12900] Loss: 0.888 | Acc: 67.750% \n",
      "[epoch:26, iter:12901] Loss: 0.888 | Acc: 67.746% \n",
      "[epoch:26, iter:12902] Loss: 0.888 | Acc: 67.759% \n",
      "[epoch:26, iter:12903] Loss: 0.888 | Acc: 67.759% \n",
      "[epoch:26, iter:12904] Loss: 0.888 | Acc: 67.765% \n",
      "[epoch:26, iter:12905] Loss: 0.888 | Acc: 67.768% \n",
      "[epoch:26, iter:12906] Loss: 0.888 | Acc: 67.781% \n",
      "[epoch:26, iter:12907] Loss: 0.888 | Acc: 67.801% \n",
      "[epoch:26, iter:12908] Loss: 0.887 | Acc: 67.819% \n",
      "[epoch:26, iter:12909] Loss: 0.887 | Acc: 67.846% \n",
      "[epoch:26, iter:12910] Loss: 0.887 | Acc: 67.859% \n",
      "[epoch:26, iter:12911] Loss: 0.887 | Acc: 67.837% \n",
      "[epoch:26, iter:12912] Loss: 0.887 | Acc: 67.850% \n",
      "[epoch:26, iter:12913] Loss: 0.887 | Acc: 67.845% \n",
      "[epoch:26, iter:12914] Loss: 0.887 | Acc: 67.833% \n",
      "[epoch:26, iter:12915] Loss: 0.887 | Acc: 67.843% \n",
      "[epoch:26, iter:12916] Loss: 0.887 | Acc: 67.858% \n",
      "[epoch:26, iter:12917] Loss: 0.887 | Acc: 67.859% \n",
      "[epoch:26, iter:12918] Loss: 0.887 | Acc: 67.849% \n",
      "[epoch:26, iter:12919] Loss: 0.887 | Acc: 67.857% \n",
      "[epoch:26, iter:12920] Loss: 0.887 | Acc: 67.876% \n",
      "[epoch:26, iter:12921] Loss: 0.887 | Acc: 67.867% \n",
      "[epoch:26, iter:12922] Loss: 0.887 | Acc: 67.884% \n",
      "[epoch:26, iter:12923] Loss: 0.887 | Acc: 67.891% \n",
      "[epoch:26, iter:12924] Loss: 0.886 | Acc: 67.892% \n",
      "[epoch:26, iter:12925] Loss: 0.886 | Acc: 67.885% \n",
      "[epoch:26, iter:12926] Loss: 0.887 | Acc: 67.864% \n",
      "[epoch:26, iter:12927] Loss: 0.887 | Acc: 67.855% \n",
      "[epoch:26, iter:12928] Loss: 0.887 | Acc: 67.843% \n",
      "[epoch:26, iter:12929] Loss: 0.887 | Acc: 67.841% \n",
      "[epoch:26, iter:12930] Loss: 0.887 | Acc: 67.840% \n",
      "[epoch:26, iter:12931] Loss: 0.887 | Acc: 67.852% \n",
      "[epoch:26, iter:12932] Loss: 0.887 | Acc: 67.861% \n",
      "[epoch:26, iter:12933] Loss: 0.887 | Acc: 67.868% \n",
      "[epoch:26, iter:12934] Loss: 0.887 | Acc: 67.857% \n",
      "[epoch:26, iter:12935] Loss: 0.887 | Acc: 67.857% \n",
      "[epoch:26, iter:12936] Loss: 0.888 | Acc: 67.842% \n",
      "[epoch:26, iter:12937] Loss: 0.888 | Acc: 67.840% \n",
      "[epoch:26, iter:12938] Loss: 0.888 | Acc: 67.847% \n",
      "[epoch:26, iter:12939] Loss: 0.888 | Acc: 67.841% \n",
      "[epoch:26, iter:12940] Loss: 0.888 | Acc: 67.839% \n",
      "[epoch:26, iter:12941] Loss: 0.888 | Acc: 67.839% \n",
      "[epoch:26, iter:12942] Loss: 0.888 | Acc: 67.853% \n",
      "[epoch:26, iter:12943] Loss: 0.887 | Acc: 67.871% \n",
      "[epoch:26, iter:12944] Loss: 0.887 | Acc: 67.878% \n",
      "[epoch:26, iter:12945] Loss: 0.887 | Acc: 67.883% \n",
      "[epoch:26, iter:12946] Loss: 0.887 | Acc: 67.886% \n",
      "[epoch:26, iter:12947] Loss: 0.887 | Acc: 67.888% \n",
      "[epoch:26, iter:12948] Loss: 0.887 | Acc: 67.868% \n",
      "[epoch:26, iter:12949] Loss: 0.887 | Acc: 67.866% \n",
      "[epoch:26, iter:12950] Loss: 0.888 | Acc: 67.853% \n",
      "[epoch:26, iter:12951] Loss: 0.888 | Acc: 67.880% \n",
      "[epoch:26, iter:12952] Loss: 0.887 | Acc: 67.887% \n",
      "[epoch:26, iter:12953] Loss: 0.888 | Acc: 67.885% \n",
      "[epoch:26, iter:12954] Loss: 0.888 | Acc: 67.890% \n",
      "[epoch:26, iter:12955] Loss: 0.888 | Acc: 67.886% \n",
      "[epoch:26, iter:12956] Loss: 0.888 | Acc: 67.877% \n",
      "[epoch:26, iter:12957] Loss: 0.889 | Acc: 67.860% \n",
      "[epoch:26, iter:12958] Loss: 0.889 | Acc: 67.860% \n",
      "[epoch:26, iter:12959] Loss: 0.889 | Acc: 67.847% \n",
      "[epoch:26, iter:12960] Loss: 0.889 | Acc: 67.861% \n",
      "[epoch:26, iter:12961] Loss: 0.889 | Acc: 67.855% \n",
      "[epoch:26, iter:12962] Loss: 0.888 | Acc: 67.861% \n",
      "[epoch:26, iter:12963] Loss: 0.889 | Acc: 67.860% \n",
      "[epoch:26, iter:12964] Loss: 0.888 | Acc: 67.873% \n",
      "[epoch:26, iter:12965] Loss: 0.889 | Acc: 67.860% \n",
      "[epoch:26, iter:12966] Loss: 0.889 | Acc: 67.841% \n",
      "[epoch:26, iter:12967] Loss: 0.889 | Acc: 67.854% \n",
      "[epoch:26, iter:12968] Loss: 0.889 | Acc: 67.861% \n",
      "[epoch:26, iter:12969] Loss: 0.889 | Acc: 67.859% \n",
      "[epoch:26, iter:12970] Loss: 0.889 | Acc: 67.860% \n",
      "[epoch:26, iter:12971] Loss: 0.889 | Acc: 67.858% \n",
      "[epoch:26, iter:12972] Loss: 0.889 | Acc: 67.864% \n",
      "[epoch:26, iter:12973] Loss: 0.889 | Acc: 67.858% \n",
      "[epoch:26, iter:12974] Loss: 0.889 | Acc: 67.861% \n",
      "[epoch:26, iter:12975] Loss: 0.889 | Acc: 67.861% \n",
      "[epoch:26, iter:12976] Loss: 0.889 | Acc: 67.861% \n",
      "[epoch:26, iter:12977] Loss: 0.889 | Acc: 67.872% \n",
      "[epoch:26, iter:12978] Loss: 0.889 | Acc: 67.847% \n",
      "[epoch:26, iter:12979] Loss: 0.889 | Acc: 67.856% \n",
      "[epoch:26, iter:12980] Loss: 0.889 | Acc: 67.850% \n",
      "[epoch:26, iter:12981] Loss: 0.890 | Acc: 67.861% \n",
      "[epoch:26, iter:12982] Loss: 0.890 | Acc: 67.859% \n",
      "[epoch:26, iter:12983] Loss: 0.890 | Acc: 67.870% \n",
      "[epoch:26, iter:12984] Loss: 0.889 | Acc: 67.874% \n",
      "[epoch:26, iter:12985] Loss: 0.889 | Acc: 67.870% \n",
      "[epoch:26, iter:12986] Loss: 0.889 | Acc: 67.877% \n",
      "[epoch:26, iter:12987] Loss: 0.889 | Acc: 67.867% \n",
      "[epoch:26, iter:12988] Loss: 0.889 | Acc: 67.861% \n",
      "[epoch:26, iter:12989] Loss: 0.889 | Acc: 67.851% \n",
      "[epoch:26, iter:12990] Loss: 0.889 | Acc: 67.859% \n",
      "[epoch:26, iter:12991] Loss: 0.889 | Acc: 67.864% \n",
      "[epoch:26, iter:12992] Loss: 0.889 | Acc: 67.862% \n",
      "[epoch:26, iter:12993] Loss: 0.889 | Acc: 67.874% \n",
      "[epoch:26, iter:12994] Loss: 0.889 | Acc: 67.897% \n",
      "[epoch:26, iter:12995] Loss: 0.888 | Acc: 67.895% \n",
      "[epoch:26, iter:12996] Loss: 0.889 | Acc: 67.883% \n",
      "[epoch:26, iter:12997] Loss: 0.889 | Acc: 67.895% \n",
      "[epoch:26, iter:12998] Loss: 0.889 | Acc: 67.912% \n",
      "[epoch:26, iter:12999] Loss: 0.888 | Acc: 67.908% \n",
      "[epoch:26, iter:13000] Loss: 0.889 | Acc: 67.892% \n",
      "Waiting Test...\n",
      "Test's ac is: 64.600%\n",
      "\n",
      "Epoch: 27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m     19\u001b[0m         param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#prepare dataset\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)\n\u001b[0;32m     24\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\dl\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "alpha=0.2\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] =  0.02\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.6, momentum=0.9)\n",
    "lr_decay_epochs=[3, 10, 20, 40]\n",
    "for epoch in range(20, 40):\n",
    "    print('\\nEpoch: %d' % (epoch + 1))\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    if epoch<15:\n",
    "        epsilon=1\n",
    "    else:\n",
    "        epsilon=.95**((epoch-15)//2)\n",
    "    if epoch in lr_decay_epochs:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *=  0.5\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        #prepare dataset\n",
    "        length = len(trainloader)\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward & backward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        for param_group in optimizer.param_groups:\n",
    "                for idx, p in enumerate(param_group['params']):\n",
    "                    constr = epsilon-(p.data**2-1)**2\n",
    "                    Kx = -4 * (p.data**2 - 1) * p.data\n",
    "                    direct_grad = torch.logical_or(torch.logical_or(constr >= 0, Kx == 0), torch.logical_and(constr < 0, (-Kx * p.grad.data) >= -alpha * constr))\n",
    "                    p.grad.data[direct_grad] = p.grad.data[direct_grad]\n",
    "                    p.grad.data[~direct_grad] = torch.clip(alpha * constr / Kx, -20, 20)[~direct_grad]\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            for param_group in optimizer.param_groups:\n",
    "                for idx, p in enumerate(param_group['params']):\n",
    "                    p.data.clamp_(-1,1)\n",
    "        #print ac & loss in each batch\n",
    "        sum_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "        print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% ' \n",
    "              % (epoch + 1, (i + 1 + (epoch) * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "        \n",
    "    #get the ac with testdataset in each epoch\n",
    "    print('Waiting Test...')\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            net.eval()\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print('Test\\'s ac is: %.3f%%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'resnet18_qt.pt'\n",
    "\n",
    "torch.save({'model_state_dict': net.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.7347, -0.8634, -0.8105],\n",
      "          [-0.8114, -0.9026, -0.8683],\n",
      "          [-0.9490, -0.9743, -0.9733]],\n",
      "\n",
      "         [[-0.9667, -0.9991, -0.8998],\n",
      "          [-0.9749, -1.0000, -0.9674],\n",
      "          [-0.9980, -0.9941, -0.9896]],\n",
      "\n",
      "         [[-0.5873,  0.5374, -0.5312],\n",
      "          [ 0.8407,  0.5274,  0.5303],\n",
      "          [ 0.5262,  0.5395,  0.5333]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9253,  0.9516,  0.9206],\n",
      "          [ 0.9566,  0.9620,  0.9505],\n",
      "          [ 0.7039,  0.7386,  0.6741]],\n",
      "\n",
      "         [[ 0.9385,  0.9858,  0.9252],\n",
      "          [ 0.6384,  0.7329,  0.6248],\n",
      "          [-0.5476, -0.5492, -0.5562]],\n",
      "\n",
      "         [[ 0.6961,  0.7949,  0.6963],\n",
      "          [ 0.6407,  0.6674,  0.5759],\n",
      "          [-0.5244, -0.5195, -0.5196]]],\n",
      "\n",
      "\n",
      "        [[[-0.6860, -0.6819, -0.6779],\n",
      "          [-0.6490, -0.6378, -0.6231],\n",
      "          [-0.5826, -0.5660, -0.5517]],\n",
      "\n",
      "         [[-0.6433, -0.6295, -0.6361],\n",
      "          [-0.5734, -0.5634, -0.5636],\n",
      "          [-0.5285, -0.5275, -0.5303]],\n",
      "\n",
      "         [[ 0.8896,  0.8876,  0.8913],\n",
      "          [ 0.9460,  0.9411,  0.9422],\n",
      "          [ 1.0000,  0.9999,  0.9995]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.6311, -0.9396, -0.6650],\n",
      "          [-0.6701, -0.9316, -0.8037],\n",
      "          [-0.6729, -0.8634, -0.6911]],\n",
      "\n",
      "         [[ 0.7087,  0.9404,  0.9557],\n",
      "          [ 0.9685,  0.9818,  0.9679],\n",
      "          [ 0.9623,  0.9759,  0.9643]],\n",
      "\n",
      "         [[ 0.5882, -0.9329, -0.7666],\n",
      "          [ 0.7129,  0.8128,  0.8821],\n",
      "          [-0.7493, -0.6623, -0.6437]]],\n",
      "\n",
      "\n",
      "        [[[-0.9746, -0.9820, -0.9818],\n",
      "          [-0.9607, -0.9700, -0.9689],\n",
      "          [-0.5758, -0.5814, -0.5772]],\n",
      "\n",
      "         [[ 0.6255,  0.5758,  0.5578],\n",
      "          [-0.7467, -0.6720, -0.7224],\n",
      "          [-0.5175, -0.5161, -0.5208]],\n",
      "\n",
      "         [[ 0.8720,  0.8435,  0.7790],\n",
      "          [ 0.8993,  0.9404,  0.9117],\n",
      "          [ 1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.7946, -0.9714, -0.5745],\n",
      "          [-0.5398, -0.6902, -0.5475],\n",
      "          [-0.5291, -0.5330, -0.5371]],\n",
      "\n",
      "         [[-0.7738, -0.9447, -0.5842],\n",
      "          [ 0.8654,  0.7496,  0.9486],\n",
      "          [ 0.9244,  0.9725,  0.9609]],\n",
      "\n",
      "         [[ 0.5243,  0.5284,  0.5308],\n",
      "          [ 0.9839,  0.9824,  0.8036],\n",
      "          [ 0.7003,  0.7561,  0.5438]]]], device='cuda:0')\n",
      "tensor([ 0.8863,  0.5374,  0.9685, -0.8827,  0.9093, -0.5312, -0.6691,  0.8787,\n",
      "         0.9084, -0.5723,  0.9167,  0.8461, -0.5907, -0.5166, -0.7202, -0.5057,\n",
      "        -0.9842, -0.9714,  0.9681,  0.6727, -0.9915,  0.5342,  0.9344, -0.6282,\n",
      "        -0.5276, -0.6060, -0.9129,  1.0000,  0.9859, -0.6042, -0.5813, -0.9785,\n",
      "        -0.6525, -0.5211,  0.7894, -0.5126, -0.6027, -0.5146, -0.5615, -0.5835,\n",
      "        -0.5154, -1.0000,  0.5203,  0.5650,  0.7743,  0.5131,  1.0000,  0.5958,\n",
      "         0.7028, -0.9471,  0.6167, -0.7360, -0.6378, -0.9849,  0.9839,  0.9018,\n",
      "         1.0000, -0.6635,  1.0000,  0.6925, -0.6746, -0.6715,  0.9999, -0.8652],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.7717, -0.9944, -0.5714,  0.5339, -0.7916, -0.9978, -0.7437, -0.9264,\n",
      "         0.8886,  0.6280, -0.5329, -0.9082, -0.9684, -1.0000,  0.6879,  0.6389,\n",
      "        -0.7085,  0.8767,  0.9606,  0.6781, -0.8179, -0.9911,  0.7047, -0.9581,\n",
      "        -0.9998, -0.8043,  0.9939, -0.5179, -0.5654, -1.0000, -0.9946, -0.8445,\n",
      "        -0.9444, -0.9996,  0.9432, -1.0000, -0.9873,  0.5601,  0.5726, -0.9954,\n",
      "        -0.9998,  0.6215, -1.0000,  0.5118, -0.7031, -1.0000,  0.8617, -0.9881,\n",
      "        -1.0000, -0.5602, -0.9442, -0.9959,  0.5287,  0.9983,  1.0000,  0.9961,\n",
      "         0.7079, -0.9272,  0.9906, -0.5969, -0.9749,  0.6454,  0.6241, -0.7891],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.5512, -0.5390, -0.5301],\n",
      "          [-0.5744, -0.5588, -0.5485],\n",
      "          [-0.5616, -0.5449, -0.5276]],\n",
      "\n",
      "         [[-0.5308, -0.5299, -0.5324],\n",
      "          [-0.5313, -0.5325, -0.5337],\n",
      "          [-0.5243, -0.5328, -0.5337]],\n",
      "\n",
      "         [[-0.5172, -0.5172, -0.5148],\n",
      "          [-0.5171, -0.5171, -0.5151],\n",
      "          [ 0.5421, -0.5149, -0.5173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5317, -0.5329, -0.5318],\n",
      "          [-0.5408, -0.5355, -0.7443],\n",
      "          [ 0.5159,  0.5634,  0.6482]],\n",
      "\n",
      "         [[-0.5334, -0.5170, -0.5178],\n",
      "          [-0.5172, -0.5186, -0.5185],\n",
      "          [-0.5373, -0.5219, -0.5181]],\n",
      "\n",
      "         [[-0.5165, -0.5181, -0.5173],\n",
      "          [-0.5244, -0.5188, -0.5176],\n",
      "          [-0.5177, -0.5169, -0.5171]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5417,  0.5261,  0.6415],\n",
      "          [-0.5221,  0.5276,  0.5297],\n",
      "          [-0.5238,  0.5272,  0.5308]],\n",
      "\n",
      "         [[ 0.5173,  0.5255,  0.5154],\n",
      "          [ 0.5179,  0.5211,  0.5152],\n",
      "          [ 0.5163,  0.5228,  0.5266]],\n",
      "\n",
      "         [[-0.5158,  0.8828,  0.5316],\n",
      "          [-0.5153,  0.5314,  0.5325],\n",
      "          [-0.5192,  0.5314,  0.5284]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5306, -0.5343, -0.5493],\n",
      "          [-0.5326, -0.5359, -0.5352],\n",
      "          [-0.5226, -0.5244, -0.5271]],\n",
      "\n",
      "         [[ 0.9715,  0.8712,  0.6740],\n",
      "          [ 0.9962,  0.8152,  0.5817],\n",
      "          [ 0.9616,  0.7576,  0.5537]],\n",
      "\n",
      "         [[-0.5159, -0.5163, -0.5147],\n",
      "          [-0.9871, -0.5165, -0.5163],\n",
      "          [ 0.5265,  0.5287,  0.5339]]],\n",
      "\n",
      "\n",
      "        [[[-0.5239,  0.9554,  0.5254],\n",
      "          [-0.5280,  0.9973,  0.5269],\n",
      "          [-0.9487,  0.5280,  0.5267]],\n",
      "\n",
      "         [[-0.5319, -0.5324, -0.5328],\n",
      "          [-0.5256, -0.5297, -0.5236],\n",
      "          [-0.5278, -0.5330, -0.5205]],\n",
      "\n",
      "         [[-0.5264, -0.5263, -0.5223],\n",
      "          [-0.5263, -0.5216, -0.5217],\n",
      "          [-0.5181, -0.5289, -0.5260]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5289, -0.5363, -0.5360],\n",
      "          [-0.5273, -0.5318, -0.5358],\n",
      "          [-0.5236, -0.5327, -0.5315]],\n",
      "\n",
      "         [[-0.5196, -0.5181, -0.5196],\n",
      "          [-0.5196, -0.5180, -0.5176],\n",
      "          [-0.5181, -0.5176, -0.5169]],\n",
      "\n",
      "         [[-0.5149, -0.5158, -0.5159],\n",
      "          [-0.5167, -0.5203, -0.5182],\n",
      "          [-0.5174, -0.5256, -0.5188]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5207,  0.5297,  0.5216],\n",
      "          [ 0.5252,  0.5167,  0.5295],\n",
      "          [ 0.5271,  0.5153,  0.5242]],\n",
      "\n",
      "         [[-0.5243, -0.5304, -0.5215],\n",
      "          [-0.5176, -0.5156, -0.5153],\n",
      "          [-0.5254, -0.5153, -0.5293]],\n",
      "\n",
      "         [[-0.5272, -0.5319, -0.5316],\n",
      "          [-0.5220, -0.5196, -0.5196],\n",
      "          [-0.5284, -0.5203, -0.5327]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5256, -0.5260, -0.5298],\n",
      "          [-0.5287, -0.5224, -0.5272],\n",
      "          [-0.5302, -0.5205, -0.5177]],\n",
      "\n",
      "         [[-0.6906, -0.6665, -0.6017],\n",
      "          [-0.6418, -0.6333, -0.5281],\n",
      "          [-0.6552, -0.5228, -0.5261]],\n",
      "\n",
      "         [[-0.7011, -0.7176, -0.6347],\n",
      "          [-0.8078, -0.7538, -0.6819],\n",
      "          [-0.6288, -0.6319, -0.5980]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5232,  0.5235,  0.5225],\n",
      "          [ 0.5235,  0.5231,  0.5226],\n",
      "          [ 0.5202,  0.5193,  0.5206]],\n",
      "\n",
      "         [[-0.5181, -0.5264, -0.5193],\n",
      "          [-0.5314, -0.5196, -0.5323],\n",
      "          [-0.5214, -0.5281, -0.5314]],\n",
      "\n",
      "         [[ 0.5388,  0.5396,  0.5411],\n",
      "          [ 0.5365,  0.5376,  0.5382],\n",
      "          [ 0.5296,  0.5307,  0.5332]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9200,  0.5196,  0.5251],\n",
      "          [-0.5503, -0.5301,  0.5209],\n",
      "          [-0.5620, -0.5528, -0.5369]],\n",
      "\n",
      "         [[ 0.5394,  0.5400,  0.5375],\n",
      "          [-0.5182, -0.5162, -0.5169],\n",
      "          [ 0.5358, -0.5183, -0.5178]],\n",
      "\n",
      "         [[-0.5279, -0.5271, -0.5177],\n",
      "          [-0.5303, -0.5293, -0.5268],\n",
      "          [-0.5197, -0.5269, -0.5221]]],\n",
      "\n",
      "\n",
      "        [[[-0.5261, -0.5285, -0.5225],\n",
      "          [-0.5167, -0.5152,  0.6174],\n",
      "          [-0.5294, -0.5199, -0.6190]],\n",
      "\n",
      "         [[ 0.8462, -0.6225,  0.5241],\n",
      "          [ 0.5650,  0.5163,  0.5294],\n",
      "          [ 0.5240,  0.5181,  0.5237]],\n",
      "\n",
      "         [[ 0.5239,  0.5572,  0.5158],\n",
      "          [ 0.5763,  0.6601,  0.5308],\n",
      "          [ 0.5555,  0.6315,  0.5212]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5537,  0.6244,  0.5227],\n",
      "          [ 0.5260,  0.5714,  0.5306],\n",
      "          [ 0.5254,  0.6121,  0.5695]],\n",
      "\n",
      "         [[-0.5326, -0.5319, -0.5278],\n",
      "          [-0.5189, -0.5154, -0.5159],\n",
      "          [-0.5334, -0.5238, -0.5322]],\n",
      "\n",
      "         [[ 0.5253,  0.5204,  0.5327],\n",
      "          [ 0.5311,  0.5184,  0.5234],\n",
      "          [ 0.5211,  0.5328,  0.5253]]]], device='cuda:0')\n",
      "tensor([ 0.6078, -0.5224,  0.5570,  0.8423, -0.9833, -0.7158, -0.7134,  0.7342,\n",
      "         0.6013, -0.5773,  0.9130, -0.8002,  0.5566,  0.6549, -0.5720,  1.0000,\n",
      "         0.9965, -0.5161, -0.7031, -0.5287,  0.5133,  0.4940, -0.5785, -0.4989,\n",
      "        -0.8153, -0.7830,  0.5141,  0.7207,  0.5705,  0.7780, -0.8485,  0.5283,\n",
      "         0.9921,  0.9941, -0.5596, -0.5600,  0.6967,  0.6608,  0.9740, -0.6416,\n",
      "         1.0000,  0.5174,  0.5796,  0.5881, -0.5865, -0.6133,  0.5289,  0.5658,\n",
      "         0.7528,  0.9981,  0.9180,  0.6633,  0.6007,  0.5811,  0.5521,  0.9433,\n",
      "         0.9483, -0.5299,  0.9739,  0.5124,  0.5069,  0.5530,  0.5941, -0.5281],\n",
      "       device='cuda:0')\n",
      "tensor([ 1.0000,  1.0000,  0.6664,  0.9290, -0.7042,  0.9953, -0.9032,  0.6436,\n",
      "         0.7895, -0.9762, -0.8210, -0.8487,  0.6513,  0.8575, -0.9826,  0.9207,\n",
      "         0.5304, -1.0000,  0.5179, -0.9994, -0.9885, -0.6407, -0.9768, -1.0000,\n",
      "        -0.8490, -0.9130, -1.0000,  0.5155, -0.8674,  0.6360, -1.0000, -0.9904,\n",
      "         0.9030,  0.6479, -0.9797, -0.5808, -0.5792, -0.9699,  0.7286, -0.9518,\n",
      "        -0.7107, -0.9994, -0.9986, -0.7189, -0.9615, -0.9981, -0.9984, -0.6665,\n",
      "        -0.5607, -0.5267, -0.5320,  0.7921, -0.8021,  0.6715, -0.5905,  0.6617,\n",
      "        -0.6313, -0.9963,  0.8085, -1.0000,  0.6262, -0.9898,  0.7033, -0.9957],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5147,  0.5148, -0.5389],\n",
      "          [ 0.5145,  0.5142, -0.5329],\n",
      "          [ 0.5149, -1.0000, -0.5304]],\n",
      "\n",
      "         [[-0.5207, -0.5270, -0.5190],\n",
      "          [-0.5152, -0.5202, -0.5157],\n",
      "          [-0.5149, -0.5192, -0.5147]],\n",
      "\n",
      "         [[ 0.5147,  0.5350, -0.5356],\n",
      "          [ 0.5180,  0.5154, -0.5281],\n",
      "          [ 0.5190,  0.5161, -0.5239]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5236,  0.7397,  0.5192],\n",
      "          [ 0.5881,  0.5149,  0.5147],\n",
      "          [ 0.5500,  0.5263,  0.5194]],\n",
      "\n",
      "         [[-0.5262, -0.5208, -0.5244],\n",
      "          [-0.5411, -0.5279, -0.5248],\n",
      "          [ 0.5247, -0.6374, -0.5669]],\n",
      "\n",
      "         [[ 0.5259,  0.5307,  0.5149],\n",
      "          [ 0.5257,  0.9995,  0.5301],\n",
      "          [ 0.5318,  0.5147,  0.5148]]],\n",
      "\n",
      "\n",
      "        [[[-0.5289, -0.5190,  0.6523],\n",
      "          [-0.5205, -0.9987, -0.6627],\n",
      "          [-0.6110,  0.9485,  0.5345]],\n",
      "\n",
      "         [[-0.5238, -0.5250, -0.5223],\n",
      "          [-0.5285, -0.5256, -0.5283],\n",
      "          [-0.5264, -0.5253, -0.5266]],\n",
      "\n",
      "         [[ 0.5266,  0.5260,  0.5168],\n",
      "          [ 0.5175,  0.5256,  0.5263],\n",
      "          [ 0.5272,  0.5265,  0.5191]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5316,  0.5310,  0.5225],\n",
      "          [ 0.5183,  0.5254,  0.5259],\n",
      "          [ 0.5260,  0.5201,  0.5316]],\n",
      "\n",
      "         [[ 0.5313,  0.5308,  0.5314],\n",
      "          [ 0.5296,  0.5279,  0.5290],\n",
      "          [ 0.5285,  0.5262,  0.5266]],\n",
      "\n",
      "         [[ 0.5204,  0.5235,  0.5259],\n",
      "          [ 0.5262,  0.5324,  0.5148],\n",
      "          [ 0.5326,  0.5149,  0.5242]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5167, -0.5903,  0.5234],\n",
      "          [ 0.5264, -0.5198,  0.6281],\n",
      "          [ 0.5202,  0.7088,  0.7666]],\n",
      "\n",
      "         [[ 0.5172,  0.6986,  0.5150],\n",
      "          [-0.5305, -0.5256, -0.5365],\n",
      "          [-0.5310, -0.5364, -0.5496]],\n",
      "\n",
      "         [[ 0.5158,  0.5196,  0.7766],\n",
      "          [ 0.5263,  0.5481, -0.5264],\n",
      "          [-0.9986, -0.9970, -0.6346]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5292,  0.5321,  0.5167],\n",
      "          [ 0.5154,  0.5299,  0.5255],\n",
      "          [ 0.5210,  0.5148,  0.9978]],\n",
      "\n",
      "         [[ 0.5236,  0.5251,  0.5359],\n",
      "          [ 0.5203,  0.8368, -0.5309],\n",
      "          [ 0.5192, -0.5286, -0.5349]],\n",
      "\n",
      "         [[ 0.5255, -0.5170, -0.5148],\n",
      "          [-0.5290, -0.5229, -0.5150],\n",
      "          [-0.5154, -0.5321, -0.5148]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9030, -0.5138,  0.5339],\n",
      "          [ 0.5679,  0.5649,  0.5578],\n",
      "          [ 0.6242,  0.7073,  0.5561]],\n",
      "\n",
      "         [[-0.5204, -0.5232, -0.5271],\n",
      "          [-0.5263, -0.5284, -0.5304],\n",
      "          [-0.5250, -0.5211, -0.5225]],\n",
      "\n",
      "         [[ 0.5227,  0.5232,  0.5237],\n",
      "          [ 0.5349,  0.5246,  0.5247],\n",
      "          [ 0.5297,  0.5317,  0.5271]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5295, -0.5242, -0.5229],\n",
      "          [-0.5217, -0.5167, -0.5234],\n",
      "          [-0.5175, -0.5312, -0.5250]],\n",
      "\n",
      "         [[-0.5169, -0.5206, -0.5193],\n",
      "          [-0.5173, -0.5174, -0.5169],\n",
      "          [ 0.5269, -0.8527, -0.5166]],\n",
      "\n",
      "         [[-0.5324, -0.5203, -0.5224],\n",
      "          [-0.5278, -0.5248, -0.5152],\n",
      "          [-0.5257, -0.5214, -0.5548]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5184,  0.5186, -0.7318],\n",
      "          [ 0.5203,  0.5198,  0.5216],\n",
      "          [ 0.5194,  0.5195, -0.9367]],\n",
      "\n",
      "         [[-0.5236, -0.5194, -0.5317],\n",
      "          [-0.5248, -0.5213, -0.5259],\n",
      "          [-0.5248, -0.5252, -0.5267]],\n",
      "\n",
      "         [[ 0.5205,  0.5202,  0.5146],\n",
      "          [ 0.5188,  0.5165,  0.5162],\n",
      "          [ 0.5197,  0.5187, -0.5318]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5269, -0.5238, -0.5309],\n",
      "          [-0.5314, -0.5236, -0.5158],\n",
      "          [-0.5188, -0.5285, -0.5261]],\n",
      "\n",
      "         [[-0.5193, -0.5213, -0.5180],\n",
      "          [-0.5169, -0.5193, -0.5180],\n",
      "          [ 0.5345,  0.5766, -0.5150]],\n",
      "\n",
      "         [[-0.5153, -0.5153, -0.5292],\n",
      "          [-0.5147, -0.5323, -0.5326],\n",
      "          [-0.5280, -0.5325, -0.5171]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5220,  0.5194,  0.5200],\n",
      "          [ 0.5180,  0.5235,  0.5206],\n",
      "          [ 0.5227,  0.5195,  0.5220]],\n",
      "\n",
      "         [[ 0.5156,  0.5153,  0.5175],\n",
      "          [ 0.5152,  0.5156,  0.5150],\n",
      "          [ 0.5148,  0.5155,  0.5582]],\n",
      "\n",
      "         [[ 0.5153,  0.5155,  0.5152],\n",
      "          [ 0.5152,  0.5195,  0.5232],\n",
      "          [ 0.5223,  0.5153,  0.5192]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5179,  0.5215,  0.5260],\n",
      "          [ 0.5148,  0.5148,  0.5301],\n",
      "          [-0.5324,  0.5246,  0.5319]],\n",
      "\n",
      "         [[ 0.5350,  0.5351,  0.5324],\n",
      "          [ 0.5310,  0.5297,  0.5349],\n",
      "          [-0.5178, -0.5155,  0.5283]],\n",
      "\n",
      "         [[ 0.5152,  0.5196,  0.5212],\n",
      "          [ 0.5324,  0.5182,  0.5151],\n",
      "          [ 0.5147,  0.5148,  0.5236]]]], device='cuda:0')\n",
      "tensor([ 0.5898,  0.5763,  0.6470,  0.5098,  0.9967, -0.6258, -0.5424,  0.6118,\n",
      "         0.7402,  0.5955, -0.5732, -0.7937,  0.9835, -0.5527, -0.5089,  0.9442,\n",
      "         0.5533, -0.5120, -0.6773,  0.8965,  0.9959,  1.0000, -0.5243, -0.6935,\n",
      "         0.5037,  0.9416,  0.5337,  0.9943,  0.5066,  0.9954,  0.5162,  0.5614,\n",
      "        -0.6123, -0.7370,  0.7328, -0.7988, -0.5096,  0.5439, -0.5568,  1.0000,\n",
      "         0.8245,  0.5694,  0.5478, -0.4747, -0.6572,  0.6970,  0.8505, -0.7843,\n",
      "         0.6008,  0.6416, -0.5420,  0.7778, -0.9427,  0.9588,  0.9920,  0.5947,\n",
      "         0.5856,  0.9831,  0.7035,  0.7136,  0.9017, -0.6081,  0.6184,  0.5476],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.5281, -0.5736,  0.5617, -1.0000,  1.0000, -0.9980, -0.6116, -1.0000,\n",
      "        -0.9784, -0.9930,  0.6183, -0.5545, -0.5534, -0.9844,  1.0000,  0.5330,\n",
      "         0.5349,  0.9461,  1.0000, -0.9737, -0.5417,  0.7528,  0.7077, -0.8823,\n",
      "        -1.0000, -0.6645,  0.5245, -0.5571, -0.7238,  0.9359, -1.0000, -0.9605,\n",
      "        -0.9686, -0.6747,  0.6056, -0.5493, -0.5992,  0.5513, -0.9396, -0.5429,\n",
      "        -0.8557, -0.9510, -0.9992, -1.0000, -0.9708, -0.5766, -0.5651, -0.7342,\n",
      "         0.5375, -0.5351, -0.5601, -0.5288, -0.9688,  0.6082,  0.8374,  1.0000,\n",
      "        -0.5002,  0.5951,  0.7700, -0.6184,  0.5185,  0.7061,  0.5052, -0.6499],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.5472, -0.5573,  0.5155],\n",
      "          [-0.7953, -0.6377,  0.8698],\n",
      "          [ 0.5632,  0.5207, -0.9997]],\n",
      "\n",
      "         [[ 0.5410,  0.5363,  0.5330],\n",
      "          [ 0.5370,  0.5341,  0.5212],\n",
      "          [ 0.6416,  0.6801,  0.7032]],\n",
      "\n",
      "         [[-1.0000,  0.5163,  0.5185],\n",
      "          [-0.5231,  0.9554,  0.5200],\n",
      "          [-0.5241, -0.5273,  0.5219]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5381, -0.5405, -0.5393],\n",
      "          [-0.5370, -0.5380, -0.5402],\n",
      "          [-0.5345, -0.5364, -0.5415]],\n",
      "\n",
      "         [[ 0.5218,  0.5210,  0.5182],\n",
      "          [-0.5458, -0.5276, -0.5474],\n",
      "          [-0.5293, -0.5308,  0.5167]],\n",
      "\n",
      "         [[-0.5162, -0.5184, -0.5187],\n",
      "          [ 0.5223,  0.5218,  0.5191],\n",
      "          [ 0.5357,  0.5573,  0.5210]]],\n",
      "\n",
      "\n",
      "        [[[-0.6399, -0.8555,  0.5221],\n",
      "          [-0.7661, -0.8128,  0.5215],\n",
      "          [ 0.5208,  0.5245,  0.5224]],\n",
      "\n",
      "         [[ 0.5784,  0.5363,  0.5336],\n",
      "          [ 0.5369,  0.5371,  0.5381],\n",
      "          [ 0.5364,  0.9956,  0.6191]],\n",
      "\n",
      "         [[ 0.5290,  0.5329,  0.5288],\n",
      "          [ 0.5256,  0.5206,  0.5275],\n",
      "          [ 0.5270,  0.5283,  0.5318]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5250, -0.5681, -0.5405],\n",
      "          [-0.5239, -0.9828, -0.9789],\n",
      "          [-0.5245,  0.5474,  0.5531]],\n",
      "\n",
      "         [[ 0.5194,  0.5221,  0.5179],\n",
      "          [-0.8696, -0.7327, -0.7918],\n",
      "          [-0.7510, -0.8386, -0.8115]],\n",
      "\n",
      "         [[-0.6623, -0.5231, -0.5229],\n",
      "          [-0.9043, -0.5217, -0.5223],\n",
      "          [-0.5215, -0.5199, -0.5213]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5142,  0.5143,  0.5154],\n",
      "          [ 0.5148,  0.5148,  0.5147],\n",
      "          [ 0.5143,  0.5144,  0.5147]],\n",
      "\n",
      "         [[-0.5218, -0.5220, -0.5282],\n",
      "          [ 0.5332, -0.5219, -0.5321],\n",
      "          [ 0.5377, -0.5343, -0.5210]],\n",
      "\n",
      "         [[ 0.5218,  0.5188,  0.5173],\n",
      "          [ 0.5263,  0.5216,  0.5227],\n",
      "          [ 0.5203,  0.5172,  0.5192]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5233, -0.5210, -0.5176],\n",
      "          [-0.5275, -0.5239, -0.5231],\n",
      "          [-0.5288, -0.5241, -0.5225]],\n",
      "\n",
      "         [[-0.5412,  0.7673, -0.9910],\n",
      "          [-0.5449, -0.5391,  0.9416],\n",
      "          [-0.5365, -0.5301, -0.5296]],\n",
      "\n",
      "         [[ 0.5225, -0.9994, -0.9994],\n",
      "          [ 0.5235,  0.5183,  0.5148],\n",
      "          [ 0.5268,  0.5231,  0.5199]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5237,  0.5223,  0.5225],\n",
      "          [ 0.5226,  0.5221,  0.5227],\n",
      "          [ 0.5234,  0.5234,  0.5240]],\n",
      "\n",
      "         [[ 0.5197,  0.5442,  0.5259],\n",
      "          [ 0.5228,  0.5197,  0.5182],\n",
      "          [-0.5203,  0.5223,  0.5188]],\n",
      "\n",
      "         [[ 0.5230,  0.5152,  0.5211],\n",
      "          [ 0.5251,  0.5154,  0.5266],\n",
      "          [ 0.5213,  0.5154,  0.5189]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5266,  0.5256,  0.5269],\n",
      "          [ 0.5250,  0.5250,  0.5252],\n",
      "          [ 0.5255,  0.5248,  0.5268]],\n",
      "\n",
      "         [[-0.5191, -0.9976, -0.5199],\n",
      "          [-0.9940,  0.5256,  0.5251],\n",
      "          [ 0.5231,  0.5330,  0.5205]],\n",
      "\n",
      "         [[ 0.5328,  0.5293,  0.5284],\n",
      "          [ 0.5309,  0.5284,  0.5273],\n",
      "          [ 0.5312,  0.5287,  0.5284]]],\n",
      "\n",
      "\n",
      "        [[[-0.5309, -0.5238,  0.6829],\n",
      "          [-0.5499, -0.5570, -0.8205],\n",
      "          [-0.5282, -0.5284,  0.5226]],\n",
      "\n",
      "         [[ 0.8928,  0.9604,  0.8440],\n",
      "          [ 0.9774,  0.9719,  0.9123],\n",
      "          [ 0.9359,  0.9613,  0.8902]],\n",
      "\n",
      "         [[ 0.5246, -0.6221, -0.5189],\n",
      "          [ 0.5255, -0.9999, -0.5301],\n",
      "          [ 0.5167, -0.9999, -0.6281]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5236,  0.5243,  0.5559],\n",
      "          [ 0.5249,  0.5257,  0.5256],\n",
      "          [ 0.5262,  0.5270,  0.5267]],\n",
      "\n",
      "         [[ 0.5319,  0.5310,  0.5323],\n",
      "          [ 0.5284,  0.5203,  0.5286],\n",
      "          [ 0.5295,  0.5150,  0.5692]],\n",
      "\n",
      "         [[-0.5236, -0.5256, -0.5319],\n",
      "          [-0.5222, -0.5254, -0.5297],\n",
      "          [ 0.5274,  0.5239,  0.6329]]],\n",
      "\n",
      "\n",
      "        [[[-0.5305, -0.5217, -0.5177],\n",
      "          [-0.5176, -0.5322, -0.5210],\n",
      "          [-0.5203, -0.5152, -0.5150]],\n",
      "\n",
      "         [[-0.5148, -0.5184, -0.5323],\n",
      "          [-0.5324, -0.5223, -0.5204],\n",
      "          [-0.5324, -0.5235, -0.5324]],\n",
      "\n",
      "         [[ 0.8346, -0.5280, -0.5290],\n",
      "          [ 0.5300, -0.5302, -0.5152],\n",
      "          [ 0.5209, -0.5275, -0.5313]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5180, -0.5324, -0.5186],\n",
      "          [-0.5318, -0.5316, -0.5284],\n",
      "          [-0.5291, -0.5327, -0.5280]],\n",
      "\n",
      "         [[-0.5236, -0.5303, -0.5184],\n",
      "          [-0.5273, -0.5171, -0.5319],\n",
      "          [-0.5245, -0.5310, -0.5147]],\n",
      "\n",
      "         [[-0.5323, -0.5182, -0.5210],\n",
      "          [ 0.5452, -0.5199, -0.5276],\n",
      "          [-0.8432, -0.5429, -0.5249]]]], device='cuda:0')\n",
      "tensor([ 0.9601,  1.0000,  0.8123,  0.9229,  0.8486,  0.8150,  0.7077,  0.8831,\n",
      "         1.0000,  0.9819,  0.5464,  0.9994,  0.8356,  0.5946,  0.9787,  0.7679,\n",
      "         0.9923,  0.5351, -0.5614,  0.5455,  0.5263,  0.9457,  0.5177,  0.5775,\n",
      "         0.5673,  0.7903,  0.8814, -0.6777,  0.5621,  0.8967,  1.0000,  0.5577,\n",
      "        -0.5317,  0.5616,  0.7175,  0.5202,  0.5226,  0.5544,  0.5533,  0.9841,\n",
      "         0.8001,  0.5465, -0.5403,  0.5303,  0.6306,  0.9322,  0.5421,  0.5671,\n",
      "         0.9530,  1.0000,  1.0000,  0.4919,  0.5053,  0.5901,  0.5339,  0.5668,\n",
      "         0.6119,  0.8222,  0.5300,  0.9519,  0.5538,  0.8981,  0.7233,  0.5347],\n",
      "       device='cuda:0')\n",
      "tensor([-0.5282,  0.5746, -0.5793, -0.9116, -0.6128, -0.9956, -0.9615,  0.5559,\n",
      "         0.7032, -0.5540, -0.5233, -0.7255, -0.7433, -0.6474, -0.5527,  0.5725,\n",
      "         0.9704,  0.5192, -0.8192, -0.8643, -0.8258, -0.5727, -0.8899, -0.7664,\n",
      "        -0.5631, -0.8665,  0.6861,  0.5662, -0.9940, -0.8793,  0.8002,  0.5222,\n",
      "        -0.6817, -0.9101, -0.5123, -0.6742, -0.9667,  0.5203,  0.6745, -0.5871,\n",
      "        -0.5129, -1.0000, -0.9489, -0.8495,  0.5373,  0.5267,  0.5197, -0.8952,\n",
      "        -0.5469,  0.5705,  0.7720, -0.7033,  0.5222, -0.5168, -0.7657,  0.5875,\n",
      "        -0.9857, -0.6894, -0.9997,  0.5203, -0.8254, -0.5056, -0.5231, -0.9177],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5345,  0.5288,  0.5287],\n",
      "          [ 0.5220,  0.5238,  0.5150],\n",
      "          [ 0.5279,  0.5261,  0.5242]],\n",
      "\n",
      "         [[-0.5217,  0.5233,  0.5189],\n",
      "          [-0.5270,  0.5250,  0.5265],\n",
      "          [-0.5327, -0.8478,  0.5225]],\n",
      "\n",
      "         [[ 0.5253,  0.5241,  0.5147],\n",
      "          [ 0.5250,  0.5180,  0.5162],\n",
      "          [ 0.5223,  0.5185,  0.9981]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5234,  0.5290,  0.5275],\n",
      "          [ 0.5315,  0.5314,  0.5270],\n",
      "          [ 0.5313,  0.5285,  0.5203]],\n",
      "\n",
      "         [[ 0.5306,  0.5256,  0.5318],\n",
      "          [ 0.5191,  0.5269,  0.5316],\n",
      "          [ 0.5290,  0.5205,  0.5273]],\n",
      "\n",
      "         [[-0.5311, -0.5294, -0.5166],\n",
      "          [-0.5195, -0.5323, -0.6474],\n",
      "          [ 0.6056, -0.5238, -0.6065]]],\n",
      "\n",
      "\n",
      "        [[[-0.5189,  0.9105, -0.5213],\n",
      "          [-0.5288,  0.5219, -0.5300],\n",
      "          [ 0.5186,  0.5161,  0.5157]],\n",
      "\n",
      "         [[-0.5237, -0.5282, -0.5204],\n",
      "          [-0.5281, -0.5397, -0.5311],\n",
      "          [-0.5274, -0.5356, -0.5306]],\n",
      "\n",
      "         [[-0.5198, -0.5158, -0.5163],\n",
      "          [ 0.5250, -0.5163, -0.5164],\n",
      "          [ 0.9281,  0.5190, -0.5270]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5150, -0.5154, -0.5144],\n",
      "          [-0.5147, -0.5151, -0.5146],\n",
      "          [-0.5147, -0.5148, -0.5147]],\n",
      "\n",
      "         [[ 0.5228,  0.5150,  0.5146],\n",
      "          [ 0.5191,  0.5147,  0.5148],\n",
      "          [ 0.5166,  0.5156,  0.5148]],\n",
      "\n",
      "         [[ 0.5148,  0.5201,  0.5324],\n",
      "          [ 0.5149,  0.5147,  0.5267],\n",
      "          [ 0.5287,  0.5149,  0.5151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8555,  0.8664,  0.8510],\n",
      "          [ 0.5520,  0.6027,  0.6788],\n",
      "          [ 0.5209,  0.5229,  0.5202]],\n",
      "\n",
      "         [[ 0.5195,  0.5179,  0.5150],\n",
      "          [ 0.5184,  0.5190,  0.5149],\n",
      "          [ 0.5194,  0.5180,  0.5160]],\n",
      "\n",
      "         [[ 0.5863,  0.5234,  0.5180],\n",
      "          [ 0.5253,  0.5185,  0.5216],\n",
      "          [ 0.5251,  0.5197,  0.5197]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6118,  0.6361,  0.6088],\n",
      "          [ 0.5336,  0.5292,  0.5250],\n",
      "          [ 0.5215,  0.5177,  0.5169]],\n",
      "\n",
      "         [[ 0.7308,  0.8107,  0.8954],\n",
      "          [ 0.6643,  0.6908,  0.7256],\n",
      "          [ 0.6153,  0.7076,  0.7915]],\n",
      "\n",
      "         [[ 0.5282,  0.5157,  0.5291],\n",
      "          [ 0.5268,  0.5325,  0.5162],\n",
      "          [ 0.5327,  0.5324,  0.5172]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5153, -0.5253, -0.5272],\n",
      "          [-0.5160, -0.5204, -0.5271],\n",
      "          [-0.5202, -0.5217, -0.5278]],\n",
      "\n",
      "         [[-0.5264, -0.5283,  0.9076],\n",
      "          [-0.5214, -0.5251, -0.5301],\n",
      "          [-0.5207, -0.5230, -0.5300]],\n",
      "\n",
      "         [[ 0.5445,  0.5263,  0.5202],\n",
      "          [ 0.5391,  0.5237,  0.5166],\n",
      "          [ 0.5322,  0.5176,  0.5155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5196,  0.5259,  0.5204],\n",
      "          [ 0.5190,  0.5819,  0.5217],\n",
      "          [ 0.5246,  0.5240,  0.5217]],\n",
      "\n",
      "         [[-0.5179, -0.5172, -0.5264],\n",
      "          [-0.5190, -0.5216, -0.5240],\n",
      "          [-0.5217, -0.5222, -0.5167]],\n",
      "\n",
      "         [[-0.5159, -0.5148, -0.5322],\n",
      "          [-0.5295, -0.5147, -0.5285],\n",
      "          [-0.5320, -0.5148, -0.5147]]],\n",
      "\n",
      "\n",
      "        [[[-0.6094,  0.5451,  0.5556],\n",
      "          [ 0.5357, -0.9072, -0.5141],\n",
      "          [ 0.5344,  0.5435,  0.5503]],\n",
      "\n",
      "         [[-0.5252, -0.5181, -0.5602],\n",
      "          [-0.5215, -0.5166,  0.5302],\n",
      "          [ 0.5373,  0.5316,  0.5353]],\n",
      "\n",
      "         [[ 0.5435,  0.6059, -0.5161],\n",
      "          [ 0.5309, -0.5164, -0.5170],\n",
      "          [-0.9398, -0.9102, -0.5183]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5367,  0.5295,  0.5400],\n",
      "          [-0.5146,  0.5695, -0.5142],\n",
      "          [ 0.5367,  0.5393,  0.5442]],\n",
      "\n",
      "         [[-0.5156,  0.6544,  0.5340],\n",
      "          [-0.5173, -0.5170, -0.9361],\n",
      "          [-0.5197, -0.5203, -0.5147]],\n",
      "\n",
      "         [[-0.5690,  0.9956, -0.5284],\n",
      "          [-0.5196, -0.5164, -0.5317],\n",
      "          [-0.5228, -0.5318, -0.5151]]],\n",
      "\n",
      "\n",
      "        [[[-0.5160, -0.5157, -0.5251],\n",
      "          [-0.5214, -0.5157, -0.5248],\n",
      "          [-0.5220, -0.5174, -0.5170]],\n",
      "\n",
      "         [[ 0.5195,  0.5211,  0.5197],\n",
      "          [ 0.5177,  0.5233,  0.5170],\n",
      "          [-0.5311, -0.5348, -0.5411]],\n",
      "\n",
      "         [[ 0.5268, -0.6862, -0.5173],\n",
      "          [-0.5147, -0.5150, -0.5267],\n",
      "          [-0.5147, -0.5230, -0.5203]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5234,  0.5260,  0.5228],\n",
      "          [ 0.5258,  0.5244,  0.5203],\n",
      "          [-0.5348,  0.5233,  0.5197]],\n",
      "\n",
      "         [[-0.5200, -0.5290, -0.5186],\n",
      "          [-0.5271, -0.5288, -0.5191],\n",
      "          [-0.5213, -0.5231, -0.5201]],\n",
      "\n",
      "         [[ 0.5233,  0.5244,  0.5293],\n",
      "          [ 0.5264,  0.5155,  0.5181],\n",
      "          [ 0.5202,  0.5175, -0.5525]]]], device='cuda:0')\n",
      "tensor([ 0.7076,  0.6682,  0.5855,  0.5230,  0.9177,  0.9365,  0.6275,  0.5302,\n",
      "         0.5363,  0.5153,  0.7368, -0.6461, -0.5183,  0.5379,  0.5165,  0.7091,\n",
      "         0.5298,  1.0000,  0.5026,  0.6296,  0.6409,  0.9949,  0.6763,  0.5152,\n",
      "         0.4972,  0.9961,  0.8022, -0.7066,  0.5757,  0.9998,  0.6971,  0.6369,\n",
      "         0.5943,  0.9988,  0.8388,  0.5044,  0.9874,  0.9002, -0.9485,  1.0000,\n",
      "         0.8310, -0.9287,  0.5511, -0.5580,  0.5285,  0.5983,  0.5819,  1.0000,\n",
      "         0.5325,  0.5470,  0.9827, -0.6161,  0.9490,  0.5058,  1.0000,  0.5198,\n",
      "        -0.6204,  0.5798,  0.6244,  0.5138,  1.0000,  0.7956,  1.0000,  0.9964],\n",
      "       device='cuda:0')\n",
      "tensor([-0.5608,  0.7243, -0.5425, -0.9792,  1.0000, -0.7601,  0.6450,  0.9981,\n",
      "        -0.7939, -0.9992, -0.9791, -0.9990,  1.0000, -0.5359,  1.0000,  0.5427,\n",
      "         0.5415,  0.8770, -0.5874,  0.5247, -0.5157, -0.5433, -0.9047, -0.9987,\n",
      "        -1.0000, -0.5996, -0.7053, -0.9211, -0.9663, -0.5363,  0.5439,  0.7393,\n",
      "        -0.7071, -0.5225, -0.7607, -0.8509, -0.5411,  0.6919, -0.9182,  0.9847,\n",
      "         0.6014, -0.9969, -0.6772, -0.6915, -0.9927, -0.9230, -0.4885, -0.6142,\n",
      "        -0.9564, -0.5391, -0.9058, -0.9842,  0.5231, -0.7187,  0.6866, -0.7410,\n",
      "        -0.6373,  0.5641,  0.6980, -0.5756,  0.5484, -0.5942, -0.7198, -0.5350],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.5154, -0.5156, -0.5195],\n",
      "          [-0.5191, -0.5202,  0.7369],\n",
      "          [ 0.6295,  0.8490,  0.6435]],\n",
      "\n",
      "         [[-0.5165, -0.5179, -0.5444],\n",
      "          [ 0.5869, -0.8583,  0.6854],\n",
      "          [ 0.5524,  0.9749,  0.5290]],\n",
      "\n",
      "         [[-0.5146, -0.5153, -0.5148],\n",
      "          [ 0.5620, -0.5153,  0.5520],\n",
      "          [ 0.5622,  0.5660,  0.5474]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5262, -0.5598, -0.5376],\n",
      "          [ 0.5218, -0.6618,  0.5416],\n",
      "          [ 0.5293, -0.6614,  0.5448]],\n",
      "\n",
      "         [[-0.5266, -0.5195, -0.5209],\n",
      "          [-0.5326, -0.5211,  0.5357],\n",
      "          [-0.5363, -0.5311,  0.5123]],\n",
      "\n",
      "         [[-0.7559, -0.5998, -0.5931],\n",
      "          [-0.6223, -0.5307, -0.5251],\n",
      "          [-0.5295, -0.5224, -0.5246]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9967, -0.5162, -0.5210],\n",
      "          [ 0.9246, -0.5161,  0.7770],\n",
      "          [-0.5146, -0.5155, -0.5203]],\n",
      "\n",
      "         [[ 0.6877, -0.9972,  0.5233],\n",
      "          [ 0.5252,  0.9168,  0.8804],\n",
      "          [ 0.9726, -0.5187, -0.5150]],\n",
      "\n",
      "         [[-0.5193, -0.5149, -0.5148],\n",
      "          [ 0.5720,  0.7689, -0.5154],\n",
      "          [ 0.5731,  0.5852, -0.5152]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5596,  0.5544,  0.5639],\n",
      "          [ 0.5709, -0.6494,  0.9598],\n",
      "          [-0.6111, -0.5413, -0.6609]],\n",
      "\n",
      "         [[ 0.8107, -0.5173,  0.9320],\n",
      "          [ 0.9995, -0.5167, -0.5156],\n",
      "          [ 1.0000, -0.5204, -0.5187]],\n",
      "\n",
      "         [[-0.5195,  0.6607,  1.0000],\n",
      "          [-0.5162, -0.5153, -0.5151],\n",
      "          [ 0.5262, -0.5251, -0.5258]]],\n",
      "\n",
      "\n",
      "        [[[-0.5240,  0.8113, -0.5387],\n",
      "          [-0.5267, -0.5585,  0.5273],\n",
      "          [-0.5301,  0.5197,  0.5194]],\n",
      "\n",
      "         [[ 0.5262,  0.5288,  0.5307],\n",
      "          [ 0.5290,  0.5332,  0.5337],\n",
      "          [-0.5164, -0.5157, -0.6348]],\n",
      "\n",
      "         [[-0.5638, -0.9993,  0.9821],\n",
      "          [ 0.5174,  0.5162,  0.5233],\n",
      "          [ 0.5773,  0.5149, -0.5168]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5205, -0.5197, -0.5250],\n",
      "          [-0.5233, -0.5206, -0.5221],\n",
      "          [-0.5313, -0.5195, -0.5268]],\n",
      "\n",
      "         [[-0.5229, -0.5332, -0.5213],\n",
      "          [-0.5204, -0.5173, -0.5189],\n",
      "          [-0.5278, -0.5233, -0.5311]],\n",
      "\n",
      "         [[-0.5192, -0.5175, -0.5150],\n",
      "          [-0.5217, -0.8545,  0.5308],\n",
      "          [ 0.5240,  0.5148,  0.5149]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5317,  0.5318,  0.5184],\n",
      "          [ 0.5311,  0.5155,  0.5161],\n",
      "          [ 0.5278,  0.5191,  0.5274]],\n",
      "\n",
      "         [[ 0.5176,  0.5306,  0.5226],\n",
      "          [ 0.9851,  0.5290,  0.5159],\n",
      "          [-0.5207, -0.5231, -0.5296]],\n",
      "\n",
      "         [[ 0.5230,  0.5201,  0.5190],\n",
      "          [ 0.5295,  0.5206, -0.9982],\n",
      "          [ 0.5188,  0.5267,  0.5195]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5297, -0.5287, -0.5262],\n",
      "          [-0.5311, -0.5242, -0.5288],\n",
      "          [-0.5180, -0.5308, -0.5180]],\n",
      "\n",
      "         [[-0.5178,  0.9837, -0.7777],\n",
      "          [ 0.5184, -0.5951, -0.5297],\n",
      "          [-0.5180,  0.9857, -0.5241]],\n",
      "\n",
      "         [[ 0.9990,  0.5322,  0.5240],\n",
      "          [-0.5170,  0.7486,  0.5252],\n",
      "          [-0.5252, -0.5196, -0.5196]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5603, -0.5244,  0.7105],\n",
      "          [ 0.5265,  1.0000,  0.5216],\n",
      "          [ 0.5233,  0.7548,  0.5262]],\n",
      "\n",
      "         [[-0.5219, -0.5167, -0.5158],\n",
      "          [-0.5192, -0.5176, -0.5222],\n",
      "          [-0.5178, -0.5162, -0.5156]],\n",
      "\n",
      "         [[-0.5148, -0.5155, -0.5196],\n",
      "          [-0.5188, -0.5267, -0.5148],\n",
      "          [-0.5217, -0.5279, -0.5281]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5213, -0.5271,  0.9232],\n",
      "          [-0.5351, -0.5356, -0.5340],\n",
      "          [-0.5348, -0.5342, -0.5329]],\n",
      "\n",
      "         [[-0.5296, -0.5298,  0.5183],\n",
      "          [-0.5267, -0.5299, -0.5188],\n",
      "          [-0.9415,  0.5240,  0.5160]],\n",
      "\n",
      "         [[ 0.5303, -0.5256, -1.0000],\n",
      "          [-0.5320, -0.5317, -0.6727],\n",
      "          [ 0.5641, -0.5320, -0.6643]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5272,  0.5149,  0.5300],\n",
      "          [ 0.5263,  0.5148,  0.5226],\n",
      "          [ 0.5314,  0.5241,  0.5177]],\n",
      "\n",
      "         [[ 0.5279,  0.5244,  0.5280],\n",
      "          [ 0.5184,  0.5320,  0.5184],\n",
      "          [-0.5235, -0.5265, -0.5223]],\n",
      "\n",
      "         [[ 0.5199,  0.5285,  0.5162],\n",
      "          [ 0.5220,  0.5148,  0.5237],\n",
      "          [ 0.5155,  0.5263,  0.5150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5174, -0.5256, -0.5178],\n",
      "          [-0.5272, -0.5177, -0.5229],\n",
      "          [-0.5176, -0.5292, -0.5176]],\n",
      "\n",
      "         [[-0.5256, -0.5265, -0.5166],\n",
      "          [-0.5320, -0.5170, -0.5178],\n",
      "          [-0.5242, -0.5168, -0.5322]],\n",
      "\n",
      "         [[ 0.6178,  0.5284,  0.5326],\n",
      "          [ 0.5223,  0.5188,  0.5326],\n",
      "          [ 0.5274,  0.5210,  0.5265]]]], device='cuda:0')\n",
      "tensor([ 1.0000,  1.0000,  0.9417,  0.5010,  0.9533,  1.0000,  1.0000,  0.5185,\n",
      "         0.5096,  1.0000,  0.5454,  0.5201,  0.5454,  0.9877,  0.5082,  0.5245,\n",
      "         1.0000,  0.9584,  0.9530,  0.5239,  0.4997,  1.0000,  1.0000,  0.5562,\n",
      "         0.5301,  0.5060,  1.0000,  0.5453,  0.9995,  0.9918,  0.5331,  0.5175,\n",
      "         0.6158,  0.5222,  0.5476,  0.5071,  0.5141,  0.5287,  1.0000,  0.5316,\n",
      "         1.0000,  0.9945,  0.8906,  0.5861,  0.5306,  0.9797,  0.5187,  0.5782,\n",
      "         0.9201,  0.6786,  0.9176,  0.9355,  0.9476,  0.5299,  0.5100,  0.8552,\n",
      "         1.0000,  0.5184,  0.5514,  0.5174,  0.5382, -0.5620,  0.5217,  0.5551,\n",
      "         0.5177,  0.5060,  0.5214,  0.9799,  0.5974,  0.5913,  1.0000,  0.6990,\n",
      "         0.5107,  0.5569,  0.9628,  0.6003,  0.5011,  0.5336,  0.9797,  0.9987,\n",
      "         0.9810,  0.5194,  0.9773,  0.5605,  0.4961,  0.5619, -0.6704,  0.5607,\n",
      "         0.5163,  0.5326,  0.9894,  0.5216,  1.0000,  0.5306,  1.0000,  0.5953,\n",
      "         0.5692,  0.7350,  0.9920,  0.8625,  0.9870,  0.9689,  0.7570,  0.5699,\n",
      "         0.5074,  0.6485,  0.5153,  0.5073,  0.9023,  1.0000,  0.9619,  0.5204,\n",
      "         0.5160,  1.0000,  1.0000,  0.9891,  0.5293,  0.8183,  0.7996,  0.9963,\n",
      "         0.5550,  1.0000,  0.5235,  1.0000,  0.4978,  0.5773,  1.0000,  0.5013],\n",
      "       device='cuda:0')\n",
      "tensor([-0.5132,  0.8555, -0.8671, -0.7374, -0.8140, -0.5225, -0.5222, -0.9195,\n",
      "        -0.9792, -0.5219, -1.0000, -0.9125, -0.9439, -0.5262,  0.5162, -0.6279,\n",
      "        -0.5076, -0.5839,  0.5974, -0.9950, -0.9409, -0.5138, -0.5173, -0.7099,\n",
      "         0.5367, -0.9917, -0.5122, -0.9342, -0.5395, -0.5530, -0.6952, -0.9377,\n",
      "         0.5657,  0.5397,  0.5189,  0.5284, -0.9646, -0.5871, -0.5002, -0.7831,\n",
      "         0.5941, -0.5238, -0.7834, -0.9511, -0.8921, -0.8150, -0.6323, -0.9786,\n",
      "         0.5351, -0.6090, -0.5626, -0.5886, -0.6933, -0.9998, -1.0000, -0.7120,\n",
      "        -0.5512, -0.7976, -0.6635, -0.9455, -0.7135,  0.5087, -0.9975,  0.5089,\n",
      "        -0.7364, -0.9935, -0.9395, -0.9055, -0.9704,  0.5657, -0.5073, -0.6842,\n",
      "        -1.0000, -0.5251, -0.8005,  0.6181, -0.9349, -0.9634, -0.6083, -0.5386,\n",
      "         0.5188, -0.5243, -0.5722, -0.5437, -0.9984, -0.9837, -0.9249, -1.0000,\n",
      "        -0.9866, -0.9995, -0.5646, -0.9519,  0.8698, -0.9957, -0.9420, -0.9880,\n",
      "        -0.9904, -0.5876, -0.5521,  0.5620, -0.5386, -0.5591, -0.6212, -0.9540,\n",
      "        -0.9973, -0.9979, -0.9666, -0.9634, -0.7994,  0.5348,  0.5441, -0.5819,\n",
      "         0.5610, -0.5168, -0.5228, -0.5764, -0.9886, -0.5063, -0.9680,  0.6641,\n",
      "        -0.7776,  0.9899, -0.9539, -0.5225, -1.0000, -0.8738, -0.5275, -0.6845],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.9961,  0.6492,  0.5256],\n",
      "          [ 0.5267,  0.5186,  0.5253],\n",
      "          [ 0.5189,  0.5164,  0.5318]],\n",
      "\n",
      "         [[ 0.5231,  0.5319,  0.5248],\n",
      "          [ 0.5321,  0.5195,  0.5202],\n",
      "          [ 0.5211,  0.5147,  0.5254]],\n",
      "\n",
      "         [[-0.5262, -0.5678,  0.5319],\n",
      "          [ 0.5213, -0.5581,  0.5323],\n",
      "          [ 0.9997, -0.5325,  0.5160]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5213,  0.5298,  0.5277],\n",
      "          [ 0.5260,  0.5180,  0.5224],\n",
      "          [ 0.5522,  0.5314,  0.5775]],\n",
      "\n",
      "         [[-0.5171,  0.5281,  1.0000],\n",
      "          [ 0.5324,  0.5325,  0.5182],\n",
      "          [ 0.5297,  0.5161,  0.5273]],\n",
      "\n",
      "         [[ 0.5202,  0.5303,  0.5170],\n",
      "          [ 0.5226, -0.5213, -0.5265],\n",
      "          [-0.5782, -0.5295, -0.5175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5390,  0.5431,  0.5149],\n",
      "          [-0.5239, -0.5367,  0.9195],\n",
      "          [-0.5169, -0.5170, -0.5429]],\n",
      "\n",
      "         [[-0.9963, -0.5443,  0.5196],\n",
      "          [-0.5655, -0.5371,  0.5251],\n",
      "          [ 0.7929,  0.5366,  0.5173]],\n",
      "\n",
      "         [[-0.5212, -0.5281,  0.5299],\n",
      "          [ 0.5282, -0.5222, -0.5255],\n",
      "          [ 0.5146, -0.5259, -0.5227]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5222, -0.5235, -0.5169],\n",
      "          [-0.5186, -0.9009, -0.5314],\n",
      "          [-0.5270, -0.6854, -0.5316]],\n",
      "\n",
      "         [[-0.5151,  0.5250, -0.5219],\n",
      "          [-0.5290,  0.5235, -0.5175],\n",
      "          [-0.5388, -0.9654, -0.5286]],\n",
      "\n",
      "         [[-0.5207, -0.5251, -0.5310],\n",
      "          [-0.7628,  0.5907, -0.5314],\n",
      "          [-0.5253,  0.9719,  0.9262]]],\n",
      "\n",
      "\n",
      "        [[[-0.5290, -0.5278,  0.5289],\n",
      "          [ 0.5255,  0.5320,  0.5288],\n",
      "          [-0.5221, -0.5877,  0.5202]],\n",
      "\n",
      "         [[ 0.5364,  0.5234,  0.5160],\n",
      "          [ 0.8932, -0.6368,  0.5155],\n",
      "          [ 0.5236,  0.5148,  0.5158]],\n",
      "\n",
      "         [[ 0.6403,  0.5323, -0.5154],\n",
      "          [ 0.5278,  0.5182, -0.5225],\n",
      "          [ 0.5970,  0.5208, -0.5150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5178,  0.5150,  0.5152],\n",
      "          [ 0.5183, -0.5297, -0.5298],\n",
      "          [ 0.5190, -0.5173, -0.5264]],\n",
      "\n",
      "         [[-0.5381,  0.5261, -0.5282],\n",
      "          [-0.5306,  0.5274, -0.5195],\n",
      "          [-0.5513,  0.5212, -0.5186]],\n",
      "\n",
      "         [[ 0.5182,  0.5151,  0.5518],\n",
      "          [-0.5250, -0.5278, -0.5267],\n",
      "          [-0.5160, -0.5209, -0.5278]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5157,  0.5163,  0.5147],\n",
      "          [ 0.5880,  0.7390,  0.6143],\n",
      "          [ 0.5188,  0.5139,  0.5147]],\n",
      "\n",
      "         [[ 0.5192, -0.5597, -0.5452],\n",
      "          [ 0.5267, -0.5606, -0.5469],\n",
      "          [ 0.5296, -0.7734,  0.6119]],\n",
      "\n",
      "         [[ 0.5255,  0.5260, -0.8677],\n",
      "          [-0.9426, -0.5256, -0.5285],\n",
      "          [-0.5175, -0.5244, -0.5303]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5252,  0.9987,  0.6839],\n",
      "          [-0.8641,  0.7596,  0.5265],\n",
      "          [ 0.5486,  0.5265,  0.5331]],\n",
      "\n",
      "         [[-0.5277, -0.5190, -0.5219],\n",
      "          [-0.5285, -0.5218, -0.5303],\n",
      "          [ 0.5326, -0.5202, -0.5325]],\n",
      "\n",
      "         [[-0.5257, -0.5227,  0.6792],\n",
      "          [-0.5264, -0.6999,  0.6280],\n",
      "          [-0.5260, -0.5201, -0.5168]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5214, -0.5338,  0.5250],\n",
      "          [ 0.9159, -0.5512, -0.5764],\n",
      "          [ 0.5258, -0.5270,  0.5194]],\n",
      "\n",
      "         [[ 0.5601,  0.5148,  0.5204],\n",
      "          [ 0.5204,  0.6594, -0.6011],\n",
      "          [ 0.5190,  0.5187, -0.5695]],\n",
      "\n",
      "         [[-0.5188, -0.5325, -0.5335],\n",
      "          [-0.5181, -0.5328, -0.5316],\n",
      "          [-0.5175, -0.5332, -0.5316]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9739,  0.5306,  1.0000],\n",
      "          [ 0.5274,  0.6770, -0.9669],\n",
      "          [-0.5163,  0.6435,  0.5323]],\n",
      "\n",
      "         [[-0.5323,  0.5149,  0.5243],\n",
      "          [-0.9885,  0.5235,  0.5234],\n",
      "          [ 0.9265,  0.5201,  0.5237]],\n",
      "\n",
      "         [[-0.5175, -0.5257, -0.5261],\n",
      "          [-0.5145,  0.5561, -0.6865],\n",
      "          [-0.6309,  0.5323,  0.5276]]],\n",
      "\n",
      "\n",
      "        [[[-0.5204, -0.5267, -0.5850],\n",
      "          [-0.5161, -0.5166, -0.5286],\n",
      "          [-0.5312, -0.5223,  0.8298]],\n",
      "\n",
      "         [[-0.7439,  0.5191, -0.5245],\n",
      "          [-0.6344,  0.5276, -0.5293],\n",
      "          [-0.7823,  0.5301, -0.5150]],\n",
      "\n",
      "         [[ 0.5321,  0.5170,  0.5243],\n",
      "          [ 0.5259,  0.8585,  0.5252],\n",
      "          [ 0.5245, -0.5319,  0.5309]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5363,  0.5175,  0.6379],\n",
      "          [ 0.5183,  0.5154,  0.5185],\n",
      "          [-0.5178, -0.5236, -0.5210]],\n",
      "\n",
      "         [[-0.5276, -0.5238, -0.5177],\n",
      "          [-0.5149, -0.5326, -0.5318],\n",
      "          [-0.5228, -0.5203, -0.5188]],\n",
      "\n",
      "         [[ 0.5235,  0.5190,  0.6012],\n",
      "          [ 0.5177,  0.5303, -0.9482],\n",
      "          [ 0.5177,  0.5261, -0.5286]]]], device='cuda:0')\n",
      "tensor([ 0.5131,  1.0000,  0.8850,  0.9974,  0.9736,  0.5183,  0.9756,  1.0000,\n",
      "         0.5337,  0.5172,  0.6341,  1.0000,  0.9401,  1.0000, -0.8543,  0.9986,\n",
      "         0.5318,  0.5238,  1.0000,  0.9823,  0.5167,  0.5398,  0.5610,  0.9920,\n",
      "         0.5423,  0.5143,  1.0000,  0.5253,  0.5725,  0.5624,  0.8819,  0.6443,\n",
      "         1.0000,  1.0000,  0.9811,  0.9932,  0.7911,  0.5220,  1.0000,  0.5171,\n",
      "         0.5173,  0.5232,  1.0000,  1.0000,  0.5907,  0.5477,  1.0000,  0.5138,\n",
      "         0.7471,  0.9996,  0.5229,  0.5313,  1.0000,  0.5441,  0.5577,  0.5275,\n",
      "         1.0000,  0.7575,  1.0000,  0.9954,  1.0000, -0.6087,  0.5187, -0.9414,\n",
      "         0.5593,  0.9999,  0.9929,  0.7243,  1.0000,  0.8796,  0.7756,  0.5289,\n",
      "         1.0000,  1.0000,  0.9604,  0.5247,  0.5518,  0.5620,  0.9675,  0.7593,\n",
      "         0.5392,  0.9923,  0.5346,  0.9752,  0.5216,  0.5997,  1.0000,  0.5180,\n",
      "         0.5476,  0.6441,  0.5436,  1.0000,  0.5523,  0.9427,  0.9793,  0.5159,\n",
      "         0.9960,  0.5370,  0.5347,  1.0000,  0.5540,  0.9778,  0.6764,  0.5242,\n",
      "         0.5473,  0.5700,  0.5280,  0.5133,  0.5162,  1.0000,  1.0000,  0.5322,\n",
      "         0.5231,  1.0000,  1.0000,  0.5539,  0.9875,  0.9988,  0.6276,  0.9954,\n",
      "         0.6126,  0.5171,  0.6711,  0.5210,  0.9929,  0.9208,  1.0000,  0.5168],\n",
      "       device='cuda:0')\n",
      "tensor([-1.0000, -0.5135,  0.5112, -0.5759,  0.5127, -1.0000, -0.5838, -0.5355,\n",
      "        -0.6367, -1.0000, -1.0000, -0.5013,  0.5013,  0.6165, -0.5529,  0.6072,\n",
      "        -0.8730, -0.9925, -0.5145,  0.5461, -0.9998, -0.8063, -0.5213,  0.5270,\n",
      "        -0.8990, -0.9755,  0.5338, -0.5245, -0.6730, -0.5194,  0.5123, -0.5944,\n",
      "        -0.5595, -0.5397, -0.5297, -0.5952, -0.5192, -0.5362, -0.5630, -0.6382,\n",
      "        -0.6063, -0.6772,  0.5927, -0.5340, -0.5103, -0.5609,  0.4983, -1.0000,\n",
      "        -0.5528, -0.5141, -0.9996, -0.5828,  0.5582, -1.0000, -0.5863, -1.0000,\n",
      "        -0.5322, -0.5443,  0.5882,  0.5064, -0.5322, -0.5314, -0.6173, -0.9407,\n",
      "         0.5544, -0.6403, -0.5465,  0.5692, -0.5077, -0.5162, -0.9924, -1.0000,\n",
      "        -0.5154, -0.5558, -0.5289, -0.5176, -1.0000, -1.0000, -0.5530, -0.5316,\n",
      "        -0.7532,  0.5424, -1.0000, -0.5257, -1.0000, -0.5221, -0.5247, -0.6458,\n",
      "        -0.6345, -0.9951, -0.9338,  0.5746, -0.5537, -0.6023, -0.5251, -0.8303,\n",
      "         0.7469, -0.7900, -0.6083, -0.4888, -0.8965, -0.8458, -1.0000,  0.5920,\n",
      "        -0.9041,  0.5741, -0.8107, -0.6876, -1.0000,  0.5207,  0.5244, -0.9999,\n",
      "        -0.5772,  0.5517,  0.6245,  0.5168,  0.5124,  0.7491, -0.7854, -0.5457,\n",
      "        -0.5286, -1.0000, -0.5294, -1.0000,  0.5224, -0.6811, -0.5311, -0.5527],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5144]],\n",
      "\n",
      "         [[-1.0000]],\n",
      "\n",
      "         [[-1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5157]],\n",
      "\n",
      "         [[ 0.5147]],\n",
      "\n",
      "         [[ 0.5281]]],\n",
      "\n",
      "\n",
      "        [[[-0.6754]],\n",
      "\n",
      "         [[ 0.7448]],\n",
      "\n",
      "         [[-0.5189]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5157]],\n",
      "\n",
      "         [[ 0.5212]],\n",
      "\n",
      "         [[ 0.5266]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5240]],\n",
      "\n",
      "         [[ 0.5471]],\n",
      "\n",
      "         [[ 0.5199]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5286]],\n",
      "\n",
      "         [[-0.5699]],\n",
      "\n",
      "         [[ 0.5156]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5621]],\n",
      "\n",
      "         [[ 0.5891]],\n",
      "\n",
      "         [[-0.5427]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9533]],\n",
      "\n",
      "         [[-0.5154]],\n",
      "\n",
      "         [[ 0.7182]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000]],\n",
      "\n",
      "         [[-0.6102]],\n",
      "\n",
      "         [[ 0.9964]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5205]],\n",
      "\n",
      "         [[ 0.9831]],\n",
      "\n",
      "         [[ 0.5194]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5153]],\n",
      "\n",
      "         [[-0.5239]],\n",
      "\n",
      "         [[-0.5150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5518]],\n",
      "\n",
      "         [[-0.9206]],\n",
      "\n",
      "         [[ 0.5294]]]], device='cuda:0')\n",
      "tensor([ 1.0000,  1.0000,  0.5147,  0.8756,  0.5205, -0.7703,  0.8997,  0.8887,\n",
      "        -1.0000,  1.0000,  0.5167,  1.0000,  0.9030,  0.6759,  0.9941,  0.9176,\n",
      "         1.0000,  0.5291,  1.0000,  0.8281, -0.9994,  0.5564,  0.9139,  0.5345,\n",
      "        -0.9805,  0.5660,  0.5299,  1.0000,  0.5926,  1.0000,  1.0000,  0.6377,\n",
      "         0.9758,  0.9897,  0.9773,  0.7383,  0.6123,  1.0000,  1.0000,  0.6433,\n",
      "         0.9316,  0.8553,  0.8583,  1.0000,  1.0000,  0.9992,  0.8260,  0.5224,\n",
      "        -0.8853,  1.0000,  0.5422, -1.0000,  1.0000, -1.0000,  0.9462,  1.0000,\n",
      "         0.9923,  0.7701,  0.9029,  0.5450,  1.0000,  1.0000,  0.5158,  0.5346,\n",
      "         0.9914,  0.5632,  0.9970,  0.7116,  1.0000,  0.8456,  0.7243,  0.5250,\n",
      "         1.0000,  0.9639,  0.8967,  1.0000,  0.5368, -1.0000,  0.9322, -0.8454,\n",
      "        -0.9998,  0.7208, -0.7543,  0.9822,  1.0000, -0.9998,  0.9968,  0.8697,\n",
      "         0.5237,  0.5949, -0.9895,  0.5281,  1.0000,  0.9675,  1.0000, -1.0000,\n",
      "         0.5698, -1.0000, -1.0000,  1.0000, -1.0000,  0.9281,  0.5773,  0.8931,\n",
      "        -1.0000,  0.5973, -1.0000,  0.9999,  1.0000,  0.8277,  0.6533,  0.9186,\n",
      "        -0.9776,  0.6746,  0.9808,  0.6890,  0.6613,  0.5040,  0.5864,  0.9286,\n",
      "         0.9532,  1.0000,  0.9599, -1.0000,  0.6198,  0.8659,  1.0000,  0.9977],\n",
      "       device='cuda:0')\n",
      "tensor([-1.0000, -0.5135,  0.5112, -0.5759,  0.5127, -1.0000, -0.5838, -0.5355,\n",
      "        -0.6367, -1.0000, -1.0000, -0.5013,  0.5013,  0.6165, -0.5529,  0.6072,\n",
      "        -0.8730, -0.9925, -0.5145,  0.5461, -0.9998, -0.8063, -0.5213,  0.5270,\n",
      "        -0.8990, -0.9755,  0.5338, -0.5245, -0.6730, -0.5194,  0.5123, -0.5944,\n",
      "        -0.5595, -0.5397, -0.5297, -0.5952, -0.5192, -0.5362, -0.5630, -0.6382,\n",
      "        -0.6063, -0.6772,  0.5927, -0.5340, -0.5103, -0.5609,  0.4983, -1.0000,\n",
      "        -0.5528, -0.5141, -0.9996, -0.5828,  0.5582, -1.0000, -0.5863, -1.0000,\n",
      "        -0.5322, -0.5443,  0.5882,  0.5064, -0.5322, -0.5314, -0.6173, -0.9407,\n",
      "         0.5544, -0.6403, -0.5465,  0.5692, -0.5077, -0.5162, -0.9924, -1.0000,\n",
      "        -0.5154, -0.5558, -0.5289, -0.5176, -1.0000, -1.0000, -0.5530, -0.5316,\n",
      "        -0.7532,  0.5424, -1.0000, -0.5257, -1.0000, -0.5221, -0.5247, -0.6458,\n",
      "        -0.6345, -0.9951, -0.9338,  0.5746, -0.5537, -0.6023, -0.5251, -0.8303,\n",
      "         0.7469, -0.7900, -0.6083, -0.4888, -0.8965, -0.8458, -1.0000,  0.5920,\n",
      "        -0.9041,  0.5741, -0.8107, -0.6876, -1.0000,  0.5207,  0.5244, -0.9999,\n",
      "        -0.5772,  0.5517,  0.6245,  0.5168,  0.5124,  0.7491, -0.7854, -0.5457,\n",
      "        -0.5286, -1.0000, -0.5294, -1.0000,  0.5224, -0.6811, -0.5311, -0.5527],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5206,  0.5185,  0.5322],\n",
      "          [ 0.5319,  0.5322,  0.5319],\n",
      "          [ 0.5175,  0.5296,  0.5271]],\n",
      "\n",
      "         [[-0.5351, -0.5396, -0.6221],\n",
      "          [-0.5833, -0.5973, -0.5908],\n",
      "          [-0.7303, -0.7150,  0.8841]],\n",
      "\n",
      "         [[ 0.5243,  0.5304,  0.5267],\n",
      "          [ 0.5191,  0.5261,  0.5277],\n",
      "          [ 0.5179,  0.5235,  0.5285]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5321, -0.5297, -0.5878],\n",
      "          [-0.5242, -0.9988, -0.5688],\n",
      "          [ 0.5197,  0.5225,  0.5254]],\n",
      "\n",
      "         [[-0.5212, -0.5253, -0.8775],\n",
      "          [-0.5152, -0.5258, -0.5580],\n",
      "          [-0.5219, -0.5239, -0.5271]],\n",
      "\n",
      "         [[ 0.6333,  1.0000, -0.5321],\n",
      "          [ 0.5203,  0.6564, -0.5520],\n",
      "          [ 0.5199, -0.5203, -0.5171]]],\n",
      "\n",
      "\n",
      "        [[[-0.5228, -0.5162,  0.9964],\n",
      "          [-0.5334, -0.6133, -0.9685],\n",
      "          [-0.9975,  0.7161, -0.5642]],\n",
      "\n",
      "         [[ 0.5172,  0.5136,  0.5162],\n",
      "          [ 0.5187,  0.5190,  0.5184],\n",
      "          [ 0.5167, -0.7315, -0.6256]],\n",
      "\n",
      "         [[ 0.7055,  0.5224,  0.5410],\n",
      "          [-0.6035,  0.5247, -0.5169],\n",
      "          [ 0.5171,  0.5253, -0.6371]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5214,  0.5238,  0.5394],\n",
      "          [-0.7330,  0.5312,  0.5333],\n",
      "          [ 0.5364,  0.5430,  0.5383]],\n",
      "\n",
      "         [[-0.5237, -0.5181,  0.6856],\n",
      "          [ 0.7602, -0.5138,  0.9920],\n",
      "          [ 0.5259, -0.5170,  0.5484]],\n",
      "\n",
      "         [[ 0.5950, -0.9994, -0.6018],\n",
      "          [-0.9977, -0.5168,  0.9699],\n",
      "          [ 0.5226, -0.5480,  0.5237]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5214,  0.5212,  0.5257],\n",
      "          [-0.5244,  0.5486,  0.5310],\n",
      "          [-0.5322, -0.5323, -0.8251]],\n",
      "\n",
      "         [[-0.5277, -0.5183, -0.5148],\n",
      "          [-0.9997,  0.5207,  0.5152],\n",
      "          [-0.9978,  0.5273, -0.9560]],\n",
      "\n",
      "         [[ 0.5277,  0.5147,  0.5149],\n",
      "          [ 0.5251,  0.5200,  0.5173],\n",
      "          [ 0.5285,  0.5206,  0.5221]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5292,  0.9939,  0.5162],\n",
      "          [ 0.5175,  0.5349, -0.7167],\n",
      "          [ 0.5969, -0.9999,  0.5199]],\n",
      "\n",
      "         [[ 0.9999,  0.9994, -0.9885],\n",
      "          [-0.7104, -0.5150, -0.5282],\n",
      "          [-0.5303, -0.5252, -0.5313]],\n",
      "\n",
      "         [[ 0.5279,  0.5307,  0.5262],\n",
      "          [ 0.5309,  0.5250,  0.5222],\n",
      "          [ 0.5173,  0.5234,  0.5152]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5225,  0.5158,  0.6602],\n",
      "          [ 0.5318,  0.5147,  0.5267],\n",
      "          [-0.5152, -0.5153, -0.5154]],\n",
      "\n",
      "         [[-0.5272, -0.5307, -0.5324],\n",
      "          [-0.5153, -0.6505, -0.5205],\n",
      "          [-0.5150, -0.5287, -0.5292]],\n",
      "\n",
      "         [[ 0.5327,  0.6268,  0.6414],\n",
      "          [ 0.5189,  0.5184,  0.5185],\n",
      "          [ 0.6820,  0.5182,  0.6355]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5303,  0.5286,  0.5161],\n",
      "          [ 0.5157,  0.5159,  0.5244],\n",
      "          [ 0.5184,  0.5155,  0.5322]],\n",
      "\n",
      "         [[ 1.0000,  0.7675, -0.5192],\n",
      "          [ 0.5253,  0.5271,  0.6196],\n",
      "          [ 0.5241,  0.5212,  0.5337]],\n",
      "\n",
      "         [[ 0.5310,  0.5249,  0.5299],\n",
      "          [ 0.5320,  0.5200,  0.5152],\n",
      "          [ 0.9926, -0.5241, -0.5219]]],\n",
      "\n",
      "\n",
      "        [[[-0.5269,  0.9647,  0.6464],\n",
      "          [ 0.8470,  0.9260,  0.5161],\n",
      "          [ 0.6670,  0.5217,  0.5283]],\n",
      "\n",
      "         [[ 0.5713,  0.5370, -0.5194],\n",
      "          [ 0.6236,  0.5167, -0.5158],\n",
      "          [ 0.9596,  0.6452, -0.5215]],\n",
      "\n",
      "         [[-0.5208, -0.5171,  0.7568],\n",
      "          [-0.9042, -0.5189,  0.5686],\n",
      "          [-0.6035, -0.5179,  0.7329]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5318,  0.5220,  0.5930],\n",
      "          [-0.6909, -0.7831,  0.5295],\n",
      "          [ 0.5930, -0.7430, -0.9128]],\n",
      "\n",
      "         [[-0.5366, -0.5414,  0.8287],\n",
      "          [-0.5367,  0.5171,  0.5376],\n",
      "          [-0.5391,  0.5168,  0.5474]],\n",
      "\n",
      "         [[ 0.9707, -0.5208, -0.6230],\n",
      "          [ 0.6714, -0.7891, -0.7456],\n",
      "          [-0.7624, -0.5318, -0.5162]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5307,  0.5228,  0.5304],\n",
      "          [ 0.5287,  0.5305,  0.5240],\n",
      "          [-0.5273, -0.5217, -0.5613]],\n",
      "\n",
      "         [[ 0.9276,  0.9997,  0.5189],\n",
      "          [ 0.5261,  0.5218,  0.5184],\n",
      "          [ 0.5341,  0.5283,  0.5282]],\n",
      "\n",
      "         [[ 0.5219,  0.5229,  0.5220],\n",
      "          [-0.5394, -0.5374, -0.5704],\n",
      "          [-0.5215, -0.5211, -0.5293]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5347, -0.9964, -0.5185],\n",
      "          [ 0.5305,  0.5313,  0.7144],\n",
      "          [ 0.5306,  0.5154,  0.5271]],\n",
      "\n",
      "         [[ 0.5268,  0.5231,  0.5205],\n",
      "          [ 0.5172,  0.5245,  0.5165],\n",
      "          [ 0.5233,  0.5208,  0.5226]],\n",
      "\n",
      "         [[ 0.5195,  0.5225,  0.5203],\n",
      "          [ 0.5205,  0.5195,  0.5177],\n",
      "          [-0.5296, -0.5162, -0.5310]]]], device='cuda:0')\n",
      "tensor([0.5900, 1.0000, 0.5228, 0.5120, 0.5352, 0.9998, 0.6576, 0.5333, 0.5276,\n",
      "        0.6587, 0.5362, 0.5423, 1.0000, 1.0000, 0.5183, 1.0000, 0.6216, 0.5763,\n",
      "        0.9959, 0.5152, 0.5281, 0.5260, 0.5161, 0.5965, 0.9979, 0.5182, 0.5231,\n",
      "        0.9942, 0.5196, 1.0000, 0.5164, 0.5158, 0.5539, 0.5193, 0.9743, 0.5658,\n",
      "        0.9927, 1.0000, 0.5871, 0.5179, 0.5411, 0.5980, 0.5261, 0.9929, 0.9181,\n",
      "        1.0000, 0.5991, 1.0000, 0.5219, 0.5303, 0.9628, 0.7307, 0.8473, 0.5548,\n",
      "        0.5929, 0.5933, 0.5249, 1.0000, 0.5184, 0.9973, 0.9269, 0.9277, 1.0000,\n",
      "        0.5676, 0.8479, 0.9941, 0.5259, 0.5599, 0.6716, 1.0000, 0.5159, 0.5642,\n",
      "        0.7710, 0.5679, 0.9886, 0.5339, 0.7073, 0.9704, 0.5192, 0.9848, 0.5156,\n",
      "        0.5806, 0.9376, 0.5274, 0.5158, 1.0000, 0.5238, 0.9715, 0.5758, 0.9873,\n",
      "        0.5634, 0.5505, 0.5251, 0.5145, 0.6042, 0.6248, 0.5157, 0.5326, 0.5703,\n",
      "        0.5169, 0.9350, 0.5838, 1.0000, 0.5825, 0.6355, 0.9803, 0.5995, 0.5213,\n",
      "        0.5491, 0.9971, 0.5630, 0.9400, 0.9155, 0.8671, 0.5349, 0.5611, 0.6640,\n",
      "        0.9291, 0.5287, 0.5351, 0.5503, 1.0000, 1.0000, 0.5190, 0.8322, 0.5165,\n",
      "        1.0000, 0.5502], device='cuda:0')\n",
      "tensor([ 0.5505, -0.5048, -0.6837, -0.5416, -0.5971, -0.5208,  0.5634, -0.8051,\n",
      "        -0.6121, -0.7392, -0.9887, -0.7171, -0.5172, -0.5123,  0.5157, -0.5269,\n",
      "         0.5122,  0.5296, -0.5206, -1.0000, -0.6777, -0.9881, -0.9999, -0.5397,\n",
      "         0.8261,  0.5238, -0.9902, -0.5510, -0.8067, -0.5131, -1.0000,  0.5241,\n",
      "        -1.0000, -0.9996,  0.5206,  0.5522, -0.5502, -0.5141, -0.5576, -1.0000,\n",
      "         0.5246,  0.5286, -0.7816, -0.5307, -0.4960,  0.5478, -0.5248, -0.5390,\n",
      "         0.5204, -0.5548, -0.5882,  0.5205, -0.5765, -0.9960, -1.0000, -0.7159,\n",
      "        -1.0000, -0.5188, -0.7669, -0.5176, -0.5671, -0.9955, -0.5213,  0.5174,\n",
      "        -0.7835, -0.5081, -1.0000,  0.5131, -0.5257, -0.5256, -1.0000,  0.5336,\n",
      "        -0.8312, -0.5948, -0.5411, -1.0000, -0.6728,  0.9966, -0.9737, -0.6097,\n",
      "        -0.8498, -0.9991, -0.5396, -0.6425, -1.0000, -0.5699, -0.5578,  0.5186,\n",
      "        -0.5312, -0.5477, -0.6554, -0.5959, -0.9999, -1.0000,  0.6528,  0.5172,\n",
      "        -1.0000, -0.8486, -0.8025, -0.9666, -0.6020, -0.7817, -0.5094, -1.0000,\n",
      "        -0.9394, -0.5411, -0.9983, -0.9996, -0.9803, -0.5309, -0.9813, -0.6628,\n",
      "        -0.9592, -0.5810, -0.9853, -0.9858,  0.5180,  0.5559, -0.7843, -0.9163,\n",
      "        -0.9980, -0.5058,  0.5411, -0.9195, -0.7006, -0.9934, -0.5689, -0.7028],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5284,  0.5308,  0.5203],\n",
      "          [-0.5240, -0.5257,  0.6974],\n",
      "          [ 0.5534, -0.5436, -0.5202]],\n",
      "\n",
      "         [[ 0.9999,  0.5346,  0.5529],\n",
      "          [ 1.0000,  0.6749,  0.5398],\n",
      "          [ 0.6980,  0.5232, -0.5190]],\n",
      "\n",
      "         [[ 0.5278,  0.5160,  0.5207],\n",
      "          [ 0.5185,  0.5182,  0.5184],\n",
      "          [ 0.5265,  0.5317,  0.5150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5228,  0.5271,  0.5150],\n",
      "          [ 0.5191,  0.5190,  0.5173],\n",
      "          [ 0.5203,  0.5314,  0.5280]],\n",
      "\n",
      "         [[ 0.5218,  0.5243,  0.5313],\n",
      "          [ 0.6558,  0.5315,  0.5250],\n",
      "          [ 0.5189,  0.5320,  0.5316]],\n",
      "\n",
      "         [[ 0.5267,  0.5226,  0.5196],\n",
      "          [ 0.5278,  0.5197,  0.5259],\n",
      "          [ 0.5259,  0.5183,  0.5173]]],\n",
      "\n",
      "\n",
      "        [[[-0.5212, -0.5206, -0.5279],\n",
      "          [ 0.9475, -0.8154,  0.5191],\n",
      "          [ 0.5174,  0.5164,  0.5162]],\n",
      "\n",
      "         [[-0.5415, -0.5284, -0.5191],\n",
      "          [-0.5276, -0.5163, -0.5264],\n",
      "          [-0.5273, -0.5169, -0.5227]],\n",
      "\n",
      "         [[-0.5178, -0.5275, -0.5223],\n",
      "          [ 0.7733,  0.5270, -0.6227],\n",
      "          [ 0.5268,  0.5202,  0.5301]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5226, -0.5205, -0.5178],\n",
      "          [ 0.5313,  0.5270,  0.5229],\n",
      "          [ 0.5204,  0.5304,  0.5277]],\n",
      "\n",
      "         [[-0.5206, -0.5201, -0.5605],\n",
      "          [-0.5927, -0.5180, -0.6451],\n",
      "          [-0.5265, -0.5495, -0.6439]],\n",
      "\n",
      "         [[ 0.9909,  0.6700,  0.5519],\n",
      "          [ 0.5264,  0.5310,  0.5254],\n",
      "          [ 0.5244,  0.5181,  0.5237]]],\n",
      "\n",
      "\n",
      "        [[[-0.9826, -0.6071, -0.9425],\n",
      "          [-0.5249, -0.5199, -0.5277],\n",
      "          [-0.5255, -0.5250, -0.5154]],\n",
      "\n",
      "         [[-0.5263, -0.5329, -0.5284],\n",
      "          [-0.5410, -0.6168, -0.5278],\n",
      "          [-0.5261, -0.5375, -0.5278]],\n",
      "\n",
      "         [[-0.9522,  0.9995, -0.6354],\n",
      "          [-0.5221, -0.5152, -0.5151],\n",
      "          [-0.5297, -0.5299, -0.5214]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9987,  0.5279,  0.5308],\n",
      "          [-0.5155, -0.6226, -0.6666],\n",
      "          [-0.6200,  0.5290,  0.5271]],\n",
      "\n",
      "         [[-0.6339, -0.5281, -0.5326],\n",
      "          [-0.5297, -0.5313, -0.5485],\n",
      "          [-0.5257, -0.6042, -0.5362]],\n",
      "\n",
      "         [[ 0.5301,  0.5216,  0.5290],\n",
      "          [ 0.9817, -0.5644,  0.7478],\n",
      "          [-0.5178,  0.9959,  0.5313]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5267, -0.5254, -0.9821],\n",
      "          [-0.5230, -0.9911,  0.5232],\n",
      "          [ 0.5249,  0.5287,  0.9212]],\n",
      "\n",
      "         [[ 0.5341, -0.5174, -0.5376],\n",
      "          [-0.5337, -0.5250, -0.5239],\n",
      "          [ 0.5160, -0.5199,  0.7382]],\n",
      "\n",
      "         [[ 0.5489, -0.9792,  0.5281],\n",
      "          [ 0.6347,  0.5324,  0.5196],\n",
      "          [ 0.5251,  0.5306,  0.5191]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5267,  0.5271,  0.5202],\n",
      "          [ 0.5196,  0.5238,  0.5202],\n",
      "          [ 0.5292,  0.5263,  0.5230]],\n",
      "\n",
      "         [[ 0.5284,  0.5971,  0.5371],\n",
      "          [-0.7699, -0.5193,  0.5294],\n",
      "          [ 0.5273,  0.5393,  0.6755]],\n",
      "\n",
      "         [[ 0.6156,  0.5226,  0.5268],\n",
      "          [ 0.5305,  0.5302,  0.5187],\n",
      "          [ 0.5219,  0.5235,  0.5196]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5219,  0.5295,  0.5270],\n",
      "          [ 0.5206,  0.5271,  0.5190],\n",
      "          [ 0.5213,  0.5257,  0.5223]],\n",
      "\n",
      "         [[-0.5348, -0.5218, -0.5199],\n",
      "          [-0.5607,  0.5175, -0.5187],\n",
      "          [-0.5286,  0.5277, -0.5549]],\n",
      "\n",
      "         [[ 0.5324,  0.5164,  0.5252],\n",
      "          [ 0.9998,  0.5448,  0.6768],\n",
      "          [ 0.5576,  0.5181,  0.9943]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5241,  0.5306,  0.5151],\n",
      "          [ 0.5204,  0.5318,  0.5302],\n",
      "          [ 0.5155,  0.5220,  0.5164]],\n",
      "\n",
      "         [[ 0.5191,  0.5147,  0.5170],\n",
      "          [ 0.5298,  0.5152,  0.5154],\n",
      "          [ 0.5290,  0.5152,  0.5147]],\n",
      "\n",
      "         [[ 0.5319,  0.5239,  0.5194],\n",
      "          [ 0.5168,  0.5212,  0.5238],\n",
      "          [-0.6720,  0.5638,  0.5147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5293,  0.5182,  0.9664],\n",
      "          [ 0.5222,  0.5153,  0.5154],\n",
      "          [ 0.5258,  0.5284,  0.5217]],\n",
      "\n",
      "         [[ 0.9982, -0.5172, -0.5204],\n",
      "          [-0.9576, -0.9675, -0.5153],\n",
      "          [ 0.5216, -0.5249, -0.5153]],\n",
      "\n",
      "         [[ 0.5250,  0.5233, -0.6357],\n",
      "          [ 0.5293,  0.5273, -0.7236],\n",
      "          [ 0.5291,  0.5267, -0.7230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5216,  0.5251,  0.7189],\n",
      "          [ 0.5185,  0.5161, -0.5285],\n",
      "          [ 0.5302,  0.5271, -0.9197]],\n",
      "\n",
      "         [[-0.5175, -0.5245, -0.5255],\n",
      "          [-0.5251, -0.5177, -0.5187],\n",
      "          [-0.5224, -0.5171, -0.5153]],\n",
      "\n",
      "         [[ 0.5216,  0.5220,  0.5466],\n",
      "          [ 0.5171,  0.5289,  0.5190],\n",
      "          [ 0.5173,  0.5167,  0.9455]]]], device='cuda:0')\n",
      "tensor([ 0.5124,  0.5379,  0.9581,  0.9981,  0.5086,  0.5370,  0.5159,  0.5146,\n",
      "         0.7626,  0.5350,  0.5057,  0.9994,  0.9622,  1.0000,  0.5830,  0.9939,\n",
      "         0.5028,  0.5567,  1.0000,  0.9310,  0.5422,  0.9954,  1.0000,  0.4890,\n",
      "         0.5995, -1.0000,  1.0000,  0.5190,  0.9861,  1.0000,  0.7669,  0.9998,\n",
      "         0.9999,  1.0000,  0.9925, -0.5266,  0.5308,  0.5009,  1.0000,  0.6142,\n",
      "         1.0000,  0.5108,  0.9995,  0.7535,  1.0000,  0.6326,  1.0000,  0.5961,\n",
      "         0.5166,  0.9965,  0.7243,  0.5063,  0.9310,  0.5130,  0.6905,  0.5416,\n",
      "        -0.8090,  1.0000,  1.0000,  1.0000,  1.0000, -0.6941,  0.9883,  0.9877,\n",
      "         0.5743,  1.0000, -0.8326, -0.5710,  1.0000,  1.0000,  0.8694, -1.0000,\n",
      "         1.0000,  0.9999,  0.5139,  0.4933,  0.9386,  0.5272,  1.0000,  1.0000,\n",
      "         0.5462,  0.7186,  0.5148,  0.9452,  0.5107,  0.5289,  0.5215,  0.7122,\n",
      "         0.7951,  0.5202,  0.5194,  0.9962,  0.9319,  0.5084,  1.0000,  0.5649,\n",
      "         1.0000, -0.6281,  0.5165,  0.7536,  1.0000,  0.9951,  0.9766,  0.8905,\n",
      "         0.5219, -0.7633,  0.5252,  0.5685,  0.5295,  1.0000, -0.5157,  0.5395,\n",
      "         0.6219,  0.5326,  0.5329,  0.9876,  1.0000,  0.9181,  0.9869,  0.7663,\n",
      "         0.6646,  0.5044,  0.9872,  0.5123, -0.5669,  0.5063,  0.4943,  0.9983],\n",
      "       device='cuda:0')\n",
      "tensor([-0.9993, -0.5132, -0.8123, -0.5401, -1.0000, -0.9971,  1.0000, -0.6450,\n",
      "        -0.5160,  0.5212, -1.0000,  0.5437,  0.5137, -0.5090, -0.5331, -0.9289,\n",
      "        -1.0000, -0.9912, -0.4980,  0.5159, -0.9994,  0.9959, -0.5601, -1.0000,\n",
      "         0.5165, -0.5949,  0.5321,  0.5726, -0.9947, -0.5180,  0.5267, -0.5462,\n",
      "         0.6469,  0.8552,  0.5371, -0.5685, -0.9997, -0.6120,  0.5135, -0.5149,\n",
      "         0.5321,  1.0000, -0.7983,  0.6631,  0.5205,  0.5227,  0.5180, -0.9981,\n",
      "        -0.8184,  0.5855, -0.8477, -0.6631, -0.6427, -1.0000,  0.5288, -0.9994,\n",
      "        -0.5169,  0.5140, -0.5372, -0.6799,  0.9842, -0.5657, -0.8245,  0.5072,\n",
      "         0.5360, -0.5470, -0.5432, -0.8721,  0.9593,  0.7766,  0.7126, -0.9418,\n",
      "        -0.5074, -0.5251, -0.5308, -1.0000, -0.5340, -0.9999, -0.5250, -0.4995,\n",
      "        -0.9993,  0.5378, -1.0000,  0.9959, -1.0000, -0.9013, -0.7300,  0.5184,\n",
      "        -0.6300,  0.5152, -1.0000,  0.6530, -0.7913, -0.5420, -0.5606, -1.0000,\n",
      "        -0.6031, -0.6422, -1.0000,  0.5142,  0.5472, -0.5273, -0.7221,  0.5239,\n",
      "        -1.0000, -0.9057, -1.0000, -1.0000, -1.0000,  0.5321, -0.9169, -0.9994,\n",
      "        -0.6036, -0.7997,  0.5281,  0.5225,  0.5168, -0.8649, -0.5280,  0.5215,\n",
      "        -0.5211, -0.9950,  0.6746, -0.9998, -0.9581, -0.9602, -0.5415, -0.5327],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5288,  0.5210,  0.5300],\n",
      "          [ 0.5199,  0.5283,  0.5215],\n",
      "          [ 0.5266,  0.5300,  0.5299]],\n",
      "\n",
      "         [[-0.5706, -0.5212, -0.5228],\n",
      "          [-0.5167, -0.9979, -0.5175],\n",
      "          [-0.5167,  0.9924, -0.5157]],\n",
      "\n",
      "         [[ 0.5246,  0.5246,  0.5171],\n",
      "          [ 0.9157,  0.9442,  0.5200],\n",
      "          [ 0.5238, -0.5768,  0.5283]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5174,  0.5163,  0.9802],\n",
      "          [ 0.9610, -0.6461, -0.5279],\n",
      "          [-0.5956, -0.5252, -0.5219]],\n",
      "\n",
      "         [[ 0.5339,  0.5347,  0.5229],\n",
      "          [ 0.5303,  0.5315,  0.5231],\n",
      "          [ 0.5309,  0.5222,  0.5316]],\n",
      "\n",
      "         [[ 0.9725, -0.5259, -0.5210],\n",
      "          [-0.5244, -0.5188, -0.5231],\n",
      "          [-0.5172, -0.5657, -0.5296]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5172,  0.5266,  0.5268],\n",
      "          [ 0.5178,  0.5244,  0.5209],\n",
      "          [ 0.5178,  0.5247,  0.5172]],\n",
      "\n",
      "         [[-0.5260, -0.5873, -0.7570],\n",
      "          [-0.5235, -0.5261, -0.5278],\n",
      "          [-0.5292, -0.5243, -0.5243]],\n",
      "\n",
      "         [[ 0.6522, -0.5178,  1.0000],\n",
      "          [-0.9924, -0.5320, -0.5176],\n",
      "          [ 0.5277, -0.5223,  0.5269]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9376, -0.9999,  0.6585],\n",
      "          [-0.7899,  0.7106,  0.6224],\n",
      "          [-0.7714,  0.5185,  0.9405]],\n",
      "\n",
      "         [[-0.7984,  0.7000, -0.5306],\n",
      "          [ 0.9177, -0.5974, -0.5597],\n",
      "          [-0.5233, -0.5228, -0.5303]],\n",
      "\n",
      "         [[-0.5777, -0.5363, -0.5367],\n",
      "          [-0.6891, -0.6999, -0.5358],\n",
      "          [-0.5580, -0.6769, -0.5272]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5237,  0.5277,  0.5172],\n",
      "          [-0.5247, -0.5246, -0.5327],\n",
      "          [-0.5254, -0.5326, -0.5149]],\n",
      "\n",
      "         [[-0.5173, -0.5191, -0.5324],\n",
      "          [-0.5310, -0.5237, -0.5311],\n",
      "          [-0.5195, -0.5234, -0.5306]],\n",
      "\n",
      "         [[-0.5224, -0.5188, -0.5160],\n",
      "          [-0.5256, -0.5326, -0.5308],\n",
      "          [-0.5173, -0.5269, -0.5150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5320,  0.5203,  0.5320],\n",
      "          [-0.5186, -0.5186, -0.6607],\n",
      "          [-0.5239, -0.5307, -0.5283]],\n",
      "\n",
      "         [[ 0.5303,  0.5148,  0.5324],\n",
      "          [ 0.7209,  0.5148,  0.5204],\n",
      "          [-0.9999, -0.5325, -0.5260]],\n",
      "\n",
      "         [[ 0.5280,  0.5270,  0.5324],\n",
      "          [ 0.5167,  0.5262,  0.5845],\n",
      "          [-0.5284, -0.5150, -0.5260]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5269,  0.5169, -0.5749],\n",
      "          [ 0.9999,  0.9924,  0.5159],\n",
      "          [-0.9373,  0.9341, -0.9690]],\n",
      "\n",
      "         [[ 0.5202,  0.5167,  0.5161],\n",
      "          [-0.5191, -0.9155, -0.5175],\n",
      "          [-0.5343,  0.8285, -0.6065]],\n",
      "\n",
      "         [[-0.5266, -0.5178, -0.5147],\n",
      "          [-0.5253, -0.5146, -0.5229],\n",
      "          [-0.5382, -0.5783,  0.9998]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6852, -0.5262, -0.5365],\n",
      "          [ 0.5238,  0.8895,  0.5141],\n",
      "          [ 0.5273,  0.8773,  0.5217]],\n",
      "\n",
      "         [[-0.5179,  0.5179,  0.5188],\n",
      "          [-0.5205,  0.5255,  0.5158],\n",
      "          [-0.5235,  0.5225,  0.5168]],\n",
      "\n",
      "         [[ 0.6724,  0.9987,  0.5203],\n",
      "          [-0.5181,  0.6729, -0.9489],\n",
      "          [-0.5274, -0.5165,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5153,  0.8522, -0.5361],\n",
      "          [ 0.5337,  0.7614,  0.9153],\n",
      "          [ 0.5742, -1.0000,  0.5178]],\n",
      "\n",
      "         [[-0.5167, -0.5140, -0.5235],\n",
      "          [-0.5224, -0.5207, -0.5217],\n",
      "          [-0.5236, -0.5168, -0.5274]],\n",
      "\n",
      "         [[-0.5281, -0.5166, -0.5200],\n",
      "          [-0.5376, -0.5875, -0.5177],\n",
      "          [-0.5300, -0.5233, -0.5205]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5156,  0.5157, -0.5338],\n",
      "          [-0.5285,  0.5171, -0.5250],\n",
      "          [-0.6640,  0.5156,  0.5222]],\n",
      "\n",
      "         [[-0.5261, -0.6231, -0.5577],\n",
      "          [ 0.5217,  0.5215, -0.5733],\n",
      "          [ 0.5165,  0.5198,  0.5159]],\n",
      "\n",
      "         [[ 0.5159, -0.5528,  0.5875],\n",
      "          [-0.5366, -0.5475, -0.9054],\n",
      "          [-0.7236, -0.9997, -0.9995]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5205,  0.5324,  0.5246],\n",
      "          [ 0.5282,  0.6429, -0.5168],\n",
      "          [-0.5311,  0.7714,  0.8270]],\n",
      "\n",
      "         [[-0.5164, -0.5296,  0.5810],\n",
      "          [-0.5167, -0.6686,  0.5817],\n",
      "          [-0.5159, -0.5842,  0.9995]],\n",
      "\n",
      "         [[-0.5848,  0.5166, -0.7084],\n",
      "          [-0.5718,  0.5325,  0.5501],\n",
      "          [-0.5284, -0.5564,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5286,  0.5203,  0.9986],\n",
      "          [-1.0000,  0.6691,  0.9991],\n",
      "          [ 0.9993, -0.6728, -0.5206]],\n",
      "\n",
      "         [[ 0.5198, -0.9999,  0.7848],\n",
      "          [ 0.7739, -0.5437, -0.5212],\n",
      "          [-0.6603, -0.5152, -0.5276]],\n",
      "\n",
      "         [[ 0.5170,  0.5312, -0.5258],\n",
      "          [-0.5192, -0.5377, -0.5268],\n",
      "          [-0.5160, -0.5147, -0.5239]]]], device='cuda:0')\n",
      "tensor([1.0000, 0.5123, 0.5906, 0.5279, 0.8988, 0.9563, 0.5155, 0.5288, 0.9908,\n",
      "        0.5596, 0.5154, 0.5149, 0.9684, 0.5349, 0.8455, 0.5230, 1.0000, 0.5158,\n",
      "        1.0000, 0.5570, 0.5800, 0.5416, 0.9711, 0.9264, 0.6319, 1.0000, 0.9953,\n",
      "        0.9539, 0.5398, 0.9932, 0.5880, 0.5176, 0.9916, 0.9982, 0.9941, 0.5278,\n",
      "        0.4987, 0.7362, 0.5116, 0.9789, 0.5293, 1.0000, 0.5086, 0.6158, 0.9183,\n",
      "        0.5324, 0.5115, 0.5905, 0.7101, 0.5192, 0.5967, 0.9808, 0.7037, 0.5508,\n",
      "        1.0000, 1.0000, 0.9749, 0.6771, 0.5088, 1.0000, 0.9729, 0.9990, 0.8632,\n",
      "        0.9532, 1.0000, 0.9949, 0.5170, 0.9334, 0.9770, 0.9972, 0.9981, 0.5060,\n",
      "        0.5592, 1.0000, 0.5253, 0.9874, 0.9998, 0.5190, 0.6564, 0.5137, 0.6945,\n",
      "        1.0000, 0.9698, 0.5244, 0.8805, 0.5833, 0.5765, 0.9624, 0.9846, 1.0000,\n",
      "        0.6609, 0.9747, 0.9890, 0.6047, 0.5745, 0.9692, 1.0000, 1.0000, 0.5210,\n",
      "        0.5172, 0.8887, 0.5045, 1.0000, 0.5550, 0.9631, 0.5262, 1.0000, 0.9044,\n",
      "        0.5154, 0.5675, 0.9986, 0.9854, 0.5244, 1.0000, 0.6238, 0.5225, 0.9979,\n",
      "        0.5304, 0.5711, 0.5268, 0.5231, 0.9303, 0.6549, 0.9883, 1.0000, 0.8898,\n",
      "        0.5226, 1.0000, 1.0000, 0.5438, 1.0000, 0.7904, 0.5149, 1.0000, 0.8081,\n",
      "        1.0000, 0.9927, 1.0000, 0.9446, 0.8123, 0.5362, 1.0000, 0.9509, 0.8470,\n",
      "        0.9950, 1.0000, 0.5095, 0.9340, 0.5184, 0.9985, 0.5925, 0.6591, 0.5569,\n",
      "        0.9679, 1.0000, 0.7976, 1.0000, 0.5804, 0.5330, 0.9411, 0.5268, 0.5264,\n",
      "        0.9910, 0.5451, 0.9815, 0.9885, 0.9962, 0.6049, 0.6349, 1.0000, 0.9037,\n",
      "        0.9470, 0.5154, 0.5171, 0.9808, 0.6983, 0.9945, 1.0000, 1.0000, 0.6220,\n",
      "        0.9919, 0.5437, 0.9795, 0.9855, 0.5204, 0.4955, 0.5815, 0.5423, 0.9172,\n",
      "        0.5268, 0.5100, 0.5290, 0.9894, 0.6154, 0.9927, 0.5413, 0.6259, 1.0000,\n",
      "        0.9807, 0.9985, 0.9732, 0.5236, 0.9929, 0.9813, 0.9568, 0.5300, 1.0000,\n",
      "        1.0000, 0.5826, 0.6511, 1.0000, 0.9973, 0.9876, 0.9561, 0.5243, 0.5243,\n",
      "        0.5148, 0.8587, 0.5278, 0.9742, 0.9751, 0.9955, 0.5248, 0.5070, 0.5230,\n",
      "        1.0000, 0.9853, 0.5159, 0.5417, 1.0000, 0.7269, 0.5352, 0.9043, 0.9798,\n",
      "        0.5455, 0.6142, 0.7826, 0.5135, 1.0000, 0.5348, 0.6034, 0.6012, 0.6274,\n",
      "        0.9343, 0.9978, 0.5251, 0.9378, 1.0000, 0.8409, 0.9975, 0.5179, 0.8919,\n",
      "        0.9998, 1.0000, 0.9464, 0.5235], device='cuda:0')\n",
      "tensor([-0.5159, -1.0000, -0.7682, -0.7269, -0.6305, -0.5461, -0.9456, -0.5307,\n",
      "        -0.5320, -0.9979, -1.0000, -0.8257, -0.5211, -0.6918, -0.5223, -1.0000,\n",
      "        -0.5293, -1.0000, -0.5392,  0.5151,  1.0000, -0.9988, -0.5710, -0.5658,\n",
      "         0.5176, -0.5010, -0.5564, -0.5743,  0.7259,  0.6576, -0.8458,  0.5164,\n",
      "        -0.5407, -0.5233, -0.5692, -1.0000, -0.7725,  0.5129, -1.0000, -0.5202,\n",
      "        -0.9973, -0.5212, -1.0000, -0.6016,  0.5639, -0.9848, -1.0000, -0.9205,\n",
      "         0.4991, -0.9150, -0.5537, -0.5321,  0.7778, -0.5455, -0.5164, -0.5061,\n",
      "        -0.5742, -0.5340, -0.5311, -0.5030, -0.5245, -0.5442, -0.6160, -0.5892,\n",
      "        -0.5191, -0.5169, -0.8200, -0.6229, -0.5977, -0.5126, -0.5320, -0.7879,\n",
      "        -0.7933, -0.5237, -0.5169, -0.5517, -0.5340,  0.5122, -0.6684,  0.6154,\n",
      "         0.5240, -0.5228, -0.5677,  0.5134, -0.6491, -0.5207, -0.9981, -0.6085,\n",
      "        -0.5516, -0.5286,  0.5270, -0.5179,  0.5258, -0.9935, -0.8742, -0.5373,\n",
      "        -0.5197, -0.5176, -0.9455,  0.5116, -0.5713, -1.0000, -0.5040, -0.6898,\n",
      "        -0.5799, -0.9948, -0.5286,  0.5745, -0.5727,  0.5879, -0.5175,  0.6046,\n",
      "        -0.8237, -0.5132, -0.8308, -0.9649, -0.5907, -0.9931,  0.5295, -0.9919,\n",
      "        -0.8451, -0.5231, -0.8177,  0.5418, -0.5314, -0.5785, -0.9892, -0.5246,\n",
      "         0.5452,  0.5237, -0.5645, -0.6774, -1.0000, -0.5271, -0.9459, -0.5188,\n",
      "        -0.5442,  1.0000,  0.5218, -0.6580,  1.0000, -0.5199, -0.5667, -0.5340,\n",
      "        -0.8575, -0.5279, -0.9895, -0.5772,  0.5163, -0.5188,  0.5069, -0.6412,\n",
      "        -0.7237, -0.5543, -0.5049, -0.7157, -0.5348, -0.5454,  0.5162, -0.6239,\n",
      "        -0.9935, -0.9999, -0.5428, -1.0000, -0.5260, -0.5372, -0.5532,  0.5361,\n",
      "        -0.6291, -0.5486, -0.5963,  0.5111, -1.0000, -0.9381,  0.5753, -0.8870,\n",
      "         0.6112, -0.5276, -0.5285, -0.7053, -0.5301, -1.0000,  0.5303,  0.5077,\n",
      "        -1.0000, -1.0000, -0.9984, -1.0000, -0.6146,  0.5190, -1.0000, -0.7255,\n",
      "        -0.8768, -0.6446, -0.5127, -0.8690, -0.9985, -0.5183, -0.5548, -0.5190,\n",
      "         0.5169, -1.0000, -0.5290, -0.6026, -0.9259, -0.9943, -0.5551, -0.5629,\n",
      "        -0.6809, -0.7068, -0.5533, -0.5229, -0.5621,  0.5186, -0.5921, -1.0000,\n",
      "        -0.7499, -0.6195, -0.9997, -0.5878, -0.5226, -0.5329, -0.6661,  0.5211,\n",
      "        -0.9970, -0.5254, -0.5479, -1.0000, -0.5559, -0.5087,  0.5414,  0.5143,\n",
      "        -0.5203, -0.5546,  0.5140,  0.5193,  0.4968,  0.5184, -0.4975, -0.9892,\n",
      "        -0.8614, -0.9932,  0.8830,  0.6162,  0.5300, -0.5565,  0.5863, -0.5223,\n",
      "        -0.7170, -0.5368, -0.8728, -0.6907, -0.5342,  0.6191,  0.5248, -0.8087],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.5153, -0.5198,  0.9996],\n",
      "          [-0.5151, -0.5308, -0.5259],\n",
      "          [-0.5147, -0.7584, -0.5929]],\n",
      "\n",
      "         [[-0.5195, -0.6099, -0.8239],\n",
      "          [-0.5151, -0.5326, -0.5326],\n",
      "          [-0.5324, -0.5324, -0.5163]],\n",
      "\n",
      "         [[-0.9322, -0.5213, -0.5181],\n",
      "          [-0.5790, -0.9998, -0.5173],\n",
      "          [-0.5187, -0.5242, -0.5233]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6139, -0.6639, -0.5948],\n",
      "          [-0.5266,  0.8284, -0.5318],\n",
      "          [-0.5244,  0.9153,  0.5756]],\n",
      "\n",
      "         [[ 0.6940,  0.5242,  0.7669],\n",
      "          [-0.5296, -0.8423, -0.5264],\n",
      "          [-0.5242,  0.5378, -0.9285]],\n",
      "\n",
      "         [[-0.5197, -0.5575, -0.5190],\n",
      "          [-0.5912, -1.0000, -0.5194],\n",
      "          [-0.5326, -0.5232, -0.5326]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000,  0.9668,  0.5249],\n",
      "          [-0.7727, -0.5181,  0.6509],\n",
      "          [-0.5297, -0.5261,  0.9994]],\n",
      "\n",
      "         [[ 0.5326,  0.5148,  0.5303],\n",
      "          [ 0.5217,  0.5177,  0.5303],\n",
      "          [ 0.5232,  0.5167,  0.5156]],\n",
      "\n",
      "         [[ 0.5154,  0.5301,  0.5148],\n",
      "          [ 0.5152,  0.5188,  0.5275],\n",
      "          [ 0.5293,  0.5325,  0.5325]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5227,  1.0000, -0.5239],\n",
      "          [ 0.5235,  0.5534,  0.5211],\n",
      "          [ 0.5287,  0.5245,  0.5295]],\n",
      "\n",
      "         [[ 0.5323,  0.5175,  0.5241],\n",
      "          [ 0.5159,  0.5209,  0.5173],\n",
      "          [ 0.8427,  0.9994,  0.9754]],\n",
      "\n",
      "         [[ 0.5287,  0.5147,  0.5155],\n",
      "          [ 0.5324,  0.5148,  0.5245],\n",
      "          [ 0.5216,  0.5277,  0.5253]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5187,  0.5233,  0.5317],\n",
      "          [ 0.5291,  0.5325,  0.5292],\n",
      "          [ 0.5158,  0.5661,  0.7800]],\n",
      "\n",
      "         [[ 0.5249,  0.5273,  0.5154],\n",
      "          [ 0.5213,  0.5217,  0.5176],\n",
      "          [ 0.5281,  0.5361,  0.5247]],\n",
      "\n",
      "         [[ 0.5193,  0.5188,  0.5228],\n",
      "          [ 0.5324,  0.5326,  0.5639],\n",
      "          [ 0.5242,  0.7045, -0.7640]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5272, -0.5884, -0.5667],\n",
      "          [-0.8022, -0.5644,  0.6068],\n",
      "          [-0.8127,  0.9974, -0.5170]],\n",
      "\n",
      "         [[-0.5150, -0.5211,  0.9999],\n",
      "          [ 0.9998, -0.5772, -0.5530],\n",
      "          [ 0.5310, -0.6317, -1.0000]],\n",
      "\n",
      "         [[ 0.5149,  0.5184,  0.5246],\n",
      "          [ 0.5159,  0.5326,  0.5236],\n",
      "          [ 0.5274,  0.5175,  0.5307]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5324, -0.5158,  0.5249],\n",
      "          [ 0.7466, -0.8011, -0.6277],\n",
      "          [ 0.5863,  0.6472,  0.5771]],\n",
      "\n",
      "         [[ 0.5564,  0.5372, -0.9995],\n",
      "          [ 0.5280,  0.6230, -0.5887],\n",
      "          [ 0.5170, -0.9938,  0.5181]],\n",
      "\n",
      "         [[-1.0000, -0.5324, -0.9998],\n",
      "          [-0.5210, -0.5294,  0.9989],\n",
      "          [ 0.6276, -0.5302, -0.5282]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5292,  0.5350,  0.5307],\n",
      "          [-0.5289,  0.5623,  0.5351],\n",
      "          [-0.5191, -0.6305,  0.5313]],\n",
      "\n",
      "         [[ 0.5486,  0.5356,  0.7364],\n",
      "          [ 0.5897,  0.8090,  0.9974],\n",
      "          [ 0.5200,  0.7010,  0.6146]],\n",
      "\n",
      "         [[-0.5541, -1.0000, -0.9547],\n",
      "          [ 0.5325,  0.8339,  1.0000],\n",
      "          [ 0.5250, -0.5160, -0.5217]]],\n",
      "\n",
      "\n",
      "        [[[-0.5155,  0.6738, -0.5257],\n",
      "          [-0.5181, -0.5341, -0.5322],\n",
      "          [-0.5268, -0.5195, -0.5274]],\n",
      "\n",
      "         [[ 0.5164, -0.8136,  0.5270],\n",
      "          [-0.5254, -0.5259, -0.5294],\n",
      "          [-0.5323, -0.5197, -0.5334]],\n",
      "\n",
      "         [[ 0.5164,  0.5209,  0.5149],\n",
      "          [ 0.6060,  0.5324,  0.5327],\n",
      "          [-0.5312,  0.9999,  0.5277]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9999,  0.9936,  0.5433],\n",
      "          [ 0.5210,  0.5183,  0.5189],\n",
      "          [ 0.6764,  0.5219,  0.5484]],\n",
      "\n",
      "         [[-0.6416, -0.5341, -0.8998],\n",
      "          [-0.5888, -0.5338,  0.6046],\n",
      "          [-0.5179, -0.5188,  0.9169]],\n",
      "\n",
      "         [[ 0.5306,  0.5194,  0.5156],\n",
      "          [ 0.6241, -0.5263,  0.5873],\n",
      "          [-0.7095, -0.5166,  0.5202]]],\n",
      "\n",
      "\n",
      "        [[[-0.9004,  0.5265,  0.5372],\n",
      "          [ 0.5152, -0.8555,  0.9508],\n",
      "          [-0.9989,  0.5324, -0.9993]],\n",
      "\n",
      "         [[ 0.5736,  0.5259,  0.5277],\n",
      "          [ 0.5366,  0.5210,  0.5236],\n",
      "          [-0.6894,  0.5179,  0.5276]],\n",
      "\n",
      "         [[ 0.5147,  0.5148,  0.5203],\n",
      "          [ 0.5269,  0.5278,  0.5324],\n",
      "          [ 0.5216,  0.5148,  0.5326]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5230, -0.9700, -0.5624],\n",
      "          [ 0.5246,  0.7252, -0.5268],\n",
      "          [ 0.5202, -0.9860,  0.9691]],\n",
      "\n",
      "         [[-0.5214,  0.6176, -0.5300],\n",
      "          [-0.5153, -0.5257, -0.5304],\n",
      "          [-0.5237, -0.6862, -0.5239]],\n",
      "\n",
      "         [[ 0.5326,  0.5213,  0.5324],\n",
      "          [ 0.5414,  0.5184,  0.5150],\n",
      "          [ 0.5223,  0.5283,  0.5418]]]], device='cuda:0')\n",
      "tensor([ 0.9584,  0.5263,  0.5034,  1.0000,  0.9393,  0.5200,  1.0000,  1.0000,\n",
      "         0.9677,  1.0000,  0.5194,  0.9575,  0.5157,  1.0000,  0.5181,  0.5157,\n",
      "         0.5206,  0.7736,  0.5321,  1.0000,  0.5310,  0.9994,  1.0000,  0.9832,\n",
      "         0.9828,  0.9502,  0.9501,  0.9570,  0.9670,  1.0000,  1.0000,  1.0000,\n",
      "         0.5387,  0.5142,  0.9396,  0.9669,  0.5449,  1.0000,  0.5210,  0.5557,\n",
      "         0.9928,  0.5897,  0.9720,  0.9710,  0.9612,  0.5453,  0.9848,  0.9305,\n",
      "         0.9980,  0.9905,  1.0000,  0.5173,  0.5692,  1.0000,  0.9870,  0.9394,\n",
      "         1.0000,  0.5448,  0.5307,  0.5260,  0.9954,  1.0000, -0.5106,  1.0000,\n",
      "         1.0000,  0.5226,  0.9598,  0.7899,  0.8348,  0.5772,  0.9993,  0.9693,\n",
      "         0.7274,  1.0000,  0.9759,  0.5704,  0.9413,  0.9877,  0.9337,  0.9950,\n",
      "         0.9664,  1.0000,  0.5251,  0.5145,  0.9788,  0.5170,  0.9484,  0.6384,\n",
      "         0.8344,  0.9965,  0.5268,  0.5281,  0.5668,  0.9940,  0.9885,  0.9921,\n",
      "         0.9741,  0.5520,  1.0000,  0.5144,  0.9916,  1.0000,  1.0000,  0.9845,\n",
      "         0.5590,  0.5185,  0.9676,  0.5935,  0.5266,  0.6137,  0.5172,  1.0000,\n",
      "         1.0000,  0.8510,  0.5163,  0.9012,  0.5143,  1.0000,  0.9253,  0.5247,\n",
      "         0.5092,  0.5728,  0.5167,  0.9906,  1.0000,  0.5037,  0.9925,  0.9571,\n",
      "         1.0000,  0.6290,  0.9997,  0.7332,  0.8607,  0.9572,  0.9727,  0.9644,\n",
      "         0.5811,  0.9838,  1.0000,  1.0000,  0.5954,  0.5580,  0.5556,  0.5226,\n",
      "        -0.9601,  0.9995,  0.5378,  0.8564,  0.5131,  0.6155,  0.5381,  1.0000,\n",
      "         0.5259,  0.5169,  0.6541,  0.5280,  1.0000,  1.0000,  0.5164,  0.5540,\n",
      "         0.5814,  0.5682,  0.5130,  0.9985,  0.5439,  1.0000,  0.5243,  0.5319,\n",
      "         0.9279,  0.5394,  0.6254,  0.5146,  0.9423,  0.9830,  0.7913,  0.5120,\n",
      "         0.5668,  0.9121,  0.6285,  0.9874,  0.9998,  0.7466,  0.5531,  1.0000,\n",
      "         1.0000,  0.8555,  0.5395,  1.0000,  0.5200,  1.0000,  0.9223,  0.9532,\n",
      "         1.0000,  0.5176,  0.9912,  0.9577,  0.5561,  0.5104,  0.9198,  0.5811,\n",
      "         0.7468,  0.5186,  0.9004,  1.0000,  0.5160,  0.9862,  0.5213,  1.0000,\n",
      "         0.5440,  0.8641,  0.5132,  1.0000,  0.9755,  0.9664,  0.5253,  0.5403,\n",
      "         0.5205,  0.8022,  0.8856,  1.0000,  0.6905,  0.8908,  1.0000,  1.0000,\n",
      "         0.5313,  0.5635,  1.0000,  0.7615,  1.0000,  0.5655,  0.7510,  0.5241,\n",
      "         0.5184,  0.9881,  1.0000,  1.0000,  1.0000,  0.5190,  0.9996,  0.5367,\n",
      "         0.9838,  0.9385,  0.9990,  0.9333,  0.6443,  0.9065,  0.9837,  0.5203,\n",
      "         0.9964,  0.5460,  0.9927,  0.9818,  1.0000,  0.9875,  1.0000,  0.5607],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.5763, -0.9882, -0.7008, -0.5125,  0.5357, -0.9030, -0.5140,  0.5482,\n",
      "        -0.5152, -0.5438, -0.7718, -0.5373, -0.5450,  0.7908, -0.6746, -0.5284,\n",
      "        -0.9954, -0.5413, -0.5152,  0.5187, -0.6202, -0.5113, -0.5206, -0.5273,\n",
      "         0.5114, -0.5534, -0.6673, -0.5949, -0.5207, -0.5106,  0.5257, -0.5184,\n",
      "        -0.5185, -0.9999, -0.5606, -0.5199, -0.9968, -0.5230, -1.0000, -0.5142,\n",
      "        -0.5298, -0.5237, -0.5564,  0.5014, -0.5425,  0.7421, -1.0000, -0.9304,\n",
      "        -0.5197, -0.5244, -0.5141, -0.5197, -0.8530,  0.5172,  0.5376, -0.5549,\n",
      "        -0.5558, -0.6365, -0.7902, -0.7863, -0.5281,  0.5114, -0.8499, -0.5153,\n",
      "        -0.5199, -0.5930, -0.5133, -0.9939, -0.5402, -0.5917, -0.5223,  0.5079,\n",
      "         0.5224, -0.5354, -0.5238,  0.7168, -0.5209, -0.5178, -0.5407, -0.5342,\n",
      "        -0.5386, -0.5110, -0.9666, -0.9528, -0.8022, -0.8988, -0.5530, -0.9927,\n",
      "        -0.9166, -0.5294, -0.5944, -0.9990, -0.6024, -0.5299, -0.5254,  0.5142,\n",
      "         0.5221, -0.5183, -0.5085, -1.0000, -0.5336, -0.5137,  0.9729, -0.5509,\n",
      "        -0.5173, -0.5244,  0.5155, -0.5321,  0.5139,  0.5253,  0.5227, -0.5255,\n",
      "         0.5081, -0.7413, -0.7767, -0.9982, -0.9998, -0.5103, -0.5689, -0.9990,\n",
      "        -0.5199, -0.8184, -0.8186,  0.5613, -0.5487,  0.5204, -0.5199, -0.5714,\n",
      "        -0.5167, -0.8672, -0.5319, -0.5213, -0.6764, -0.5433,  0.5332, -0.9806,\n",
      "         0.6068, -0.5601, -0.5316, -0.5164, -0.5871,  0.5260, -0.5247, -0.5264,\n",
      "         0.5495, -0.5238, -0.5544, -0.5940, -0.5144, -0.6059, -0.9995,  0.5202,\n",
      "        -0.9995, -0.7950, -0.9500, -0.7193, -0.5027, -0.5153, -0.7549, -0.5116,\n",
      "        -0.6863, -0.8594, -0.5251, -0.5310, -0.8242, -0.5153, -1.0000, -0.9996,\n",
      "        -0.9945, -0.7511,  0.5121, -0.9587, -0.5468, -0.5419, -0.5511, -0.5173,\n",
      "        -0.9584, -0.6681, -0.5146, -0.5258, -0.5292, -0.9679, -0.9842,  0.5315,\n",
      "        -0.5146, -0.5933, -0.7780, -0.5153, -0.7564, -0.5216, -0.5650, -0.7843,\n",
      "        -0.5133, -0.5201, -0.5299, -0.5428, -0.8019, -0.9976, -0.5674, -0.5130,\n",
      "        -0.5425, -1.0000, -0.7157, -0.5153, -0.5880,  0.5367, -0.7045,  0.5091,\n",
      "        -0.9347,  0.6110, -1.0000, -0.5122, -0.5366, -0.5438, -0.8100, -0.5999,\n",
      "        -0.8707, -0.5517,  0.5222,  0.5563, -0.8447, -0.9930, -0.5421, -0.5222,\n",
      "        -0.5180,  0.5186, -0.5087, -0.5890, -0.5303, -0.5884, -1.0000, -1.0000,\n",
      "        -0.8924, -0.5355, -0.5494, -0.5110, -0.5126, -0.9999, -0.5468, -0.9423,\n",
      "         0.5205,  0.5246, -0.5136,  0.9907, -0.6128, -1.0000, -0.5345, -0.7915,\n",
      "        -0.5256, -0.9967,  0.5204, -0.5978, -0.5179, -0.5269, -0.5118, -0.5466],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.5326]],\n",
      "\n",
      "         [[-0.5295]],\n",
      "\n",
      "         [[-0.5193]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5235]],\n",
      "\n",
      "         [[-0.5350]],\n",
      "\n",
      "         [[ 0.5154]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5219]],\n",
      "\n",
      "         [[ 0.5215]],\n",
      "\n",
      "         [[ 0.5170]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5189]],\n",
      "\n",
      "         [[ 0.8571]],\n",
      "\n",
      "         [[-0.6577]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5141]],\n",
      "\n",
      "         [[-0.5449]],\n",
      "\n",
      "         [[-0.5212]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5634]],\n",
      "\n",
      "         [[ 0.5151]],\n",
      "\n",
      "         [[ 0.5160]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5301]],\n",
      "\n",
      "         [[-0.5271]],\n",
      "\n",
      "         [[-0.5179]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5213]],\n",
      "\n",
      "         [[-0.5293]],\n",
      "\n",
      "         [[-0.5297]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5191]],\n",
      "\n",
      "         [[ 0.5216]],\n",
      "\n",
      "         [[-0.5323]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5244]],\n",
      "\n",
      "         [[-0.5209]],\n",
      "\n",
      "         [[ 0.5255]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8017]],\n",
      "\n",
      "         [[ 0.5296]],\n",
      "\n",
      "         [[-0.5205]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5274]],\n",
      "\n",
      "         [[-0.5255]],\n",
      "\n",
      "         [[-0.5175]]]], device='cuda:0')\n",
      "tensor([0.5093, 0.9906, 0.6891, 0.9817, 0.5155, 0.5153, 0.7958, 0.6326, 0.9985,\n",
      "        0.8995, 0.5193, 0.9928, 0.5486, 0.5414, 0.9946, 0.5181, 0.5860, 0.8553,\n",
      "        0.6885, 0.8370, 0.9965, 1.0000, 0.9977, 0.9967, 0.6199, 0.7862, 0.5263,\n",
      "        0.8542, 0.9936, 0.7348, 0.9184, 0.8355, 0.5058, 0.5268, 0.5129, 0.6608,\n",
      "        0.5437, 0.9712, 0.7867, 0.9957, 0.9976, 0.9972, 0.5148, 0.6261, 0.9915,\n",
      "        0.6600, 0.5613, 0.5489, 0.9679, 0.5834, 0.9997, 0.5667, 0.5131, 0.6398,\n",
      "        0.5931, 0.9776, 0.8745, 0.5136, 0.9975, 0.9994, 0.6931, 0.6530, 0.6759,\n",
      "        0.9975, 1.0000, 0.7757, 0.9988, 0.5160, 0.9862, 0.7154, 0.5062, 0.5411,\n",
      "        0.7150, 0.9842, 0.9568, 0.5277, 0.9988, 0.9443, 0.5958, 0.5528, 0.9925,\n",
      "        1.0000, 0.6835, 0.6452, 0.5184, 0.5733, 0.8289, 0.5168, 0.5165, 0.9508,\n",
      "        0.8944, 0.9988, 0.5339, 0.5415, 0.5324, 0.5363, 0.5188, 0.5171, 0.6741,\n",
      "        0.5173, 0.6409, 0.6578, 0.5351, 0.5181, 0.5681, 0.5368, 0.6256, 0.8741,\n",
      "        0.5684, 0.5256, 0.6014, 0.5167, 0.5414, 0.5151, 0.5155, 0.6007, 1.0000,\n",
      "        0.6871, 0.9999, 1.0000, 0.5128, 0.5155, 1.0000, 0.5386, 0.6132, 0.5312,\n",
      "        0.9987, 0.9790, 0.8942, 0.5145, 0.9992, 0.9987, 0.5803, 0.5388, 0.5198,\n",
      "        0.8645, 0.6587, 0.5207, 1.0000, 0.5483, 0.5458, 0.6313, 0.5722, 0.9998,\n",
      "        0.5155, 0.9916, 0.9515, 0.5135, 0.5294, 0.9978, 0.9445, 0.5452, 0.9970,\n",
      "        0.7253, 0.5183, 0.5681, 1.0000, 1.0000, 0.9077, 1.0000, 0.9948, 0.5078,\n",
      "        0.5178, 0.7921, 0.5323, 1.0000, 0.9691, 1.0000, 0.5156, 0.5152, 0.5379,\n",
      "        0.6808, 0.9785, 0.7786, 0.7685, 0.5406, 0.5145, 0.5208, 0.9992, 0.9976,\n",
      "        0.5981, 0.5143, 1.0000, 0.5755, 0.9985, 0.6052, 0.5233, 1.0000, 0.5348,\n",
      "        1.0000, 0.5916, 0.5194, 1.0000, 0.5172, 0.9751, 0.8518, 0.9364, 0.8355,\n",
      "        0.9905, 1.0000, 0.7526, 0.5153, 0.5308, 0.9997, 0.5148, 0.6481, 0.8628,\n",
      "        0.5439, 0.7040, 0.7246, 0.5157, 0.8863, 0.5935, 0.6121, 1.0000, 0.5234,\n",
      "        0.5222, 0.9646, 0.6601, 0.5264, 0.5154, 0.8948, 0.7447, 0.9449, 0.9991,\n",
      "        0.9738, 0.9848, 0.6866, 0.9992, 0.5285, 0.5144, 0.5171, 0.5521, 0.9966,\n",
      "        0.9932, 0.6845, 0.9247, 0.5194, 0.9435, 0.7693, 0.5544, 0.5274, 0.9999,\n",
      "        0.5645, 0.7754, 0.5261, 0.9916, 0.5223, 0.9768, 0.5244, 0.6307, 0.6395,\n",
      "        0.5630, 0.9991, 0.9849, 0.9969], device='cuda:0')\n",
      "tensor([ 0.5763, -0.9882, -0.7008, -0.5125,  0.5357, -0.9030, -0.5140,  0.5482,\n",
      "        -0.5152, -0.5438, -0.7718, -0.5373, -0.5450,  0.7908, -0.6746, -0.5284,\n",
      "        -0.9954, -0.5413, -0.5152,  0.5187, -0.6202, -0.5113, -0.5206, -0.5273,\n",
      "         0.5114, -0.5534, -0.6673, -0.5949, -0.5207, -0.5106,  0.5257, -0.5184,\n",
      "        -0.5185, -0.9999, -0.5606, -0.5199, -0.9968, -0.5230, -1.0000, -0.5142,\n",
      "        -0.5298, -0.5237, -0.5564,  0.5014, -0.5425,  0.7421, -1.0000, -0.9304,\n",
      "        -0.5197, -0.5244, -0.5141, -0.5197, -0.8530,  0.5172,  0.5376, -0.5549,\n",
      "        -0.5558, -0.6365, -0.7902, -0.7863, -0.5281,  0.5114, -0.8499, -0.5153,\n",
      "        -0.5199, -0.5930, -0.5133, -0.9939, -0.5402, -0.5917, -0.5223,  0.5079,\n",
      "         0.5224, -0.5354, -0.5238,  0.7168, -0.5209, -0.5178, -0.5407, -0.5342,\n",
      "        -0.5386, -0.5110, -0.9666, -0.9528, -0.8022, -0.8988, -0.5530, -0.9927,\n",
      "        -0.9166, -0.5294, -0.5944, -0.9990, -0.6024, -0.5299, -0.5254,  0.5142,\n",
      "         0.5221, -0.5183, -0.5085, -1.0000, -0.5336, -0.5137,  0.9729, -0.5509,\n",
      "        -0.5173, -0.5244,  0.5155, -0.5321,  0.5139,  0.5253,  0.5227, -0.5255,\n",
      "         0.5081, -0.7413, -0.7767, -0.9982, -0.9998, -0.5103, -0.5689, -0.9990,\n",
      "        -0.5199, -0.8184, -0.8186,  0.5613, -0.5487,  0.5204, -0.5199, -0.5714,\n",
      "        -0.5167, -0.8672, -0.5319, -0.5213, -0.6764, -0.5433,  0.5332, -0.9806,\n",
      "         0.6068, -0.5601, -0.5316, -0.5164, -0.5871,  0.5260, -0.5247, -0.5264,\n",
      "         0.5495, -0.5238, -0.5544, -0.5940, -0.5144, -0.6059, -0.9995,  0.5202,\n",
      "        -0.9995, -0.7950, -0.9500, -0.7193, -0.5027, -0.5153, -0.7549, -0.5116,\n",
      "        -0.6863, -0.8594, -0.5251, -0.5310, -0.8242, -0.5153, -1.0000, -0.9996,\n",
      "        -0.9945, -0.7511,  0.5121, -0.9587, -0.5468, -0.5419, -0.5511, -0.5173,\n",
      "        -0.9584, -0.6681, -0.5146, -0.5258, -0.5292, -0.9679, -0.9842,  0.5315,\n",
      "        -0.5146, -0.5933, -0.7780, -0.5153, -0.7564, -0.5216, -0.5650, -0.7843,\n",
      "        -0.5133, -0.5201, -0.5299, -0.5428, -0.8019, -0.9976, -0.5674, -0.5130,\n",
      "        -0.5425, -1.0000, -0.7157, -0.5153, -0.5880,  0.5367, -0.7045,  0.5091,\n",
      "        -0.9347,  0.6110, -1.0000, -0.5122, -0.5366, -0.5438, -0.8100, -0.5999,\n",
      "        -0.8707, -0.5517,  0.5222,  0.5563, -0.8447, -0.9930, -0.5421, -0.5222,\n",
      "        -0.5180,  0.5186, -0.5087, -0.5890, -0.5303, -0.5884, -1.0000, -1.0000,\n",
      "        -0.8924, -0.5355, -0.5494, -0.5110, -0.5126, -0.9999, -0.5468, -0.9423,\n",
      "         0.5205,  0.5246, -0.5136,  0.9907, -0.6128, -1.0000, -0.5345, -0.7915,\n",
      "        -0.5256, -0.9967,  0.5204, -0.5978, -0.5179, -0.5269, -0.5118, -0.5466],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.9805, -0.8804, -0.6434],\n",
      "          [ 0.9805, -0.9881, -0.9998],\n",
      "          [ 0.9736,  0.5696, -0.5210]],\n",
      "\n",
      "         [[-0.5204, -0.5175, -0.5260],\n",
      "          [-0.7162, -0.5326, -0.5318],\n",
      "          [-0.5294, -0.5281, -0.5311]],\n",
      "\n",
      "         [[-0.5236, -1.0000, -0.5198],\n",
      "          [-0.9992, -0.5272, -0.5327],\n",
      "          [-0.9989, -0.6658, -0.8397]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5149,  0.5165, -0.5194],\n",
      "          [-0.9839,  0.5166, -0.6041],\n",
      "          [ 0.5280,  0.5168, -0.5295]],\n",
      "\n",
      "         [[ 0.8469,  0.9987, -0.6099],\n",
      "          [-0.9980,  0.9953, -0.5282],\n",
      "          [ 0.8546, -0.5220, -0.5470]],\n",
      "\n",
      "         [[-1.0000,  0.7156, -0.9879],\n",
      "          [-0.6407, -1.0000, -0.5314],\n",
      "          [-0.5324, -0.7016, -0.5325]]],\n",
      "\n",
      "\n",
      "        [[[-0.5253,  0.5748, -0.5824],\n",
      "          [ 0.5831,  0.5312,  0.5529],\n",
      "          [-0.7859,  0.5253,  0.9475]],\n",
      "\n",
      "         [[ 0.5285,  0.5527,  0.5182],\n",
      "          [-0.5167,  0.9986,  0.5229],\n",
      "          [-0.5227, -0.9958,  0.5178]],\n",
      "\n",
      "         [[ 0.5318,  0.9090,  0.5991],\n",
      "          [ 0.5796,  0.5263,  0.7145],\n",
      "          [-0.5183, -0.5287, -0.6351]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5152, -0.5162, -0.5238],\n",
      "          [-0.5299, -0.5163, -0.5562],\n",
      "          [-0.5192, -0.5305, -0.5148]],\n",
      "\n",
      "         [[ 0.5274,  0.5211,  0.6166],\n",
      "          [-0.5949, -0.6140, -0.5314],\n",
      "          [-0.5381, -0.5264, -0.5279]],\n",
      "\n",
      "         [[ 0.5532,  0.5398,  0.5313],\n",
      "          [ 0.5278,  0.5240,  0.5855],\n",
      "          [-0.5409,  0.5228,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.5201,  0.8671, -0.5287],\n",
      "          [-0.5337, -0.5880, -0.5768],\n",
      "          [-0.5193, -0.5277, -0.6210]],\n",
      "\n",
      "         [[ 0.8352, -1.0000, -0.5297],\n",
      "          [-0.5325, -0.5182, -0.9997],\n",
      "          [-0.7016, -0.5325, -0.5293]],\n",
      "\n",
      "         [[ 0.5226,  0.7835,  0.9525],\n",
      "          [ 0.5156,  0.5255,  0.5195],\n",
      "          [ 0.8447,  0.9968,  0.6064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5216,  0.5716, -0.5223],\n",
      "          [ 0.5281,  0.6025, -0.5279],\n",
      "          [ 0.5177,  0.5149, -0.5306]],\n",
      "\n",
      "         [[ 0.9606,  0.5598,  0.5152],\n",
      "          [ 0.5315,  0.5275,  0.8117],\n",
      "          [-0.5865, -0.6516, -0.5155]],\n",
      "\n",
      "         [[ 0.5587, -0.7450,  0.5210],\n",
      "          [-0.6368,  0.9667,  0.9976],\n",
      "          [-0.5204, -0.5215, -0.5577]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5184,  0.6835, -0.6388],\n",
      "          [-0.5261, -0.9962, -0.6182],\n",
      "          [-0.5289, -0.5265, -0.5291]],\n",
      "\n",
      "         [[-0.5281, -0.5235, -0.5148],\n",
      "          [-0.5176, -0.5149, -0.5321],\n",
      "          [-0.5326, -0.5326, -0.5149]],\n",
      "\n",
      "         [[ 0.5299,  0.5255,  0.9990],\n",
      "          [ 0.5162,  0.6973, -0.8922],\n",
      "          [ 0.5216,  0.5465,  0.5210]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5307, -0.6402, -0.5163],\n",
      "          [-0.5323, -0.5148, -0.5174],\n",
      "          [-0.5204, -0.5195, -0.5312]],\n",
      "\n",
      "         [[-0.5301, -0.5209, -0.5233],\n",
      "          [-0.5205, -0.5324, -0.5298],\n",
      "          [-0.5295, -0.5326, -0.5299]],\n",
      "\n",
      "         [[-1.0000, -0.5325, -0.5866],\n",
      "          [-0.6277, -0.9996,  0.6052],\n",
      "          [-0.5285, -0.5148, -0.5269]]],\n",
      "\n",
      "\n",
      "        [[[-0.5242,  1.0000, -0.5267],\n",
      "          [-0.5274,  0.5585,  0.9983],\n",
      "          [-0.5159, -0.5176, -0.5289]],\n",
      "\n",
      "         [[ 0.9999, -0.5277, -0.5189],\n",
      "          [-0.5207, -0.5149, -0.5148],\n",
      "          [-0.5275, -0.5323, -0.5160]],\n",
      "\n",
      "         [[ 0.5299,  0.5257,  0.5238],\n",
      "          [ 0.5155,  0.5249,  0.5328],\n",
      "          [-0.5561, -0.9998, -0.9994]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5148, -0.5259, -0.5264],\n",
      "          [-0.7613, -0.5276, -0.5148],\n",
      "          [-0.5163, -0.5323, -0.5244]],\n",
      "\n",
      "         [[-0.5321,  0.9957,  0.6740],\n",
      "          [-0.5254, -0.5229, -0.5470],\n",
      "          [-0.5196, -0.5312, -0.7106]],\n",
      "\n",
      "         [[ 0.5161, -0.5232,  0.5288],\n",
      "          [-0.9995, -0.5953, -0.5232],\n",
      "          [-0.5148, -0.5296,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.7596, -0.5250, -0.5208],\n",
      "          [ 0.5363,  0.5290,  0.5859],\n",
      "          [ 0.5186,  0.5290,  0.7140]],\n",
      "\n",
      "         [[ 0.5150,  0.5217,  0.5322],\n",
      "          [ 0.5183,  0.5278,  0.5246],\n",
      "          [ 0.5327,  0.5160,  0.5293]],\n",
      "\n",
      "         [[-0.6683, -0.5709, -0.5824],\n",
      "          [-0.5268, -0.5315, -0.5492],\n",
      "          [ 0.8207,  0.7716, -0.5214]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9993, -0.5269, -0.5205],\n",
      "          [-0.5334, -0.8837, -0.5914],\n",
      "          [ 0.5330,  0.7028,  0.5272]],\n",
      "\n",
      "         [[ 0.5316, -0.5320, -0.5188],\n",
      "          [ 0.5229,  0.5320, -0.6733],\n",
      "          [ 0.5314,  0.5150,  0.5277]],\n",
      "\n",
      "         [[ 0.5226,  1.0000,  0.5248],\n",
      "          [ 1.0000,  0.5584, -0.9142],\n",
      "          [ 0.5181,  0.5195,  0.5180]]]], device='cuda:0')\n",
      "tensor([0.5137, 0.5256, 0.9046, 0.9526, 0.9843, 1.0000, 0.5094, 0.9975, 0.9205,\n",
      "        0.6799, 0.6942, 0.9629, 0.6321, 0.9888, 0.5113, 0.9799, 0.5134, 0.5277,\n",
      "        1.0000, 0.5277, 0.9956, 0.5184, 0.5782, 0.5161, 1.0000, 0.9541, 0.7675,\n",
      "        1.0000, 0.5473, 0.9483, 0.9977, 0.9673, 0.5256, 0.9940, 0.5138, 0.5078,\n",
      "        0.9971, 0.9789, 0.5354, 0.5193, 0.6059, 0.5152, 1.0000, 0.6830, 0.5177,\n",
      "        1.0000, 0.5283, 0.5205, 0.5224, 0.9625, 0.9529, 0.5560, 0.9644, 0.5121,\n",
      "        0.9970, 0.9277, 0.9996, 1.0000, 0.5145, 0.5212, 0.5864, 0.8374, 0.9819,\n",
      "        0.9877, 0.5960, 0.5186, 0.9895, 0.6701, 1.0000, 0.9837, 0.5531, 0.5170,\n",
      "        0.9203, 0.5126, 0.9477, 0.5234, 0.5143, 0.9950, 0.9446, 0.5130, 0.9998,\n",
      "        0.5118, 0.9901, 0.5446, 0.5164, 0.5148, 0.5407, 0.5144, 1.0000, 1.0000,\n",
      "        0.5228, 1.0000, 1.0000, 0.9679, 0.5198, 0.6218, 0.5191, 0.9803, 0.5418,\n",
      "        1.0000, 0.9968, 0.5866, 0.5144, 0.5115, 0.6506, 0.5709, 0.5311, 0.5372,\n",
      "        0.5476, 1.0000, 0.6137, 0.5209, 0.9921, 0.5169, 0.9764, 0.9862, 0.5262,\n",
      "        0.9680, 0.8307, 0.7746, 0.9171, 0.5130, 0.5319, 1.0000, 0.5342, 0.8429,\n",
      "        0.9305, 0.9915, 0.5166, 0.9940, 0.5382, 0.9451, 0.5294, 0.5267, 0.5147,\n",
      "        1.0000, 1.0000, 0.6717, 0.5445, 0.9480, 0.9528, 0.5358, 0.5168, 0.5249,\n",
      "        1.0000, 0.5185, 0.9218, 0.9977, 0.5347, 0.9782, 0.5086, 0.9662, 0.5199,\n",
      "        0.9974, 0.5136, 0.6588, 0.9276, 1.0000, 1.0000, 0.7882, 0.5339, 1.0000,\n",
      "        0.5244, 0.9300, 0.9983, 0.8853, 1.0000, 0.6733, 0.5138, 0.5103, 1.0000,\n",
      "        0.5222, 0.5192, 1.0000, 1.0000, 0.5086, 0.5342, 1.0000, 0.9686, 0.9021,\n",
      "        0.5205, 1.0000, 0.5319, 0.5443, 0.5328, 0.5186, 0.5219, 0.9968, 0.5977,\n",
      "        0.6199, 0.9679, 0.9919, 1.0000, 0.5198, 0.9807, 0.9620, 0.8064, 0.9843,\n",
      "        0.9871, 0.5162, 0.9503, 0.6944, 0.9034, 0.8104, 0.5362, 0.8902, 1.0000,\n",
      "        0.5210, 0.5263, 0.5861, 0.5135, 0.5137, 0.5948, 0.5189, 0.9669, 0.9932,\n",
      "        0.5193, 0.6521, 0.9639, 0.5598, 0.5202, 0.8047, 1.0000, 0.9930, 0.5376,\n",
      "        0.9260, 0.9975, 0.5222, 0.5462, 0.5149, 0.5091, 0.5344, 1.0000, 1.0000,\n",
      "        0.9913, 1.0000, 0.9707, 0.8036, 0.5382, 0.9671, 0.9762, 0.5317, 0.9988,\n",
      "        0.5562, 0.9395, 0.5159, 1.0000, 0.5342, 0.5597, 0.5625, 0.9775, 0.9522,\n",
      "        0.9999, 0.5244, 0.5427, 0.5680], device='cuda:0')\n",
      "tensor([ 0.5160, -0.5280,  0.9161, -0.5808, -0.5843, -0.5245, -0.5388,  0.5131,\n",
      "        -0.5431,  0.5422, -0.7009, -0.5718, -0.5145,  1.0000, -0.5176, -0.5359,\n",
      "        -0.5141, -0.5147, -0.5105, -0.5148, -0.5274, -0.5558, -0.7353, -0.5196,\n",
      "        -0.5359,  0.5871, -0.6429,  0.5490,  0.5182,  0.5709, -0.5714, -0.5110,\n",
      "         0.9616, -0.5198, -0.5154, -0.5275, -0.5284, -0.5240, -0.5177, -0.5504,\n",
      "         0.9902,  0.5154, -0.5187, -0.6197, -0.5845, -0.5361,  0.5129, -0.7655,\n",
      "        -0.9592, -0.5411, -0.6227,  0.5157, -0.5765, -0.5204, -0.5152, -0.6383,\n",
      "        -0.5212, -0.5243, -0.5310, -0.5297,  0.5991, -0.5374, -0.5318, -0.5757,\n",
      "        -0.6785, -0.5326, -0.5503, -0.7538,  0.5339, -0.5520, -0.5925, -0.5314,\n",
      "        -0.6153, -0.5308, -0.5814, -0.8063, -0.5155, -0.5199, -0.5240, -0.5199,\n",
      "        -0.5257, -0.9492, -0.5592, -0.5286, -0.6805, -0.6224, -0.5504, -0.6659,\n",
      "        -0.5211, -0.5154, -0.7317, -0.5272, -0.5134,  0.5292, -0.5323, -0.5402,\n",
      "        -0.5139, -0.5344, -0.5190, -0.5230, -0.5138, -0.5237, -0.5168, -0.6515,\n",
      "         0.5231, -0.5427, -0.6857, -0.7915,  0.5101,  0.5258, -0.6364, -0.5201,\n",
      "        -0.5172, -0.5812, -0.5493, -0.5376, -0.5524, -0.5523,  0.5350,  0.5168,\n",
      "        -0.6083, -0.5410, -0.5296, -0.5227,  0.5350, -0.5813, -0.5102, -0.5551,\n",
      "        -0.6454, -0.5225, -0.5180, -0.5909,  0.5286, -0.5471, -0.6619,  0.5142,\n",
      "        -0.5300, -0.5179, -0.5313, -0.5829, -0.5137, -0.5313, -0.5499,  0.5150,\n",
      "        -0.5197, -0.6157, -0.5744, -0.5246,  0.5202, -0.5846,  0.5028, -0.5472,\n",
      "        -0.7891,  0.5220, -0.5156,  0.6064, -0.6089, -0.5141,  0.5130, -0.5979,\n",
      "        -0.7938, -0.5287, -0.5206, -0.5513, -0.5184,  0.6654, -0.5130, -0.5385,\n",
      "         0.5373, -0.5141, -0.5385, -0.5507, -0.5191, -0.5231, -0.5144, -0.5368,\n",
      "        -0.5267, -0.5227, -0.5501, -0.5737, -0.5208, -0.5096, -0.8482,  0.5140,\n",
      "        -0.5631, -0.8806, -0.8650, -0.5243,  1.0000, -0.6912, -0.5431, -0.5259,\n",
      "         0.5202, -0.5585, -0.5307, -0.5847, -0.5158,  0.5670, -0.5415, -0.5464,\n",
      "         0.6740,  0.5209, -0.5410,  0.5195,  0.5414, -0.5620, -0.5477, -0.8628,\n",
      "        -0.5556,  0.5148, -0.5145, -0.5360,  0.5447, -0.7006, -0.5510, -0.5223,\n",
      "        -0.8113,  0.5467, -0.5602,  0.5198, -0.5170, -0.5590,  0.5249, -0.5146,\n",
      "        -0.5266, -0.6266,  0.5170, -0.8732, -0.6805, -0.5520, -0.7321, -0.5623,\n",
      "        -0.5244, -0.5255, -0.5353, -0.5542, -0.5427, -0.5428,  0.5214, -0.5759,\n",
      "         0.5107, -0.5315,  0.5143, -0.5312, -0.5664, -0.9300, -0.5555, -0.5932,\n",
      "        -0.6873,  0.5660, -0.5244, -0.5668,  0.5174, -0.7225, -0.7246,  0.5387],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.7920,  0.5869,  0.7732],\n",
      "          [-1.0000, -1.0000, -0.8821],\n",
      "          [ 0.5215,  0.9814, -0.5725]],\n",
      "\n",
      "         [[-0.5355, -0.5208, -0.7028],\n",
      "          [-0.5147, -0.5466, -0.5299],\n",
      "          [-0.7173, -0.5315, -0.5274]],\n",
      "\n",
      "         [[-0.7216,  0.6079, -0.9448],\n",
      "          [-0.5186, -0.9730,  0.5330],\n",
      "          [-0.5309,  0.7921,  0.5818]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5230,  0.9988, -0.9998],\n",
      "          [-1.0000, -0.5151, -0.5223],\n",
      "          [-0.5292,  0.9981,  0.8994]],\n",
      "\n",
      "         [[ 0.9987,  0.6484,  0.9992],\n",
      "          [-0.5366,  0.6716,  0.9991],\n",
      "          [-0.5285, -0.8142,  0.5150]],\n",
      "\n",
      "         [[-0.5280, -0.5226, -0.5212],\n",
      "          [-0.5355, -0.5524,  0.9785],\n",
      "          [-0.5301, -0.5262, -0.5926]]],\n",
      "\n",
      "\n",
      "        [[[-0.9979,  0.5160,  0.7439],\n",
      "          [ 0.5579,  0.9974,  0.5285],\n",
      "          [ 0.5247,  0.5299,  0.5310]],\n",
      "\n",
      "         [[ 0.5316,  0.5193,  0.5288],\n",
      "          [ 0.5168,  0.5323,  0.5324],\n",
      "          [ 0.5154,  0.9998,  0.7777]],\n",
      "\n",
      "         [[-0.5154,  0.7734,  0.8864],\n",
      "          [-0.9935,  0.5153,  0.5702],\n",
      "          [ 0.9968, -0.5786, -0.5509]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9994, -0.5183, -0.5203],\n",
      "          [-0.5319, -0.5242, -0.9577],\n",
      "          [-0.5219, -0.5239, -0.5315]],\n",
      "\n",
      "         [[-0.5149,  0.9988, -0.5326],\n",
      "          [-0.5819,  0.5231,  0.5650],\n",
      "          [-0.8134, -0.5527, -0.6071]],\n",
      "\n",
      "         [[ 0.5324,  0.8097,  0.5158],\n",
      "          [ 0.5192,  0.5235,  0.5279],\n",
      "          [-0.5317, -0.5883, -0.5211]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9975,  0.7747, -0.7856],\n",
      "          [ 0.6175,  0.5187,  0.6828],\n",
      "          [ 0.9959,  0.9886,  0.5733]],\n",
      "\n",
      "         [[ 0.5273,  0.9920,  0.5303],\n",
      "          [-0.8386,  0.9886,  0.9908],\n",
      "          [ 0.5306, -1.0000,  0.9894]],\n",
      "\n",
      "         [[ 0.7082, -0.9791,  0.5940],\n",
      "          [ 0.5872,  0.7380,  0.5399],\n",
      "          [ 0.5252,  0.9237, -0.9336]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5748, -0.5226, -0.7237],\n",
      "          [-0.5169, -0.5159, -0.7617],\n",
      "          [-0.5279, -0.9999, -0.6849]],\n",
      "\n",
      "         [[ 0.9764, -0.5767, -0.5237],\n",
      "          [ 0.9991,  1.0000, -0.5235],\n",
      "          [ 0.7995, -0.5902, -0.5326]],\n",
      "\n",
      "         [[ 0.5316,  0.6533,  0.8566],\n",
      "          [-1.0000, -1.0000, -0.5297],\n",
      "          [-0.5306, -0.5309, -0.5264]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.6565,  0.5240,  0.5326],\n",
      "          [-1.0000,  0.5781,  0.5165],\n",
      "          [ 0.5178,  0.5273,  0.5178]],\n",
      "\n",
      "         [[-0.6278, -0.7153, -0.5308],\n",
      "          [-1.0000, -0.5222, -0.5297],\n",
      "          [-0.5334, -0.5244, -0.5159]],\n",
      "\n",
      "         [[-0.5409, -0.5255,  0.5248],\n",
      "          [-0.5180, -0.5156,  0.5284],\n",
      "          [-0.5340,  0.5240,  0.5236]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7023, -0.7053,  0.9960],\n",
      "          [-0.7113,  0.9950,  0.5281],\n",
      "          [-0.8177, -0.5740,  0.5314]],\n",
      "\n",
      "         [[ 1.0000,  0.8539,  0.5191],\n",
      "          [-0.5218, -0.9921,  0.5272],\n",
      "          [-0.5293, -0.9875,  0.5226]],\n",
      "\n",
      "         [[ 0.9063,  0.5164,  0.5824],\n",
      "          [ 0.5269,  0.6098, -1.0000],\n",
      "          [-0.9997, -0.6299, -0.5199]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9992,  0.5150,  0.5650],\n",
      "          [ 0.5203,  0.5324,  0.5301],\n",
      "          [ 0.9979,  0.5315,  0.5779]],\n",
      "\n",
      "         [[ 0.5164,  0.5241,  0.5299],\n",
      "          [ 0.5901, -0.5424, -0.5449],\n",
      "          [ 1.0000, -0.5186, -0.5169]],\n",
      "\n",
      "         [[-0.7028, -0.9025,  0.9953],\n",
      "          [-0.9733, -0.9981, -0.5201],\n",
      "          [-0.5293, -0.5275, -0.9989]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6876, -0.5508, -0.5315],\n",
      "          [-0.5149, -0.5727, -0.5320],\n",
      "          [ 0.8063, -0.5276, -0.5197]],\n",
      "\n",
      "         [[-0.5772, -1.0000, -0.5775],\n",
      "          [ 0.9997, -1.0000, -0.8252],\n",
      "          [-0.5300, -0.5923, -1.0000]],\n",
      "\n",
      "         [[ 0.5294,  0.5296,  0.5704],\n",
      "          [ 0.5289, -0.5611, -0.5241],\n",
      "          [ 0.6974,  0.5842, -0.5429]]],\n",
      "\n",
      "\n",
      "        [[[-0.9753,  0.5171, -0.5325],\n",
      "          [ 0.6541,  0.5289, -0.7429],\n",
      "          [ 0.7480,  0.9999, -0.5152]],\n",
      "\n",
      "         [[-0.5806, -0.5302, -0.5195],\n",
      "          [-0.5222, -0.8426, -0.5608],\n",
      "          [-0.5659, -0.9924, -0.5153]],\n",
      "\n",
      "         [[ 0.5329,  0.5238, -0.5216],\n",
      "          [ 0.5219,  0.5189, -0.6347],\n",
      "          [ 0.5226, -0.5241, -0.5286]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5222,  0.8991, -0.9982],\n",
      "          [-0.5276,  0.8995, -0.5300],\n",
      "          [-0.8032, -0.9984, -0.5152]],\n",
      "\n",
      "         [[-1.0000,  0.9985,  0.5305],\n",
      "          [ 0.7693,  0.5226,  0.9994],\n",
      "          [-0.9929, -0.5808, -0.9997]],\n",
      "\n",
      "         [[-0.6135, -0.8732, -0.9724],\n",
      "          [-0.5437,  0.9998, -0.5208],\n",
      "          [-0.7354, -0.5287, -0.5153]]]], device='cuda:0')\n",
      "tensor([1.0000, 0.5219, 0.8073, 0.9971, 0.5634, 1.0000, 0.9687, 0.5305, 1.0000,\n",
      "        1.0000, 0.5239, 0.9931, 1.0000, 0.9508, 0.5322, 0.9979, 0.6333, 0.9533,\n",
      "        0.5318, 0.9928, 0.5238, 0.5519, 0.9951, 0.9860, 0.9886, 0.9999, 0.5200,\n",
      "        0.9821, 0.5381, 0.9839, 0.8320, 0.9982, 0.9822, 0.9322, 0.7256, 0.5183,\n",
      "        0.9690, 0.9881, 0.5268, 0.9129, 0.9988, 0.5449, 0.9950, 1.0000, 0.5198,\n",
      "        0.8421, 0.5381, 0.5274, 0.9915, 0.9127, 0.7959, 0.9939, 0.5172, 0.5153,\n",
      "        0.9984, 0.9937, 0.9548, 0.8671, 0.8549, 0.5123, 1.0000, 1.0000, 0.9971,\n",
      "        0.9817, 0.9682, 0.5180, 0.9310, 0.5132, 0.9836, 0.9978, 0.5762, 0.5364,\n",
      "        0.5154, 0.5147, 0.5304, 0.9342, 0.9475, 0.9319, 0.5407, 0.9981, 0.7998,\n",
      "        0.9642, 0.5362, 0.5256, 0.8967, 0.9832, 0.9950, 0.5145, 0.6206, 0.9933,\n",
      "        0.9948, 0.9899, 0.5339, 0.9075, 0.9945, 0.5819, 1.0000, 0.5150, 0.5888,\n",
      "        0.5158, 0.5378, 0.5194, 0.5389, 0.9960, 0.5313, 0.5254, 1.0000, 0.9676,\n",
      "        1.0000, 0.9900, 0.9821, 1.0000, 0.9936, 0.9910, 0.8719, 0.9803, 0.9959,\n",
      "        0.5181, 1.0000, 0.9825, 0.8945, 0.5253, 0.5320, 0.5288, 1.0000, 0.5623,\n",
      "        0.9799, 0.9956, 0.9829, 0.9764, 0.5169, 0.9985, 0.5228, 0.9957, 0.7338,\n",
      "        0.9893, 0.9260, 0.5627, 0.5460, 1.0000, 0.7429, 0.9839, 0.5144, 0.6768,\n",
      "        0.9675, 0.5238, 0.5955, 0.9953, 0.5154, 0.5214, 0.5116, 1.0000, 0.5134,\n",
      "        0.5327, 0.5429, 0.7208, 1.0000, 0.9660, 0.8688, 0.5482, 0.9919, 0.5146,\n",
      "        0.8196, 0.9660, 0.5494, 0.8824, 0.5210, 0.5296, 0.5211, 0.5355, 0.9538,\n",
      "        0.9956, 0.9952, 0.8920, 0.9825, 0.9975, 1.0000, 0.5268, 0.5456, 0.9971,\n",
      "        0.9982, 0.5244, 0.5180, 0.9395, 0.5289, 0.9989, 0.8148, 0.9760, 0.5431,\n",
      "        0.9775, 0.9942, 0.7036, 0.8695, 1.0000, 0.9869, 0.9993, 0.5230, 0.5468,\n",
      "        0.9527, 0.6026, 0.5363, 0.5161, 0.5308, 0.9607, 0.9816, 1.0000, 0.5275,\n",
      "        0.9985, 0.5141, 0.9836, 0.5259, 0.5310, 0.9815, 0.9898, 0.5306, 0.9385,\n",
      "        0.9762, 0.9931, 0.9594, 0.5208, 0.5319, 0.8217, 0.9968, 0.9997, 0.9859,\n",
      "        1.0000, 0.9708, 0.9980, 0.9677, 0.5290, 0.5189, 0.5193, 0.5161, 0.5303,\n",
      "        0.5088, 0.5473, 0.9931, 0.6250, 0.9873, 0.5341, 1.0000, 0.6023, 0.7911,\n",
      "        0.9582, 0.9890, 0.5160, 0.9348, 0.5181, 0.9859, 0.5975, 0.5335, 0.8887,\n",
      "        0.7534, 0.7888, 0.5198, 1.0000], device='cuda:0')\n",
      "tensor([ 0.5277, -0.5162,  0.5669, -0.5141,  0.5314,  0.5130, -0.5424, -0.7845,\n",
      "         0.5116, -0.5507, -0.5154,  0.6298,  0.5249,  0.5311, -0.5184, -0.9654,\n",
      "        -0.5405,  0.6149, -0.6609,  0.5265, -0.5140,  0.5191, -0.5143,  0.8893,\n",
      "         0.5231,  0.5327, -0.6515,  0.9187, -0.8525,  0.7286, -0.5706,  0.5332,\n",
      "        -0.5457, -0.5561, -0.6040,  0.5144,  0.6770, -0.9592, -0.5167, -0.5177,\n",
      "         0.9986, -0.5143,  0.5256,  0.5179, -0.5237,  0.5276, -0.5178, -0.7646,\n",
      "        -0.6853,  0.9997, -0.5154, -0.5348, -0.5142,  0.5171,  1.0000, -0.5154,\n",
      "         0.5147, -0.5275, -0.5144, -0.5167, -0.5214,  0.9997, -0.5150, -0.5380,\n",
      "         0.9968, -0.7428, -0.6217, -0.8922, -0.6306, -0.5537, -0.5559,  0.5702,\n",
      "         0.5148, -0.5405,  0.5225,  0.5223, -0.5395,  0.5548, -0.7028,  0.5189,\n",
      "        -0.5465,  0.7325, -0.5154, -0.5134, -0.5169,  0.5274,  0.8373, -0.7947,\n",
      "        -0.5220,  1.0000,  0.6899, -0.5583, -0.5223, -0.5136, -0.5303,  0.5260,\n",
      "         0.8806, -0.5145, -0.5140, -0.6143, -0.5300,  0.5394,  0.5232,  0.5182,\n",
      "        -0.5146, -0.5144,  0.6784, -0.5169,  0.6142,  0.7735,  0.8609, -0.5164,\n",
      "         0.5162,  0.5398, -0.7503,  0.8741, -0.8409, -0.5339,  0.9881,  0.5361,\n",
      "         0.5477, -0.5141, -0.5147,  0.5250, -0.5165,  0.5228, -0.5750,  0.5929,\n",
      "        -0.5209, -0.5054, -0.5212,  0.9095, -0.8750,  0.5665,  0.5222,  0.6398,\n",
      "         0.5599,  0.5419,  0.5175,  0.9933, -0.5189,  0.5708, -0.6870,  0.5196,\n",
      "         0.5177, -0.5370, -0.5148, -0.5375, -0.5143, -0.5149, -0.5733,  0.5209,\n",
      "        -0.5191, -0.5176, -0.5205,  0.5282,  0.9990, -0.5470, -0.5250, -0.5148,\n",
      "        -0.5345, -0.6141, -0.5198, -0.5963, -0.5154,  0.9814, -0.6350,  0.5256,\n",
      "        -0.5226, -0.5633,  0.5281,  0.5380, -0.6379, -0.5427,  0.9360, -0.5550,\n",
      "        -0.5317, -0.5151, -0.5126, -0.5310,  0.5761, -0.5148, -0.5136,  0.5246,\n",
      "        -0.5148,  0.5496, -0.5212, -0.5344,  0.5201,  0.5289,  0.9966, -0.5199,\n",
      "        -0.5145, -0.5456,  0.6646,  0.5262, -0.5155,  0.5139, -0.5163,  0.5276,\n",
      "        -0.5534, -0.6675, -0.5143, -0.5199,  0.8492,  0.5206, -0.5152,  0.9992,\n",
      "        -0.5653,  0.5171, -0.6456, -0.5474,  0.5294,  0.5205, -0.5144, -0.9425,\n",
      "         0.6003,  0.5422,  0.5461, -0.8486, -0.6221, -0.5185,  0.9658, -0.6756,\n",
      "        -0.5441,  0.5789,  0.6230,  0.9985, -0.5534, -0.5151, -0.5179, -0.7496,\n",
      "        -0.5146, -0.5244,  0.5304, -0.5126, -0.5213, -0.5140,  0.5235, -0.5159,\n",
      "         0.5239,  0.5392, -0.5480,  0.5153,  0.9983, -0.7682, -0.6932, -0.5154,\n",
      "        -0.5246, -0.6077,  0.5291, -0.5434, -0.5140,  0.5123, -0.5187,  0.5153],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-0.6778,  0.5781, -1.0000],\n",
      "          [-0.9983,  0.5223,  1.0000],\n",
      "          [ 0.5244,  0.5316,  0.5201]],\n",
      "\n",
      "         [[ 0.5221,  0.5825, -1.0000],\n",
      "          [ 0.5147,  0.9956, -1.0000],\n",
      "          [ 0.5301,  0.5727, -0.5308]],\n",
      "\n",
      "         [[-0.9992,  0.5149,  0.9996],\n",
      "          [-1.0000,  0.9986,  0.9969],\n",
      "          [-0.5256, -0.7710, -0.5166]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9997, -0.7110, -0.9969],\n",
      "          [-0.9930, -0.5957,  0.5335],\n",
      "          [-0.8094,  0.9989,  0.5474]],\n",
      "\n",
      "         [[ 0.5249,  0.6863,  0.9391],\n",
      "          [ 0.7212,  0.5148,  0.5147],\n",
      "          [ 0.5290,  0.5233,  0.5275]],\n",
      "\n",
      "         [[-0.9998,  0.5203,  0.8271],\n",
      "          [ 0.7812,  0.5604,  0.5149],\n",
      "          [ 1.0000,  0.5438, -0.9988]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5149,  0.5183,  0.5265],\n",
      "          [ 0.5293,  0.5210,  0.5191],\n",
      "          [ 0.5696,  0.5270,  0.5322]],\n",
      "\n",
      "         [[ 0.5194,  0.5171,  0.5274],\n",
      "          [ 0.5172,  0.5208,  0.5268],\n",
      "          [ 0.5255,  0.5281,  0.9975]],\n",
      "\n",
      "         [[ 0.5229,  0.6167,  0.5235],\n",
      "          [ 0.6477,  0.7829,  0.5276],\n",
      "          [-0.5760,  1.0000,  0.9991]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6545, -0.9978,  1.0000],\n",
      "          [ 0.6840,  0.5172,  0.5728],\n",
      "          [ 0.5253,  1.0000,  0.5718]],\n",
      "\n",
      "         [[ 0.5149,  0.5243,  0.5276],\n",
      "          [ 0.5148,  0.5183,  0.5200],\n",
      "          [ 0.5260,  0.5322,  0.5232]],\n",
      "\n",
      "         [[ 0.6836,  1.0000, -0.5893],\n",
      "          [ 0.7331, -1.0000, -0.5888],\n",
      "          [ 0.9999,  0.9601,  0.5249]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000, -0.5183,  1.0000],\n",
      "          [ 1.0000, -0.8142,  1.0000],\n",
      "          [-0.5254, -0.5778, -0.5217]],\n",
      "\n",
      "         [[-0.5183, -0.9995, -0.5320],\n",
      "          [-0.5309, -0.5256, -0.5307],\n",
      "          [-0.5206, -0.5307, -0.7860]],\n",
      "\n",
      "         [[ 0.6078,  0.6279,  0.5472],\n",
      "          [-0.9995,  0.6281,  0.7345],\n",
      "          [-0.9990,  1.0000, -0.7152]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5496,  0.5661,  0.5175],\n",
      "          [ 0.9998,  0.9999,  0.6718],\n",
      "          [ 0.9996, -0.7281, -0.6043]],\n",
      "\n",
      "         [[-0.9996, -0.5185, -0.5618],\n",
      "          [-0.5182, -0.5529, -0.5221],\n",
      "          [-0.7683, -0.9992, -0.5952]],\n",
      "\n",
      "         [[ 1.0000,  0.5980, -0.9995],\n",
      "          [ 1.0000, -0.6131, -0.5778],\n",
      "          [ 0.9107,  0.9996,  0.7426]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0000, -0.6515, -0.5617],\n",
      "          [-0.6447, -0.5286,  0.9999],\n",
      "          [-0.5293, -0.5218, -0.5167]],\n",
      "\n",
      "         [[-0.5263, -0.6030, -0.5365],\n",
      "          [-1.0000,  1.0000, -0.5280],\n",
      "          [-1.0000, -0.6162,  1.0000]],\n",
      "\n",
      "         [[ 0.7236, -0.6784,  0.5183],\n",
      "          [ 0.5314,  0.5265,  0.7023],\n",
      "          [-0.5851, -0.6435,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7425,  0.6148,  1.0000],\n",
      "          [-0.5283,  1.0000,  0.9999],\n",
      "          [-0.6403,  0.5194, -0.5215]],\n",
      "\n",
      "         [[-0.7616, -0.5283, -0.8477],\n",
      "          [-0.5226, -0.7752, -0.5152],\n",
      "          [-0.5254, -0.5148, -0.7443]],\n",
      "\n",
      "         [[-0.8648, -1.0000, -1.0000],\n",
      "          [-0.5245, -1.0000,  1.0000],\n",
      "          [ 0.5607,  0.5508, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5149,  0.5302,  0.6989],\n",
      "          [ 0.5799,  0.5285,  0.7016],\n",
      "          [ 0.6028,  0.5149,  0.5166]],\n",
      "\n",
      "         [[ 1.0000,  0.5836,  0.5148],\n",
      "          [ 0.5195,  0.5210,  0.5160],\n",
      "          [ 0.5315, -0.9998,  0.5284]],\n",
      "\n",
      "         [[ 1.0000,  1.0000, -0.7292],\n",
      "          [-0.6281,  1.0000,  0.5218],\n",
      "          [-0.6797,  0.5618,  0.5229]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9999, -0.6386, -0.5523],\n",
      "          [-1.0000, -0.5580, -0.5584],\n",
      "          [-1.0000, -0.8382, -0.9984]],\n",
      "\n",
      "         [[ 0.7163,  0.5161,  0.5256],\n",
      "          [ 1.0000,  0.6164,  0.5181],\n",
      "          [ 0.5600,  0.5288,  0.5338]],\n",
      "\n",
      "         [[-0.5971,  0.6959, -0.8922],\n",
      "          [-0.5484, -0.6971, -0.7891],\n",
      "          [ 0.6783,  0.8475,  0.5877]]],\n",
      "\n",
      "\n",
      "        [[[-0.9919, -0.9952, -0.5236],\n",
      "          [ 0.9195,  1.0000, -0.7479],\n",
      "          [-0.9925, -0.7241,  0.5993]],\n",
      "\n",
      "         [[-0.9997,  0.9376, -0.9366],\n",
      "          [-0.9995,  0.7286, -0.6619],\n",
      "          [-0.9997,  0.5625,  0.7489]],\n",
      "\n",
      "         [[-0.9984,  0.5206,  0.9949],\n",
      "          [ 0.5300, -0.8125, -0.5196],\n",
      "          [ 0.5156,  0.9843,  0.5959]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6975, -0.7906, -0.6275],\n",
      "          [-0.6757, -0.5230, -0.6680],\n",
      "          [ 0.8561,  0.9909,  0.6934]],\n",
      "\n",
      "         [[ 0.7038, -0.9999, -0.9986],\n",
      "          [ 0.5721,  0.5309,  0.5197],\n",
      "          [-0.6518,  0.6486,  0.5330]],\n",
      "\n",
      "         [[-0.5176,  0.5476,  1.0000],\n",
      "          [-0.6091, -0.8067, -0.9978],\n",
      "          [ 0.8939,  0.9981, -0.7551]]]], device='cuda:0')\n",
      "tensor([0.5147, 0.5140, 0.9999, 0.6522, 0.9993, 0.9999, 0.7668, 1.0000, 0.5155,\n",
      "        0.5145, 0.5140, 0.5170, 0.5149, 0.7311, 0.9993, 0.9984, 0.7161, 0.8603,\n",
      "        0.9773, 0.9902, 0.9997, 1.0000, 0.5220, 0.9979, 0.5145, 1.0000, 0.9997,\n",
      "        0.5203, 0.8815, 0.5146, 0.5151, 0.9979, 0.8463, 0.9987, 0.6769, 0.5152,\n",
      "        0.9969, 0.5147, 0.9998, 0.5146, 0.9955, 0.5430, 0.5147, 0.5144, 0.5132,\n",
      "        0.5156, 0.9638, 0.5143, 0.5198, 0.5153, 0.9193, 0.9971, 0.9994, 0.8220,\n",
      "        0.5178, 0.9125, 0.7554, 0.5173, 0.8818, 0.5178, 0.9196, 0.5138, 0.5095,\n",
      "        0.9999, 0.5161, 1.0000, 0.5159, 1.0000, 0.5156, 0.9998, 0.8919, 0.5148,\n",
      "        0.5145, 0.9999, 0.9865, 0.9391, 0.5154, 0.5153, 0.9980, 1.0000, 0.9995,\n",
      "        0.5142, 0.9994, 0.9698, 0.7953, 0.5152, 1.0000, 0.5174, 0.5984, 0.5150,\n",
      "        0.8739, 0.9966, 0.9655, 0.5582, 0.6836, 0.9006, 1.0000, 0.5146, 0.5131,\n",
      "        0.8029, 0.9286, 0.8965, 0.9992, 1.0000, 0.9999, 1.0000, 1.0000, 0.5145,\n",
      "        0.9481, 1.0000, 1.0000, 0.5126, 0.9997, 0.9283, 0.9955, 0.7192, 0.6344,\n",
      "        0.5143, 0.5164, 1.0000, 0.9464, 0.9967, 0.5147, 0.5139, 0.9965, 0.9983,\n",
      "        0.9146, 0.9992, 0.9970, 0.7430, 0.9927, 0.5162, 0.5135, 0.9527, 0.9181,\n",
      "        0.5131, 0.9970, 1.0000, 0.5135, 0.9965, 0.5433, 0.8579, 0.9807, 0.9718,\n",
      "        0.9959, 0.5150, 0.9902, 0.9992, 0.9999, 0.5142, 0.5132, 0.5149, 0.5152,\n",
      "        0.9994, 0.5140, 0.9984, 0.8545, 0.9982, 0.5178, 0.5178, 0.5160, 0.9508,\n",
      "        0.5166, 0.5141, 1.0000, 0.8235, 0.9059, 0.9906, 0.7942, 0.9986, 0.5147,\n",
      "        0.5132, 0.5158, 1.0000, 0.5205, 0.9989, 0.5176, 0.5180, 0.5149, 0.5125,\n",
      "        0.9961, 0.9083, 0.9979, 0.5154, 1.0000, 1.0000, 0.5157, 0.5153, 0.5112,\n",
      "        0.5148, 0.5150, 0.9973, 0.5126, 0.9331, 0.5130, 0.9999, 0.9661, 0.5340,\n",
      "        0.6041, 0.5155, 0.8558, 1.0000, 0.5149, 0.5313, 0.9962, 1.0000, 0.5849,\n",
      "        0.9945, 0.5163, 0.5142, 0.9987, 0.9495, 0.5153, 0.5156, 0.5146, 0.5165,\n",
      "        0.5155, 0.5143, 0.6272, 0.5138, 1.0000, 0.9599, 0.9276, 0.9975, 0.9638,\n",
      "        0.5136, 0.8605, 0.5379, 0.8300, 0.5276, 0.9439, 0.9297, 0.5145, 1.0000,\n",
      "        0.5172, 1.0000, 0.5161, 0.5139, 0.8015, 0.5145, 0.5161, 0.8929, 0.5914,\n",
      "        0.9989, 0.9991, 0.5145, 0.5151, 0.5156, 0.6797, 0.9993, 0.8676, 1.0000,\n",
      "        0.9958, 1.0000, 0.9789, 1.0000, 0.8315, 0.9082, 0.5120, 0.9991, 0.5163,\n",
      "        0.5131, 0.5150, 0.5144, 1.0000, 0.9998, 0.5156, 0.7950, 0.7159, 0.5153,\n",
      "        0.5155, 0.9999, 0.9984, 0.9937, 0.5165, 0.5129, 0.5159, 0.9996, 0.5216,\n",
      "        0.9676, 0.6992, 0.9726, 0.5127, 0.5148, 0.9980, 1.0000, 0.9967, 0.9793,\n",
      "        1.0000, 1.0000, 1.0000, 0.5151, 0.5185, 0.5143, 0.9008, 0.9809, 0.9998,\n",
      "        0.8079, 0.6136, 0.5150, 0.5140, 0.5153, 0.9525, 0.5142, 0.9103, 0.9961,\n",
      "        0.8440, 0.5150, 0.5133, 0.9797, 0.7335, 0.9999, 0.8697, 0.5159, 0.9768,\n",
      "        0.9965, 0.9986, 1.0000, 0.5150, 0.5150, 0.5146, 1.0000, 0.9648, 0.5165,\n",
      "        0.9983, 0.9981, 1.0000, 0.8687, 0.5222, 0.5159, 0.9976, 0.6802, 0.9364,\n",
      "        0.9484, 0.9541, 0.9685, 0.9599, 0.5130, 0.5145, 0.5165, 0.5126, 0.9997,\n",
      "        1.0000, 0.5639, 0.5141, 0.8704, 0.9728, 0.5171, 1.0000, 0.5135, 0.5142,\n",
      "        0.9476, 0.5145, 0.9998, 0.5157, 0.7945, 0.9998, 0.5141, 0.9974, 0.5133,\n",
      "        1.0000, 0.9755, 0.9995, 0.9304, 0.9989, 0.5149, 0.9999, 0.5150, 0.5130,\n",
      "        0.9734, 1.0000, 0.5205, 1.0000, 0.9836, 0.5172, 1.0000, 0.5137, 0.8348,\n",
      "        0.5224, 0.8361, 0.7607, 0.9992, 1.0000, 0.9300, 1.0000, 0.9999, 0.8627,\n",
      "        0.8835, 0.5129, 0.5152, 0.5149, 1.0000, 0.9966, 0.5147, 0.9952, 0.5189,\n",
      "        0.9869, 0.5249, 1.0000, 0.9994, 0.5149, 0.5146, 0.9893, 0.5155, 0.5160,\n",
      "        0.5151, 0.5152, 0.9263, 0.5151, 0.5194, 0.9999, 0.9551, 0.5150, 0.5172,\n",
      "        0.5154, 0.9891, 0.9140, 1.0000, 0.8727, 0.5152, 0.5150, 0.5142, 0.5151,\n",
      "        0.9009, 0.5147, 0.9957, 0.5158, 1.0000, 0.9065, 1.0000, 0.9191, 0.5156,\n",
      "        0.6097, 0.5135, 0.5155, 0.5151, 0.9977, 0.5145, 0.9992, 0.5135, 0.9034,\n",
      "        0.9231, 1.0000, 0.9997, 0.9997, 0.5143, 0.5126, 0.5138, 0.6533, 0.9998,\n",
      "        1.0000, 0.5147, 1.0000, 0.9998, 0.5190, 1.0000, 0.5157, 0.9617, 0.5192,\n",
      "        0.9995, 0.9120, 0.5181, 0.8342, 0.9949, 1.0000, 0.5145, 1.0000, 0.5111,\n",
      "        0.8190, 0.9451, 0.5141, 0.9912, 0.5138, 0.5147, 1.0000, 0.5144, 0.9999,\n",
      "        0.5146, 0.8346, 0.6092, 0.5169, 0.9990, 0.9999, 1.0000, 0.9797, 0.5153,\n",
      "        0.5152, 0.9999, 0.9643, 0.9346, 0.5146, 0.9714, 0.9997, 0.9303, 0.5120,\n",
      "        0.5156, 0.8754, 1.0000, 0.5151, 0.9988, 0.5143, 0.9933, 1.0000, 1.0000,\n",
      "        0.9986, 0.9986, 0.5159, 1.0000, 0.5155, 0.9830, 0.5171, 0.9976],\n",
      "       device='cuda:0')\n",
      "tensor([-0.6618, -0.5471, -0.5228, -0.5374,  0.9999,  1.0000, -0.5240, -0.5298,\n",
      "        -0.5633, -0.6509, -0.5418, -0.5712, -0.5389, -0.5313,  1.0000,  0.9999,\n",
      "        -0.5220, -0.5301, -0.5258,  0.9996,  1.0000, -0.5148, -0.5364,  1.0000,\n",
      "        -0.5356,  0.8717,  1.0000, -0.5420,  0.9997, -0.5784, -0.6699,  1.0000,\n",
      "        -0.5296, -0.5172,  0.9915, -0.5672,  0.8350, -0.5577,  1.0000, -0.5389,\n",
      "         0.9993, -0.5407, -0.5705, -0.5745, -0.6238, -0.5330, -0.5265, -0.5396,\n",
      "        -0.5310, -0.6350,  0.9994,  0.6443, -0.8618, -0.5215, -0.5313, -0.5210,\n",
      "         0.9210, -0.5877,  0.9996, -0.5419, -0.5272, -1.0000, -1.0000, -0.5164,\n",
      "        -0.5311,  0.7082, -0.5538,  0.8253, -0.5525, -0.5159, -0.5386, -0.5495,\n",
      "        -0.5386,  0.9997,  0.5200, -0.5156, -0.5263, -0.6183,  1.0000, -0.5313,\n",
      "         0.9161, -0.5581,  1.0000, -0.6494, -0.5236, -0.6096,  0.5174, -0.6320,\n",
      "        -0.5378, -0.5517,  0.9989, -0.6494,  0.5763, -0.5241,  0.9999, -0.9999,\n",
      "        -0.5144, -0.5500, -0.5616,  0.9999, -0.5242,  0.7485,  0.9073,  0.7497,\n",
      "         1.0000,  1.0000, -0.5370, -0.5846, -0.5206,  0.9996,  1.0000, -0.5685,\n",
      "         0.5423, -0.5314, -0.5293, -0.5313,  0.7399, -0.5587, -0.6028, -0.5245,\n",
      "        -0.5727, -0.5188, -0.5677, -0.5361, -0.5224,  0.5170, -0.5185,  0.8300,\n",
      "        -0.5178, -0.5321, -0.5311, -0.5454, -0.6651,  0.8332, -0.5232, -0.5425,\n",
      "        -0.5184,  0.8274, -0.6290,  0.9137, -0.5302, -0.5374, -0.9998, -0.5190,\n",
      "        -0.5222, -0.5284, -0.5289,  0.9511,  1.0000, -0.5366, -0.5890, -0.6179,\n",
      "        -0.5776, -0.5171, -0.5654, -0.5219, -0.5181,  1.0000, -0.5623, -0.5586,\n",
      "        -0.5655, -0.5227, -0.5384, -0.5739,  0.5419, -0.5327, -0.6128, -0.5173,\n",
      "         0.9940,  0.9998, -0.6802, -0.5327, -0.5501,  0.6958, -0.5459,  0.9998,\n",
      "        -0.9998, -0.5915, -0.5369, -0.5928,  0.6882,  0.8239,  1.0000, -0.5615,\n",
      "         1.0000,  0.5192, -0.6277, -0.5352, -0.5957, -0.6698, -0.5307,  0.8077,\n",
      "        -0.5657, -0.5421, -0.5409,  0.9997, -0.9996, -0.5265,  0.9999, -0.5521,\n",
      "        -0.5188,  0.5311, -0.5908, -0.5228,  0.9998, -0.5229, -0.5341,  0.7659,\n",
      "        -0.5348, -0.5634,  1.0000, -0.5231, -0.5323, -0.6124, -0.6324, -1.0000,\n",
      "        -0.5432, -0.5834, -0.5390, -0.5459,  0.7793, -0.5320, -0.5217,  1.0000,\n",
      "         0.7035, -0.5736,  1.0000,  0.9999,  1.0000, -0.5327, -0.6930, -0.5155,\n",
      "        -0.6110,  0.5165, -0.5570,  1.0000, -0.5522, -0.5407,  0.9677, -0.5441,\n",
      "        -0.5936, -0.5244, -0.5208, -0.5164,  1.0000, -0.5776, -0.5699, -0.5570,\n",
      "         0.9998,  1.0000,  1.0000,  1.0000,  0.9999,  0.9988, -0.6789,  0.9928,\n",
      "         0.9999, -0.5202, -0.6015,  1.0000, -0.5630, -0.5903, -0.5480, -0.6887,\n",
      "         0.9997,  0.9990, -0.5528,  0.5269,  0.8362, -0.6427, -0.5754,  1.0000,\n",
      "         1.0000,  0.9999, -0.5398, -0.5544, -0.5276, -0.5254, -0.5576,  1.0000,\n",
      "        -0.5305, -0.9992, -0.5980, -0.6210,  0.9268, -0.5163, -0.5360, -0.9646,\n",
      "         1.0000,  0.9995,  0.5152, -0.5726, -0.5263, -0.5307, -0.5155,  0.9966,\n",
      "         1.0000, -0.5176, -0.9990, -0.5259, -0.5415, -0.5685, -0.5237, -0.7482,\n",
      "        -0.5284,  0.9996,  0.6178, -0.5622, -0.5496, -0.5159, -0.5339,  0.6026,\n",
      "        -0.5205, -0.5352, -0.9998, -0.5282,  1.0000,  0.5185, -0.5567, -0.6923,\n",
      "        -0.6582,  0.9989, -0.5274, -0.6264, -0.5184,  0.9982,  0.8335,  0.9999,\n",
      "        -0.5404, -0.5530, -0.5310,  1.0000, -0.5226,  0.5366, -0.5189,  0.9232,\n",
      "         0.9415, -0.5686, -0.5370, -0.5277, -0.5595,  1.0000,  0.8003, -0.5294,\n",
      "        -0.5349, -0.5213, -0.5297, -0.5333,  0.5186, -0.5328, -0.5574, -0.5260,\n",
      "        -0.6834,  0.6946, -0.5694, -0.5284,  0.9999, -0.6323,  0.9999, -0.5970,\n",
      "         1.0000,  0.9997, -0.6433, -0.5258,  0.9998, -0.6131,  0.9999, -0.5676,\n",
      "        -0.6261,  1.0000,  0.5172, -0.5496,  0.6330, -0.5246, -0.5314, -0.5149,\n",
      "        -0.5460,  0.7890, -0.5356,  0.5158, -0.5258,  0.7731, -0.6306, -0.5356,\n",
      "         1.0000,  0.9999, -0.5312,  0.5180, -0.5775, -0.5772, -0.7380, -0.5147,\n",
      "        -0.5161,  0.7047, -0.5316, -0.5477,  0.9994, -0.5626,  0.7269,  1.0000,\n",
      "        -0.5410, -0.5308, -0.9986, -0.5292, -0.5319, -0.5960, -0.5803, -0.5149,\n",
      "        -0.5431, -0.5432,  0.9995, -0.5242, -0.5615, -0.6014, -0.5466, -0.5190,\n",
      "        -0.5279, -0.9427, -0.5413, -0.5885, -0.5839, -0.5412, -0.5299, -0.5335,\n",
      "        -0.5689,  0.9280, -0.5880,  0.9998,  0.5465,  0.9996, -0.5176, -0.6919,\n",
      "        -0.5351, -0.5807, -0.5557, -0.6623,  0.9990, -0.5451, -0.5199, -0.6116,\n",
      "         0.9997,  0.7413, -0.5220,  0.6899, -0.5182, -0.5323, -0.5576, -0.6892,\n",
      "        -0.5204,  0.7175,  1.0000, -0.5426,  0.9985,  0.8986, -0.5287,  1.0000,\n",
      "        -0.5242, -0.5199, -0.5587,  1.0000, -0.5219, -0.5282,  1.0000,  1.0000,\n",
      "         1.0000, -0.6312,  0.6147, -0.5839, -0.6013, -0.5260, -0.6196,  0.9998,\n",
      "        -0.5662, -0.5759, -0.5166, -0.5624, -0.5154, -0.5468,  0.8976,  0.9961,\n",
      "        -0.5419,  1.0000,  1.0000,  1.0000, -0.7797, -0.5915, -0.6521,  0.9992,\n",
      "         0.8568,  0.5482, -0.5364,  0.5491, -0.5177,  0.7610, -0.5438, -0.5626,\n",
      "         0.9996,  0.9993, -0.5342, -0.5203, -0.5873,  0.5145, -0.5171,  0.9482,\n",
      "         0.9102,  1.0000, -0.7315, -0.6115, -0.5325, -0.9956, -0.5390, -0.5196],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5419,  0.5245,  0.5842],\n",
      "          [-0.5603, -0.5263, -0.5182],\n",
      "          [-0.5225, -0.5179, -0.5277]],\n",
      "\n",
      "         [[ 0.5326,  0.5301,  0.5147],\n",
      "          [ 0.5969,  0.5263,  0.5282],\n",
      "          [-0.5228, -0.5212, -0.5326]],\n",
      "\n",
      "         [[-0.7170,  0.9018,  0.5368],\n",
      "          [ 0.6847,  0.5406,  0.7428],\n",
      "          [ 0.5182,  0.5190,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  1.0000, -0.8817],\n",
      "          [-0.8102,  0.7631, -0.7248],\n",
      "          [ 1.0000,  0.5825,  0.5418]],\n",
      "\n",
      "         [[ 0.5744,  0.5326,  0.5270],\n",
      "          [-0.5294, -0.7445, -1.0000],\n",
      "          [-0.5323, -0.5148, -0.5191]],\n",
      "\n",
      "         [[-0.5293,  0.7694, -1.0000],\n",
      "          [-0.7887,  0.5286,  0.5324],\n",
      "          [ 0.9996,  0.9997,  0.9999]]],\n",
      "\n",
      "\n",
      "        [[[-0.5946, -0.5247, -0.5202],\n",
      "          [-0.5202, -0.5231, -0.5160],\n",
      "          [-0.5265, -0.5324, -0.5295]],\n",
      "\n",
      "         [[-0.5209, -0.5295, -0.7120],\n",
      "          [-0.5149, -0.5279, -0.5237],\n",
      "          [-0.5326, -0.5149, -0.5234]],\n",
      "\n",
      "         [[-0.6491,  0.7409,  1.0000],\n",
      "          [-0.7671,  0.6857,  0.5229],\n",
      "          [ 0.5521,  0.6782,  0.9422]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7218,  1.0000, -1.0000],\n",
      "          [ 1.0000,  0.5210, -1.0000],\n",
      "          [ 0.6624,  0.6144,  0.7728]],\n",
      "\n",
      "         [[-0.5223, -0.5285, -0.8438],\n",
      "          [-0.5150, -0.5195, -0.5246],\n",
      "          [-0.5229, -0.5170, -0.5327]],\n",
      "\n",
      "         [[-1.0000,  0.6184,  0.6888],\n",
      "          [ 1.0000,  0.5217,  0.5265],\n",
      "          [ 0.5550,  0.5318,  0.5169]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5184,  0.9404,  0.5192],\n",
      "          [-0.5192, -0.5244, -0.5287],\n",
      "          [-0.5320, -0.5246, -0.5245]],\n",
      "\n",
      "         [[ 0.5277,  0.5241,  0.5154],\n",
      "          [-0.5193, -0.5242, -0.6463],\n",
      "          [-0.5272, -0.5149, -0.5151]],\n",
      "\n",
      "         [[ 1.0000, -0.9999,  1.0000],\n",
      "          [-1.0000,  0.5307,  0.5449],\n",
      "          [ 1.0000,  0.8646,  0.5307]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000, -1.0000, -0.5220],\n",
      "          [ 1.0000,  0.5467, -0.6051],\n",
      "          [ 0.7825,  0.6109,  0.5416]],\n",
      "\n",
      "         [[ 0.5167,  0.5164,  0.5789],\n",
      "          [-0.5293, -0.5227, -0.5934],\n",
      "          [-0.5315, -0.5229, -0.5263]],\n",
      "\n",
      "         [[-0.5851,  0.5222, -0.9988],\n",
      "          [-0.5390,  0.5676,  0.5490],\n",
      "          [ 1.0000,  1.0000, -0.9974]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5148,  0.5240,  0.5205],\n",
      "          [ 0.5176,  0.5173,  0.5322],\n",
      "          [ 0.7452,  0.5313,  0.5867]],\n",
      "\n",
      "         [[ 0.5278,  0.5177,  0.5240],\n",
      "          [ 0.5172,  0.5276,  0.5255],\n",
      "          [-0.9977,  0.8951,  0.7643]],\n",
      "\n",
      "         [[-1.0000, -0.7257, -0.9241],\n",
      "          [-1.0000, -0.6527, -0.5363],\n",
      "          [-1.0000, -0.7973, -0.5151]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000, -0.6400,  1.0000],\n",
      "          [-0.6557, -0.7920,  1.0000],\n",
      "          [-0.5292, -0.6378,  0.6734]],\n",
      "\n",
      "         [[ 0.5250,  0.5209,  0.5147],\n",
      "          [ 0.5175,  0.5321,  0.5317],\n",
      "          [ 0.8686,  1.0000,  0.9149]],\n",
      "\n",
      "         [[ 1.0000,  0.7403, -0.6114],\n",
      "          [-0.9962, -0.7122,  0.9926],\n",
      "          [ 1.0000, -0.7900, -0.5497]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5292,  0.5148,  0.7675],\n",
      "          [-1.0000, -0.9982, -0.9999],\n",
      "          [ 0.7169, -0.7409, -0.5227]],\n",
      "\n",
      "         [[ 0.5294,  0.5318,  0.5269],\n",
      "          [ 0.6148,  0.5227,  0.5400],\n",
      "          [ 0.6789,  0.5153,  0.5250]],\n",
      "\n",
      "         [[ 1.0000,  0.8720,  1.0000],\n",
      "          [ 0.5729,  0.5368, -1.0000],\n",
      "          [-1.0000,  0.5621,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0000, -1.0000, -0.5760],\n",
      "          [ 0.6159,  0.5172,  0.8065],\n",
      "          [ 0.5512,  0.5261, -1.0000]],\n",
      "\n",
      "         [[ 0.5326,  0.5309,  0.5196],\n",
      "          [ 0.5267,  0.5222,  0.5241],\n",
      "          [ 0.7357,  0.5555, -0.7389]],\n",
      "\n",
      "         [[-0.5314, -0.5431, -0.5735],\n",
      "          [-0.9978,  0.7878,  0.5296],\n",
      "          [ 0.6177,  1.0000, -0.9993]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000, -0.5586, -1.0000],\n",
      "          [-0.5311, -0.5323, -0.5178],\n",
      "          [-0.7545, -0.5301, -0.6449]],\n",
      "\n",
      "         [[-1.0000, -0.5324, -0.7675],\n",
      "          [-0.5316, -0.5321, -0.5321],\n",
      "          [-0.5276, -0.5231, -0.5193]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -0.7886],\n",
      "          [ 0.6612,  0.8096,  0.9999],\n",
      "          [ 0.6050,  0.5436,  0.9999]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  0.6430, -1.0000],\n",
      "          [-1.0000,  0.5865,  0.7343],\n",
      "          [-0.6524,  0.6376,  0.6313]],\n",
      "\n",
      "         [[-0.6285, -1.0000, -1.0000],\n",
      "          [-0.5148, -0.5318, -0.5209],\n",
      "          [-0.7763, -0.5325, -0.5255]],\n",
      "\n",
      "         [[-1.0000, -0.9998,  0.5648],\n",
      "          [ 0.5504,  0.6555,  0.8673],\n",
      "          [-0.9182, -0.9999,  0.5732]]]], device='cuda:0')\n",
      "tensor([ 0.5250,  0.9628,  1.0000,  0.7796, -0.5134,  0.5307,  0.7955,  0.5293,\n",
      "         0.9823,  0.5125,  0.9998,  0.8399,  0.9997,  0.9984,  0.5201,  0.7336,\n",
      "         0.9589,  0.5337,  0.5199,  0.5336,  0.5249,  0.9167,  0.9782,  0.5735,\n",
      "         0.8601,  0.9036,  0.5188,  0.5945,  0.9984,  0.5262,  0.9359,  0.5267,\n",
      "         0.5243, -0.5114,  0.9785,  0.8695,  0.9869,  1.0000,  1.0000,  0.5322,\n",
      "         0.5305,  0.5273,  0.7402,  0.5192,  0.5290,  0.9442,  0.8371,  0.5240,\n",
      "         1.0000, -1.0000,  0.5352,  0.5789,  1.0000,  0.9950,  0.6204,  0.6868,\n",
      "         0.9175,  0.5882,  0.6729,  0.8259,  0.5216,  0.5181,  0.5158,  0.5220,\n",
      "         0.5587,  0.9990,  0.5232,  0.5161,  0.5205,  0.9954,  0.5207,  0.5293,\n",
      "         0.8843,  0.6505,  0.9820,  0.5160,  0.6745,  1.0000,  0.5092,  0.6376,\n",
      "         0.5123,  0.5148,  0.5206,  1.0000,  0.5739,  0.7941,  0.5468,  1.0000,\n",
      "         0.9757,  0.9994,  0.8260, -0.5866,  1.0000,  0.5262,  0.5119,  0.5124,\n",
      "         0.5558,  0.8851,  1.0000,  0.5301,  0.6037,  0.9780,  0.5904, -0.6167,\n",
      "         0.5273, -0.5182, -0.5153,  0.6784,  0.5890,  0.5276,  0.9876,  0.5157,\n",
      "         0.5303,  0.5175,  0.5191,  0.8721,  0.5184,  0.5205,  0.9894, -0.5878,\n",
      "         0.5259,  0.5309,  0.9996, -0.5635,  0.9815,  0.5086,  0.5168,  1.0000,\n",
      "         1.0000,  0.5184,  0.5365,  1.0000,  0.8544,  0.9396,  0.9051,  0.5645,\n",
      "         0.5297,  0.5265, -0.5500,  0.5210,  1.0000,  0.5281,  0.5357,  0.8858,\n",
      "         0.7841,  0.9921, -0.5213,  0.5316,  0.5106,  0.5162,  1.0000, -0.5087,\n",
      "         0.5337, -0.5208,  0.9438,  0.5336,  0.5230,  1.0000,  0.5744,  0.5213,\n",
      "         0.5227,  0.7316,  0.9977,  0.5207,  0.5325,  0.5147,  0.5253, -1.0000,\n",
      "         0.5292,  0.5263,  0.5635,  0.5493,  0.5116,  0.5131,  0.5165,  0.5217,\n",
      "         0.5150,  1.0000,  0.4870,  0.8270,  0.7705,  0.9486,  0.5853,  0.8360,\n",
      "         0.9774, -0.5161,  0.8926,  1.0000,  0.5328,  1.0000,  0.9963,  0.5320,\n",
      "         0.5184,  0.5281,  0.5154,  0.5203, -0.5161,  0.5154,  0.5211, -0.5913,\n",
      "        -0.9896,  0.5396,  0.5157,  0.5207,  1.0000,  1.0000,  0.5283,  0.5121,\n",
      "         0.5173,  0.5233,  0.5156,  0.7336,  0.9970,  1.0000,  0.9372,  0.5154,\n",
      "         0.5149,  0.9925,  0.5041, -0.5383,  0.8508,  0.9703, -0.5174,  0.5119,\n",
      "         0.5317,  1.0000,  0.9093,  0.5263,  0.5064,  0.5192,  0.9733,  0.5292,\n",
      "         1.0000,  0.6642,  0.5293,  0.5259,  1.0000,  0.9959,  0.9997,  0.5234,\n",
      "         0.9803,  0.6158,  1.0000,  0.5093,  0.5576,  0.5222,  0.9120,  0.5214,\n",
      "         0.9991,  0.7742,  0.5319,  0.9996,  0.7277,  0.7306,  1.0000,  0.5446,\n",
      "        -1.0000,  0.5228,  0.6726,  0.5574,  0.5154,  0.5170,  0.5168,  0.5206,\n",
      "         0.5137,  0.5445,  0.6098,  0.5741,  0.5319,  0.9798,  0.5797, -1.0000,\n",
      "         0.5020,  0.9484,  0.9936,  0.5157,  0.5386,  0.5292,  0.5213,  0.7331,\n",
      "         0.5288,  0.5383,  1.0000,  0.5187,  0.5352,  0.5200,  0.5353,  0.5181,\n",
      "        -0.5223,  0.5119,  0.5282,  0.9996,  0.5471,  0.5059,  0.5142,  0.5322,\n",
      "         0.8796,  0.5192,  0.6474,  0.5214, -0.5189,  0.5870,  0.5181,  0.5168,\n",
      "        -0.8448, -0.9905,  0.5194,  1.0000, -0.6076,  0.6234,  0.9998,  0.7064,\n",
      "         0.5207,  0.7255,  0.5205,  0.5150,  0.8579,  0.7835,  0.5279,  0.5325,\n",
      "         0.9299,  0.5160,  0.9970,  0.5171,  0.5325,  0.8334,  0.9877,  0.5164,\n",
      "         0.5188,  0.9743,  0.5229,  1.0000,  0.5256,  0.8763,  0.5365, -0.5520,\n",
      "        -0.5290,  0.5200,  0.5241,  0.5144, -0.5228, -1.0000,  0.5148,  0.5321,\n",
      "         0.5927,  0.5135,  0.5171,  0.5239,  0.5319,  0.5171,  1.0000,  0.5265,\n",
      "         0.9212,  0.9986,  0.8903,  0.5180,  0.5649,  0.8332,  0.9165,  1.0000,\n",
      "         0.9987, -0.5176, -0.8194, -0.5324,  0.9829,  0.5480,  1.0000,  1.0000,\n",
      "         0.7472,  0.6667,  0.5177,  1.0000,  0.5220,  0.9109,  0.9023,  0.5139,\n",
      "         0.7481,  0.5177,  1.0000, -0.5657,  1.0000,  0.9560,  0.5206,  1.0000,\n",
      "         0.9771,  0.5178,  0.5300,  0.5174,  0.5186,  0.5302,  0.5242,  0.5178,\n",
      "         0.5326,  0.5217,  0.9799,  0.5317,  0.5150, -0.5340,  0.5309,  0.6495,\n",
      "         0.5245,  0.9723,  0.9753,  0.5275,  0.5180,  0.8209,  0.5153, -0.9907,\n",
      "         0.5145, -0.5138,  0.5120,  0.9981,  0.9759,  0.5271,  1.0000,  0.9956,\n",
      "         0.5253,  1.0000,  1.0000,  0.6516,  0.5136,  0.9905, -0.6281,  0.9913,\n",
      "        -0.5264,  0.5191,  0.5319,  0.8438,  0.5193,  0.8303,  0.5194,  0.5286,\n",
      "         1.0000, -0.5287,  0.5140,  1.0000, -0.5322,  0.6523,  0.9998, -0.7314,\n",
      "         0.9998,  1.0000,  0.5106,  0.9254,  0.6367,  0.5228,  0.9878,  0.5135,\n",
      "         1.0000,  0.5437,  0.9940,  0.6966,  0.5220,  0.5314,  0.5128,  0.9803,\n",
      "         1.0000,  0.5680,  0.7860,  0.5219,  0.5196,  0.5236,  0.9839,  0.9986,\n",
      "         0.9636,  1.0000,  0.9854,  0.9360,  0.9804,  0.5311,  0.9961,  0.9842,\n",
      "         0.9999,  1.0000,  0.9357,  0.5196,  1.0000,  0.5151,  0.5274, -0.5383,\n",
      "         0.5162,  1.0000,  1.0000,  0.6371,  0.9969,  0.5294,  0.5192,  0.9428,\n",
      "         0.5137,  0.5534,  0.9994,  0.8961,  0.9991,  0.5148,  0.5503,  0.5254,\n",
      "         0.5155,  0.9956,  0.5639,  0.9955,  0.9827,  0.5410,  0.9971,  0.5359,\n",
      "         0.7890,  0.6281,  0.5243, -0.5213,  0.5208,  0.5292,  0.5247,  0.9829],\n",
      "       device='cuda:0')\n",
      "tensor([-0.9956, -0.7437, -1.0000, -0.9809,  0.5207, -1.0000, -0.6882, -0.7654,\n",
      "        -0.9874, -1.0000, -0.5169, -0.7951, -0.6932, -0.7938, -0.5285, -0.8243,\n",
      "        -0.9134, -1.0000, -0.9017, -0.9989, -1.0000, -0.8858, -0.5622, -0.9955,\n",
      "        -0.6646, -0.5472, -1.0000, -0.6054, -0.7247, -0.9979, -0.7765, -0.9981,\n",
      "        -0.9996, -0.8197, -0.5662, -0.7758, -0.5229, -1.0000, -1.0000, -0.9993,\n",
      "        -0.5614, -1.0000, -0.5338, -1.0000, -0.8993, -0.5957, -0.5672, -1.0000,\n",
      "        -1.0000, -0.9071, -1.0000, -0.7826, -0.6780, -0.5951,  0.5870, -1.0000,\n",
      "        -0.9161, -0.9915, -0.8366, -0.7921, -1.0000, -0.7836, -0.9969, -0.5314,\n",
      "        -0.9994, -0.7067, -1.0000, -1.0000, -0.6590, -0.6034, -0.9995, -0.9999,\n",
      "        -0.9552, -0.9229, -0.7845, -0.7912, -0.5234, -0.5932, -0.7007, -1.0000,\n",
      "        -0.9995, -0.7957, -1.0000, -0.5218, -0.7008, -0.9710, -0.7028, -0.6717,\n",
      "        -0.5723, -0.7007, -0.7543, -0.7346, -0.7852, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -0.6290, -0.6838, -0.9988, -1.0000, -0.5407, -0.9867, -0.9941,\n",
      "        -1.0000, -0.8219, -0.6763, -0.8756,  0.6417, -1.0000, -0.7056, -1.0000,\n",
      "        -0.6564, -0.6196, -0.8440, -0.9363, -1.0000, -1.0000, -0.7782, -0.9692,\n",
      "        -1.0000, -0.5842, -0.5202, -0.9957, -0.6384, -1.0000, -1.0000, -0.5483,\n",
      "        -1.0000, -1.0000,  0.7862, -1.0000, -0.9893, -0.7258, -0.9978, -0.9994,\n",
      "        -0.6726, -1.0000, -0.9958, -0.6248, -0.8426, -0.9989, -1.0000, -0.6083,\n",
      "        -0.6614, -0.7219, -1.0000, -0.9924, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -0.7512, -0.6402, -0.9986, -0.7087, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -0.8070, -0.8049, -0.9993, -0.9485, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -0.5539, -0.9640, -1.0000, -1.0000, -1.0000, -0.8248,\n",
      "        -1.0000, -1.0000, -0.5797, -0.7161, -0.7159, -0.6253, -1.0000, -0.8560,\n",
      "        -0.9734, -0.6587, -0.5170, -0.8237, -0.8368, -0.6397, -1.0000, -0.6300,\n",
      "        -0.8457, -0.9508, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9975,\n",
      "        -0.8634, -0.9191, -0.8415, -1.0000, -1.0000, -0.6679, -1.0000, -0.9950,\n",
      "        -1.0000, -1.0000, -1.0000, -0.5210, -0.7117, -0.9825, -0.8854, -1.0000,\n",
      "        -0.8772, -0.5360, -1.0000, -0.5509, -0.9443, -0.5997, -1.0000, -0.6800,\n",
      "        -1.0000, -0.7319, -0.8513, -1.0000, -1.0000, -1.0000, -0.7907, -0.8355,\n",
      "        -0.6933, -0.8015, -1.0000, -0.7245, -1.0000, -0.7363, -0.5910, -1.0000,\n",
      "        -0.5286, -0.9862, -1.0000, -1.0000, -0.5973, -1.0000, -0.5358, -0.6686,\n",
      "        -0.8620, -1.0000, -0.8079, -1.0000, -0.5626, -0.6765, -0.8135, -1.0000,\n",
      "        -1.0000, -1.0000, -0.9269, -0.9965, -0.7107, -0.7651, -0.9997, -0.8126,\n",
      "        -0.9004, -0.5483, -0.8202,  0.4383, -1.0000, -0.6487, -0.6657, -1.0000,\n",
      "        -0.5739, -0.9331, -0.7074, -1.0000, -0.9937, -0.5294, -0.9284, -0.8200,\n",
      "        -1.0000, -0.9188, -0.8746, -0.8129, -1.0000, -0.8377, -0.9997, -0.8247,\n",
      "        -0.9999, -1.0000, -0.9986, -0.6407, -0.9514, -0.8540, -1.0000, -1.0000,\n",
      "        -0.7729, -1.0000, -0.5183, -0.8496, -0.6673, -0.5336, -0.5148, -0.9826,\n",
      "        -0.9996, -0.9994, -1.0000, -0.9217, -0.9907, -1.0000, -0.8737, -0.6439,\n",
      "        -1.0000, -0.5151, -0.7292, -0.5319, -0.7744, -0.5907, -0.6441, -0.5210,\n",
      "        -0.5148, -1.0000, -0.5435, -1.0000, -0.9085, -0.9490, -0.7279, -1.0000,\n",
      "        -1.0000, -0.6821, -0.6346, -1.0000, -0.9879, -0.9974, -0.9997, -0.7119,\n",
      "        -0.9971, -0.9977, -0.8896, -0.9223, -1.0000, -0.9994, -1.0000, -0.9152,\n",
      "        -0.9994, -0.9943, -1.0000, -1.0000, -0.8013, -0.5425, -0.6945, -0.9021,\n",
      "        -0.5893, -0.6703, -0.7226,  0.5097, -0.9970, -0.7655, -0.5883, -0.9072,\n",
      "        -0.6678, -0.5824, -0.6969, -0.5382, -0.7907, -0.9369, -0.6384, -1.0000,\n",
      "        -0.8064, -0.9915, -1.0000, -1.0000, -0.6579, -0.7710, -0.7819, -0.9227,\n",
      "        -0.5502, -1.0000, -1.0000, -0.9901, -0.7150, -0.5595, -0.7528, -0.5814,\n",
      "        -0.8226, -0.5258, -0.6491, -1.0000, -1.0000, -0.7016, -1.0000, -1.0000,\n",
      "        -0.6850, -0.9998, -0.7922, -0.6483, -1.0000, -0.9571, -0.9984, -0.7200,\n",
      "        -0.9990, -0.8114, -0.5726, -0.8435, -1.0000, -0.8555, -0.9997, -0.6015,\n",
      "        -1.0000,  0.5742, -1.0000, -0.7527, -0.5208, -1.0000, -0.7779, -0.7609,\n",
      "        -1.0000, -0.5181, -1.0000, -0.5455, -1.0000, -0.6260, -0.8912, -0.9830,\n",
      "        -0.9997, -1.0000, -1.0000, -1.0000, -1.0000, -0.7997, -1.0000, -1.0000,\n",
      "        -0.8725, -0.7544, -1.0000, -1.0000, -0.9619, -0.8281, -0.5901, -0.7572,\n",
      "        -0.7103, -1.0000, -1.0000, -0.9134, -0.5578, -1.0000, -0.5934, -1.0000,\n",
      "        -0.9351, -0.9977, -0.7852, -0.9955, -0.8611, -0.8731, -0.8107, -0.5379,\n",
      "        -1.0000, -0.5319, -0.5955, -0.8793, -1.0000, -0.7307, -0.8180, -0.7374,\n",
      "        -0.6721, -1.0000, -0.7173, -0.7155, -0.9815, -1.0000, -0.5454, -0.5390,\n",
      "        -0.8168, -1.0000, -0.7949, -1.0000, -1.0000, -0.8912, -0.9408, -0.9957,\n",
      "        -1.0000, -0.6995, -0.8944, -0.6131, -0.7433, -0.9989, -1.0000, -0.7164,\n",
      "        -1.0000, -1.0000, -0.5323, -0.8574, -0.6950, -1.0000, -0.8881, -0.8821,\n",
      "        -0.6007, -0.6625, -0.9528, -0.5782, -0.5209, -1.0000, -0.8141, -1.0000,\n",
      "        -0.6949, -0.6277, -0.9995, -0.6330, -1.0000, -1.0000, -1.0000, -0.5355],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5154]],\n",
      "\n",
      "         [[-0.5147]],\n",
      "\n",
      "         [[-0.5151]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5212]],\n",
      "\n",
      "         [[-0.5191]],\n",
      "\n",
      "         [[ 0.5311]]],\n",
      "\n",
      "\n",
      "        [[[-0.5169]],\n",
      "\n",
      "         [[ 0.5319]],\n",
      "\n",
      "         [[ 0.5729]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5147]],\n",
      "\n",
      "         [[ 0.5150]],\n",
      "\n",
      "         [[ 0.5432]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5226]],\n",
      "\n",
      "         [[ 0.5252]],\n",
      "\n",
      "         [[ 0.5220]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5220]],\n",
      "\n",
      "         [[-0.5145]],\n",
      "\n",
      "         [[ 0.5152]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5254]],\n",
      "\n",
      "         [[-0.5193]],\n",
      "\n",
      "         [[-0.5230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8764]],\n",
      "\n",
      "         [[-0.5261]],\n",
      "\n",
      "         [[ 0.5255]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5314]],\n",
      "\n",
      "         [[-0.5198]],\n",
      "\n",
      "         [[-0.5179]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5153]],\n",
      "\n",
      "         [[-0.5304]],\n",
      "\n",
      "         [[ 0.5193]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5247]],\n",
      "\n",
      "         [[ 0.5176]],\n",
      "\n",
      "         [[-0.5325]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8758]],\n",
      "\n",
      "         [[ 0.7169]],\n",
      "\n",
      "         [[ 0.5147]]]], device='cuda:0')\n",
      "tensor([ 0.9768,  0.5837,  0.4872,  0.5589,  0.8308, -0.5306,  0.9512, -0.5232,\n",
      "         0.5941,  1.0000,  0.5173,  0.5168,  0.5201,  0.5259,  0.9376,  0.6231,\n",
      "         0.5237,  0.9998,  0.9999,  1.0000,  1.0000,  0.5930,  0.9844,  1.0000,\n",
      "         0.5410,  0.7509,  1.0000,  0.8770,  0.5399, -0.6261, -0.5259,  0.9985,\n",
      "         0.6001,  0.9696,  0.6930,  0.5233,  0.5378, -0.5155, -0.5116,  0.9966,\n",
      "         0.5210,  1.0000,  0.8238,  1.0000,  1.0000,  0.5347,  0.5326,  1.0000,\n",
      "        -1.0000,  0.9261, -0.9235, -0.5150,  0.5755,  0.5418,  0.9937,  0.5287,\n",
      "         0.5305,  1.0000, -0.5238,  0.6369,  1.0000,  0.9997,  0.5412, -0.5203,\n",
      "         0.9988,  0.5250,  1.0000,  1.0000,  0.5287,  0.5233,  1.0000, -0.5325,\n",
      "         0.5188,  0.9776,  0.5312,  0.6710,  0.9858,  0.5912,  1.0000, -0.6097,\n",
      "         0.9975,  0.5202,  1.0000,  0.9421,  0.7669,  0.5791, -0.6232,  0.5131,\n",
      "         0.5170,  0.5250,  0.5151,  0.5814,  0.5475,  1.0000,  1.0000,  0.9975,\n",
      "        -0.5255,  0.5160,  0.5504,  0.9953,  0.6402,  0.7250,  0.9598, -0.5310,\n",
      "         1.0000,  0.7387,  0.8900, -0.5242,  1.0000, -0.5280,  0.5150, -1.0000,\n",
      "         0.5507,  0.7346, -0.5311,  0.6032,  1.0000,  1.0000,  0.5302,  0.9999,\n",
      "         1.0000,  0.5153,  0.5183, -0.9953,  0.5519,  1.0000,  1.0000,  0.5147,\n",
      "        -0.5354,  1.0000,  0.9754, -0.5120,  0.5268,  0.5437,  0.5148,  0.9803,\n",
      "         0.5157,  1.0000,  1.0000,  0.6798,  0.5293,  0.9982,  1.0000, -0.5280,\n",
      "        -0.5409,  0.5627,  0.9996,  0.7086, -0.9884,  1.0000,  0.5169,  1.0000,\n",
      "         0.9989,  1.0000,  0.5220,  0.9968,  1.0000,  0.5326,  0.5186,  1.0000,\n",
      "         1.0000,  0.7646,  0.5224,  0.9992, -0.5225,  1.0000,  0.7090,  1.0000,\n",
      "        -1.0000,  1.0000,  0.9920, -0.5210,  0.9971,  1.0000,  1.0000,  1.0000,\n",
      "         0.5249, -0.5078,  0.9732, -0.5319, -0.5220,  0.5472, -1.0000,  0.5188,\n",
      "         0.5193,  0.5608,  0.5327,  0.5007,  0.8240,  0.5524,  0.4945, -0.5194,\n",
      "         0.9002,  0.9971,  0.5273,  1.0000,  0.7387,  1.0000,  1.0000, -0.9950,\n",
      "         0.9872,  0.9995,  0.5174,  0.9994,  0.5062,  0.5147,  1.0000,  1.0000,\n",
      "         0.7416,  1.0000,  1.0000,  0.9689,  0.5231,  0.5031,  0.5237,  1.0000,\n",
      "        -0.5856,  0.5185,  1.0000,  0.7995,  0.5150,  0.5200,  1.0000,  1.0000,\n",
      "         1.0000,  0.5189,  0.5274,  1.0000,  1.0000,  1.0000,  0.5341,  0.5210,\n",
      "         0.5424,  0.5165,  1.0000, -0.5147,  0.4984,  0.5211,  0.5228,  1.0000,\n",
      "         0.5149,  0.9757, -0.5029,  1.0000, -0.5224,  1.0000,  0.5169, -0.5326,\n",
      "         0.5241,  0.5149, -0.5217,  0.5199,  0.5433,  0.9430,  0.5228,  0.9973,\n",
      "         1.0000,  1.0000, -0.5234,  1.0000,  1.0000,  0.5164,  0.9971,  0.9756,\n",
      "         1.0000,  1.0000, -0.5195,  0.9683,  0.5319,  0.5362,  0.9999,  1.0000,\n",
      "         1.0000,  0.5331,  0.5181,  1.0000,  1.0000,  0.5161,  1.0000,  0.6262,\n",
      "         1.0000,  0.9986,  0.5094,  1.0000,  0.9933,  0.8545,  0.9954,  0.5149,\n",
      "        -0.5276, -0.6487, -0.5208,  0.5193,  0.9944,  1.0000,  0.9985,  1.0000,\n",
      "         0.5573,  1.0000,  0.7080,  1.0000, -0.5278,  0.7317,  0.5479,  1.0000,\n",
      "         0.8285,  0.9871,  1.0000,  0.5272, -0.9974,  0.5852,  0.5510,  0.8264,\n",
      "         1.0000,  0.9564,  0.8238,  0.8148,  0.5193, -0.5188,  0.9971,  0.5227,\n",
      "         0.5631,  1.0000,  0.5670, -1.0000,  0.7431,  0.5503, -0.5325,  1.0000,\n",
      "         1.0000,  0.5244,  0.5618,  0.5266,  0.9976,  0.5232,  0.9985,  1.0000,\n",
      "         0.9988,  0.6475,  1.0000,  1.0000,  0.7603, -0.4999,  0.5323,  1.0000,\n",
      "         0.9973,  1.0000, -0.5945,  1.0000,  0.7115,  0.5157,  0.5280,  0.8685,\n",
      "         0.5147,  0.5476,  0.9507,  0.9670,  0.9864,  0.6443,  0.5535,  0.5150,\n",
      "         0.5310,  0.9224,  0.9161,  0.9755,  0.5893,  0.9889,  0.5258,  0.5212,\n",
      "         0.5190,  0.9800, -1.0000, -0.5196,  0.5158,  0.5748, -0.8614,  1.0000,\n",
      "         0.8091,  1.0000,  0.5220,  1.0000,  0.5267,  0.5332,  1.0000,  0.5060,\n",
      "        -0.9735,  0.5241,  0.5232,  1.0000,  1.0000,  0.6690,  0.9989,  1.0000,\n",
      "         1.0000,  0.9996,  0.5799,  0.7191,  1.0000,  0.5272,  0.9963, -0.5149,\n",
      "        -0.7878,  0.5194,  0.5218, -0.5280,  1.0000,  0.5137,  0.9949,  0.5257,\n",
      "         1.0000,  1.0000,  0.9876,  0.5284,  0.5431,  1.0000,  0.5067,  0.5149,\n",
      "         1.0000,  0.5645, -0.5059,  0.5298,  1.0000,  0.5160,  0.9688,  0.5069,\n",
      "         0.9992,  1.0000, -0.8939,  0.5003,  1.0000,  0.5217,  0.8411,  1.0000,\n",
      "         0.5216,  1.0000, -1.0000,  0.5094,  1.0000,  0.5770,  0.5165,  0.5433,\n",
      "         0.5198,  0.4855,  1.0000,  0.5567,  1.0000,  1.0000,  0.5300,  0.9998,\n",
      "         0.5145,  0.9945,  0.5229, -0.8033, -0.5324,  1.0000,  1.0000,  0.5978,\n",
      "         0.5233,  0.5771,  0.8033, -0.5276,  1.0000, -0.5279,  0.5277,  0.5535,\n",
      "         0.5836, -0.4956, -0.7803, -0.5325,  0.5141,  1.0000,  0.5301,  0.6824,\n",
      "         0.5321,  0.5215,  0.5570,  1.0000,  0.5096,  1.0000, -0.5321, -0.9895,\n",
      "         1.0000,  0.5299, -0.5229,  0.7649,  0.5325,  0.9971,  0.9988, -0.9682,\n",
      "         1.0000,  1.0000,  0.5442,  0.5199,  0.5174,  0.9987, -0.5171,  0.8424,\n",
      "         0.7474,  0.5354,  0.9745,  0.5173,  0.5310,  1.0000,  0.5310,  1.0000,\n",
      "        -0.5188,  0.5162, -0.5226, -0.5324,  1.0000,  0.9999,  1.0000,  0.6326],\n",
      "       device='cuda:0')\n",
      "tensor([-0.9956, -0.7437, -1.0000, -0.9809,  0.5207, -1.0000, -0.6882, -0.7654,\n",
      "        -0.9874, -1.0000, -0.5169, -0.7951, -0.6932, -0.7938, -0.5285, -0.8243,\n",
      "        -0.9134, -1.0000, -0.9017, -0.9989, -1.0000, -0.8858, -0.5622, -0.9955,\n",
      "        -0.6646, -0.5472, -1.0000, -0.6054, -0.7247, -0.9979, -0.7765, -0.9981,\n",
      "        -0.9996, -0.8197, -0.5662, -0.7758, -0.5229, -1.0000, -1.0000, -0.9993,\n",
      "        -0.5614, -1.0000, -0.5338, -1.0000, -0.8993, -0.5957, -0.5672, -1.0000,\n",
      "        -1.0000, -0.9071, -1.0000, -0.7826, -0.6780, -0.5951,  0.5870, -1.0000,\n",
      "        -0.9161, -0.9915, -0.8366, -0.7921, -1.0000, -0.7836, -0.9969, -0.5314,\n",
      "        -0.9994, -0.7067, -1.0000, -1.0000, -0.6590, -0.6034, -0.9995, -0.9999,\n",
      "        -0.9552, -0.9229, -0.7845, -0.7912, -0.5234, -0.5932, -0.7007, -1.0000,\n",
      "        -0.9995, -0.7957, -1.0000, -0.5218, -0.7008, -0.9710, -0.7028, -0.6717,\n",
      "        -0.5723, -0.7007, -0.7543, -0.7346, -0.7852, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -0.6290, -0.6838, -0.9988, -1.0000, -0.5407, -0.9867, -0.9941,\n",
      "        -1.0000, -0.8219, -0.6763, -0.8756,  0.6417, -1.0000, -0.7056, -1.0000,\n",
      "        -0.6564, -0.6196, -0.8440, -0.9363, -1.0000, -1.0000, -0.7782, -0.9692,\n",
      "        -1.0000, -0.5842, -0.5202, -0.9957, -0.6384, -1.0000, -1.0000, -0.5483,\n",
      "        -1.0000, -1.0000,  0.7862, -1.0000, -0.9893, -0.7258, -0.9978, -0.9994,\n",
      "        -0.6726, -1.0000, -0.9958, -0.6248, -0.8426, -0.9989, -1.0000, -0.6083,\n",
      "        -0.6614, -0.7219, -1.0000, -0.9924, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -0.7512, -0.6402, -0.9986, -0.7087, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -0.8070, -0.8049, -0.9993, -0.9485, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -0.5539, -0.9640, -1.0000, -1.0000, -1.0000, -0.8248,\n",
      "        -1.0000, -1.0000, -0.5797, -0.7161, -0.7159, -0.6253, -1.0000, -0.8560,\n",
      "        -0.9734, -0.6587, -0.5170, -0.8237, -0.8368, -0.6397, -1.0000, -0.6300,\n",
      "        -0.8457, -0.9508, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9975,\n",
      "        -0.8634, -0.9191, -0.8415, -1.0000, -1.0000, -0.6679, -1.0000, -0.9950,\n",
      "        -1.0000, -1.0000, -1.0000, -0.5210, -0.7117, -0.9825, -0.8854, -1.0000,\n",
      "        -0.8772, -0.5360, -1.0000, -0.5509, -0.9443, -0.5997, -1.0000, -0.6800,\n",
      "        -1.0000, -0.7319, -0.8513, -1.0000, -1.0000, -1.0000, -0.7907, -0.8355,\n",
      "        -0.6933, -0.8015, -1.0000, -0.7245, -1.0000, -0.7363, -0.5910, -1.0000,\n",
      "        -0.5286, -0.9862, -1.0000, -1.0000, -0.5973, -1.0000, -0.5358, -0.6686,\n",
      "        -0.8620, -1.0000, -0.8079, -1.0000, -0.5626, -0.6765, -0.8135, -1.0000,\n",
      "        -1.0000, -1.0000, -0.9269, -0.9965, -0.7107, -0.7651, -0.9997, -0.8126,\n",
      "        -0.9004, -0.5483, -0.8202,  0.4383, -1.0000, -0.6487, -0.6657, -1.0000,\n",
      "        -0.5739, -0.9331, -0.7074, -1.0000, -0.9937, -0.5294, -0.9284, -0.8200,\n",
      "        -1.0000, -0.9188, -0.8746, -0.8129, -1.0000, -0.8377, -0.9997, -0.8247,\n",
      "        -0.9999, -1.0000, -0.9986, -0.6407, -0.9514, -0.8540, -1.0000, -1.0000,\n",
      "        -0.7729, -1.0000, -0.5183, -0.8496, -0.6673, -0.5336, -0.5148, -0.9826,\n",
      "        -0.9996, -0.9994, -1.0000, -0.9217, -0.9907, -1.0000, -0.8737, -0.6439,\n",
      "        -1.0000, -0.5151, -0.7292, -0.5319, -0.7744, -0.5907, -0.6441, -0.5210,\n",
      "        -0.5148, -1.0000, -0.5435, -1.0000, -0.9085, -0.9490, -0.7279, -1.0000,\n",
      "        -1.0000, -0.6821, -0.6346, -1.0000, -0.9879, -0.9974, -0.9997, -0.7119,\n",
      "        -0.9971, -0.9977, -0.8896, -0.9223, -1.0000, -0.9994, -1.0000, -0.9152,\n",
      "        -0.9994, -0.9943, -1.0000, -1.0000, -0.8013, -0.5425, -0.6945, -0.9021,\n",
      "        -0.5893, -0.6703, -0.7226,  0.5097, -0.9970, -0.7655, -0.5883, -0.9072,\n",
      "        -0.6678, -0.5824, -0.6969, -0.5382, -0.7907, -0.9369, -0.6384, -1.0000,\n",
      "        -0.8064, -0.9915, -1.0000, -1.0000, -0.6579, -0.7710, -0.7819, -0.9227,\n",
      "        -0.5502, -1.0000, -1.0000, -0.9901, -0.7150, -0.5595, -0.7528, -0.5814,\n",
      "        -0.8226, -0.5258, -0.6491, -1.0000, -1.0000, -0.7016, -1.0000, -1.0000,\n",
      "        -0.6850, -0.9998, -0.7922, -0.6483, -1.0000, -0.9571, -0.9984, -0.7200,\n",
      "        -0.9990, -0.8114, -0.5726, -0.8435, -1.0000, -0.8555, -0.9997, -0.6015,\n",
      "        -1.0000,  0.5742, -1.0000, -0.7527, -0.5208, -1.0000, -0.7779, -0.7609,\n",
      "        -1.0000, -0.5181, -1.0000, -0.5455, -1.0000, -0.6260, -0.8912, -0.9830,\n",
      "        -0.9997, -1.0000, -1.0000, -1.0000, -1.0000, -0.7997, -1.0000, -1.0000,\n",
      "        -0.8725, -0.7544, -1.0000, -1.0000, -0.9619, -0.8281, -0.5901, -0.7572,\n",
      "        -0.7103, -1.0000, -1.0000, -0.9134, -0.5578, -1.0000, -0.5934, -1.0000,\n",
      "        -0.9351, -0.9977, -0.7852, -0.9955, -0.8611, -0.8731, -0.8107, -0.5379,\n",
      "        -1.0000, -0.5319, -0.5955, -0.8793, -1.0000, -0.7307, -0.8180, -0.7374,\n",
      "        -0.6721, -1.0000, -0.7173, -0.7155, -0.9815, -1.0000, -0.5454, -0.5390,\n",
      "        -0.8168, -1.0000, -0.7949, -1.0000, -1.0000, -0.8912, -0.9408, -0.9957,\n",
      "        -1.0000, -0.6995, -0.8944, -0.6131, -0.7433, -0.9989, -1.0000, -0.7164,\n",
      "        -1.0000, -1.0000, -0.5323, -0.8574, -0.6950, -1.0000, -0.8881, -0.8821,\n",
      "        -0.6007, -0.6625, -0.9528, -0.5782, -0.5209, -1.0000, -0.8141, -1.0000,\n",
      "        -0.6949, -0.6277, -0.9995, -0.6330, -1.0000, -1.0000, -1.0000, -0.5355],\n",
      "       device='cuda:0')\n",
      "tensor([[[[-1.0000, -1.0000,  0.9607],\n",
      "          [ 0.5621,  0.5221,  0.5225],\n",
      "          [ 0.5363, -0.8805, -1.0000]],\n",
      "\n",
      "         [[ 0.8984, -1.0000,  0.5740],\n",
      "          [ 0.5322,  0.6987,  1.0000],\n",
      "          [-1.0000, -0.7895,  1.0000]],\n",
      "\n",
      "         [[ 0.5297,  0.5150,  0.5175],\n",
      "          [-1.0000,  0.9425,  0.9534],\n",
      "          [-0.5259, -0.5293, -0.5153]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7287,  0.7495,  0.9999],\n",
      "          [ 0.9965,  0.5247, -1.0000],\n",
      "          [-0.7718, -1.0000, -0.5205]],\n",
      "\n",
      "         [[ 0.5171,  0.5325,  0.5195],\n",
      "          [ 0.5323,  0.5209,  0.5325],\n",
      "          [ 0.5462,  0.5182,  0.5213]],\n",
      "\n",
      "         [[ 0.7865, -1.0000,  0.8725],\n",
      "          [-1.0000, -0.9999, -0.5806],\n",
      "          [-1.0000, -0.6897, -0.9999]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000,  0.7190,  0.5202],\n",
      "          [ 0.7961,  0.5165,  0.5326],\n",
      "          [ 0.5254,  0.5184,  0.5379]],\n",
      "\n",
      "         [[ 1.0000,  0.9999, -1.0000],\n",
      "          [-0.8759, -0.5175, -1.0000],\n",
      "          [-0.5262, -0.5202, -0.5174]],\n",
      "\n",
      "         [[ 0.6172,  0.5929,  0.5236],\n",
      "          [-0.5209, -0.5189, -0.5513],\n",
      "          [-0.5229, -0.5155, -0.5295]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6589,  0.9997, -1.0000],\n",
      "          [-0.5353,  0.9945,  0.9946],\n",
      "          [ 0.9996, -0.6097,  0.9981]],\n",
      "\n",
      "         [[ 0.9999,  0.5170,  0.5155],\n",
      "          [ 0.5245,  0.5202,  0.5183],\n",
      "          [ 0.7982,  0.5166,  0.5204]],\n",
      "\n",
      "         [[ 0.9997,  0.8069,  0.9363],\n",
      "          [ 0.9999,  0.9999,  0.7114],\n",
      "          [ 0.8599, -0.9999,  0.9999]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5183,  0.5508,  0.5163],\n",
      "          [ 0.5158,  0.5286,  0.5187],\n",
      "          [-1.0000,  0.5778,  0.5222]],\n",
      "\n",
      "         [[ 0.7863, -0.5754, -1.0000],\n",
      "          [-0.9700, -0.7104, -0.5312],\n",
      "          [-0.6081, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 0.5326,  0.5285,  0.5170],\n",
      "          [-0.5203, -0.5335, -0.5165],\n",
      "          [-0.5323, -0.5263, -0.5218]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6425,  0.8747, -1.0000],\n",
      "          [-0.6624,  0.5315, -1.0000],\n",
      "          [-1.0000, -0.7369, -0.9789]],\n",
      "\n",
      "         [[ 0.5170,  0.5323,  0.5278],\n",
      "          [ 0.5305,  0.5325,  0.5312],\n",
      "          [-0.5805, -0.6179,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -0.5937, -1.0000],\n",
      "          [-0.5177,  0.9996, -0.8331],\n",
      "          [-0.5245, -1.0000, -0.6342]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5319,  0.5175,  0.5166],\n",
      "          [ 0.5285,  0.5261,  0.5325],\n",
      "          [ 0.6096,  0.6189,  0.5204]],\n",
      "\n",
      "         [[ 1.0000, -0.5263, -0.5212],\n",
      "          [-0.5687, -1.0000, -0.6498],\n",
      "          [-1.0000, -0.5803, -0.5773]],\n",
      "\n",
      "         [[ 0.5198,  0.5221,  0.5308],\n",
      "          [ 0.5491,  0.6319,  0.9801],\n",
      "          [-0.5275, -0.5167, -0.5287]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5830,  0.9999, -0.5445],\n",
      "          [ 0.5278,  0.5147,  0.5888],\n",
      "          [-0.5187, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 0.5267,  0.5257,  0.5147],\n",
      "          [ 0.5176,  0.5213,  0.5309],\n",
      "          [ 0.5676,  0.5303,  0.5241]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  0.7228],\n",
      "          [ 0.9998, -1.0000, -0.7820],\n",
      "          [-0.8436, -0.7524, -0.5420]]],\n",
      "\n",
      "\n",
      "        [[[-0.5236, -0.5200, -0.5257],\n",
      "          [-0.5274, -0.5315, -0.5318],\n",
      "          [ 1.0000,  0.7907,  0.7254]],\n",
      "\n",
      "         [[-1.0000, -0.8919,  0.5833],\n",
      "          [-0.6602,  0.5963, -0.6579],\n",
      "          [ 1.0000, -0.9018,  0.9263]],\n",
      "\n",
      "         [[-0.5198, -0.5319, -0.5194],\n",
      "          [-0.9985,  0.8304,  1.0000],\n",
      "          [-0.5296, -0.5317, -0.5298]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5262,  1.0000, -0.6608],\n",
      "          [ 0.5247,  0.5235,  0.6862],\n",
      "          [ 0.5149,  0.5298, -0.9999]],\n",
      "\n",
      "         [[-0.5166, -0.5264, -0.5292],\n",
      "          [-0.6838, -1.0000, -0.5157],\n",
      "          [-0.5284, -0.5323, -0.5155]],\n",
      "\n",
      "         [[ 0.9999,  1.0000, -0.6204],\n",
      "          [ 0.7210,  1.0000,  0.9711],\n",
      "          [-0.5240, -0.5212, -0.5293]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5195,  0.5251,  0.5178],\n",
      "          [ 0.5274,  0.5261,  0.5154],\n",
      "          [-0.5309,  0.7070,  0.5298]],\n",
      "\n",
      "         [[-0.6960,  1.0000, -1.0000],\n",
      "          [-0.7523, -0.5318, -1.0000],\n",
      "          [ 1.0000, -1.0000, -0.5194]],\n",
      "\n",
      "         [[ 0.5351,  0.5183,  0.7142],\n",
      "          [-0.5214, -0.5317, -0.5303],\n",
      "          [-0.5254, -0.5323, -0.5195]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9543, -1.0000, -0.7069],\n",
      "          [-0.9398,  0.5974,  1.0000],\n",
      "          [ 0.7398,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.5148,  0.5229,  0.5257],\n",
      "          [ 0.5249,  0.5255,  0.5299],\n",
      "          [ 0.9998, -0.6058,  0.7619]],\n",
      "\n",
      "         [[ 1.0000, -0.9473, -1.0000],\n",
      "          [ 1.0000, -0.9999, -1.0000],\n",
      "          [-1.0000, -0.5169, -0.5609]]]], device='cuda:0')\n",
      "tensor([0.5479, 0.8678, 0.6034, 0.5408, 0.8890, 0.9974, 0.5597, 0.5143, 0.5162,\n",
      "        1.0000, 0.5147, 0.8608, 0.9017, 0.9467, 0.9949, 0.8242, 0.5069, 0.5123,\n",
      "        0.5249, 0.5191, 0.5606, 0.7886, 0.8059, 0.5217, 0.6536, 0.7470, 0.9147,\n",
      "        0.9340, 0.5288, 0.8302, 0.8813, 0.9831, 0.9619, 0.9902, 0.5815, 0.9487,\n",
      "        0.9997, 0.6463, 0.9553, 0.5148, 0.9814, 0.7712, 0.6934, 0.5128, 0.9785,\n",
      "        1.0000, 0.5549, 0.5416, 0.6972, 0.9655, 0.9608, 0.5241, 0.8997, 0.9903,\n",
      "        0.5210, 0.5310, 0.9884, 1.0000, 0.5181, 0.9964, 0.9653, 0.8630, 0.7452,\n",
      "        0.5563, 1.0000, 0.5117, 0.9235, 0.9466, 0.5195, 0.7834, 0.9457, 0.9325,\n",
      "        0.5656, 0.5182, 0.7312, 0.7922, 0.8876, 0.5899, 0.9148, 0.7467, 0.7471,\n",
      "        0.5238, 0.8606, 0.9704, 0.9994, 0.5162, 0.9966, 1.0000, 0.5150, 0.9281,\n",
      "        1.0000, 0.8880, 0.5510, 0.9983, 1.0000, 0.6225, 0.5155, 0.9475, 0.5774,\n",
      "        0.9936, 1.0000, 0.5466, 0.5249, 0.5869, 0.9243, 0.7186, 0.5200, 0.6369,\n",
      "        0.9950, 0.5285, 0.5211, 0.6442, 0.6409, 0.6837, 0.5989, 1.0000, 0.9812,\n",
      "        0.9544, 1.0000, 0.8251, 0.5241, 0.9998, 0.9997, 0.6237, 0.9712, 0.9591,\n",
      "        0.5343, 0.9878, 0.9836, 0.9587, 0.7007, 0.5480, 0.9448, 0.5267, 0.9975,\n",
      "        0.7091, 0.5181, 0.8397, 0.9693, 0.6055, 0.5328, 1.0000, 0.9800, 0.9994,\n",
      "        1.0000, 0.9972, 0.9610, 0.5355, 0.5380, 0.9627, 0.5608, 0.9931, 0.9923,\n",
      "        0.9194, 0.5214, 0.9798, 0.9999, 0.5192, 0.9893, 0.9646, 1.0000, 0.5336,\n",
      "        0.6007, 0.5254, 0.9978, 0.9913, 0.9259, 0.7417, 0.9981, 0.8716, 0.5184,\n",
      "        0.5244, 0.9696, 0.9983, 0.5200, 0.5686, 0.9967, 0.9936, 0.5622, 1.0000,\n",
      "        0.8814, 0.5118, 0.9707, 1.0000, 0.5288, 0.7807, 0.6729, 0.9815, 0.9643,\n",
      "        0.7667, 0.6029, 0.9349, 0.9263, 0.6772, 0.6308, 0.5165, 0.5539, 0.9621,\n",
      "        0.6170, 0.9191, 0.9157, 0.9921, 0.5901, 0.9154, 0.5317, 1.0000, 0.7583,\n",
      "        1.0000, 0.9785, 0.6056, 0.9364, 0.9854, 0.9644, 0.8534, 0.5183, 0.9436,\n",
      "        0.9879, 0.7294, 0.9638, 0.9467, 0.9859, 0.9500, 0.5128, 0.5411, 0.9270,\n",
      "        0.5888, 0.9084, 0.9155, 0.9505, 0.9957, 0.9772, 0.8588, 0.9146, 0.8710,\n",
      "        0.9893, 0.7227, 0.9700, 0.7872, 0.5133, 0.6122, 1.0000, 0.9931, 0.9787,\n",
      "        0.7483, 0.9566, 1.0000, 0.9903, 0.5192, 0.8199, 0.8995, 0.8344, 0.5229,\n",
      "        0.5400, 0.9343, 0.5189, 0.5773, 0.9828, 0.5067, 0.5070, 0.5238, 0.9742,\n",
      "        0.9887, 0.9779, 0.5185, 0.9498, 0.6721, 1.0000, 1.0000, 0.9055, 0.5200,\n",
      "        1.0000, 0.9338, 0.8586, 0.5790, 0.6032, 0.9993, 0.6905, 0.8674, 0.5647,\n",
      "        0.9779, 0.8434, 0.9995, 0.9798, 0.7053, 0.9814, 0.9893, 0.9977, 0.9120,\n",
      "        1.0000, 0.5577, 1.0000, 0.5134, 0.9403, 0.7074, 1.0000, 0.9793, 0.9935,\n",
      "        0.9739, 0.8866, 0.9486, 0.5376, 0.8109, 0.6492, 0.9392, 1.0000, 0.9982,\n",
      "        1.0000, 0.9649, 1.0000, 0.5401, 0.5290, 0.9995, 0.9519, 0.9427, 0.9668,\n",
      "        0.6499, 0.5297, 0.5215, 0.9917, 0.5179, 0.9481, 0.7323, 0.9493, 0.7424,\n",
      "        0.6917, 0.6219, 0.6425, 0.9602, 0.9378, 0.6475, 1.0000, 0.9949, 0.9036,\n",
      "        0.8188, 1.0000, 0.5850, 0.9917, 0.5148, 0.6959, 0.5014, 1.0000, 0.9815,\n",
      "        0.9928, 0.9977, 0.9792, 0.6870, 0.8834, 0.5878, 0.9772, 0.9678, 0.7696,\n",
      "        0.5176, 0.5130, 0.5168, 0.6356, 0.7799, 0.5282, 1.0000, 0.5296, 0.9813,\n",
      "        0.9271, 0.5279, 0.9607, 0.9108, 0.8122, 0.8806, 0.5222, 0.9235, 0.9995,\n",
      "        0.6211, 0.7434, 0.9884, 0.5365, 0.9969, 0.9719, 0.9512, 0.9082, 0.7223,\n",
      "        0.9479, 0.8596, 0.9909, 0.9914, 0.9999, 0.5279, 0.9100, 0.5746, 0.7713,\n",
      "        0.8996, 0.5515, 0.5413, 0.8617, 0.5331, 0.5235, 0.9505, 0.9415, 0.5471,\n",
      "        0.5331, 0.5240, 0.7737, 0.7091, 0.9805, 0.9998, 0.9415, 0.6230, 0.5209,\n",
      "        0.9578, 0.9904, 0.9959, 0.9672, 0.8789, 0.5697, 1.0000, 0.9460, 0.5406,\n",
      "        0.9678, 0.6734, 0.5619, 0.6148, 0.5422, 0.9657, 0.9451, 0.9701, 0.9905,\n",
      "        1.0000, 0.9785, 0.9228, 0.9986, 0.7200, 0.9879, 0.5139, 0.9898, 1.0000,\n",
      "        0.5504, 1.0000, 0.9576, 0.5472, 1.0000, 0.5262, 0.5147, 0.8209, 0.9888,\n",
      "        0.5142, 0.6361, 0.6437, 0.5477, 0.9824, 0.5605, 0.5120, 0.9337, 0.5226,\n",
      "        0.9983, 0.8638, 0.9661, 0.7809, 0.5273, 0.5293, 0.5852, 0.9998, 0.9822,\n",
      "        0.9755, 0.9990, 0.7191, 0.9943, 0.9835, 0.9843, 0.7608, 0.9719, 0.5327,\n",
      "        0.9901, 0.9912, 0.9734, 0.8260, 0.5241, 0.9989, 0.9734, 0.8733, 0.8167,\n",
      "        1.0000, 0.9331, 0.6324, 0.5491, 0.8337, 0.7158, 0.6199, 0.5180, 0.8719,\n",
      "        0.5153, 0.9754, 0.5285, 0.5714, 0.9300, 0.5147, 0.5210, 0.9694, 0.9991,\n",
      "        0.6261, 1.0000, 0.5126, 0.5915, 0.7649, 0.7928, 0.5420, 0.5275, 1.0000,\n",
      "        0.9088, 0.9643, 1.0000, 0.8872, 0.5258, 0.5224, 0.9969, 0.8173],\n",
      "       device='cuda:0')\n",
      "tensor([-0.5205, -0.5681, -0.5191, -0.9948, -0.5770, -0.5284, -0.8644, -0.9078,\n",
      "        -1.0000, -0.5249, -1.0000, -0.7937, -0.6442, -0.5668, -0.5442, -0.5952,\n",
      "        -0.8049, -1.0000, -0.9040, -0.9758, -0.9797, -0.5999, -0.5492, -0.6796,\n",
      "        -0.9224, -0.7436, -0.6685, -0.5693, -0.9958, -0.5430, -0.5904, -0.5440,\n",
      "        -0.8369, -0.6357,  0.5916, -0.5284, -0.5318, -0.5328, -0.7214, -1.0000,\n",
      "        -0.5188, -0.6594, -0.8178, -0.5696, -0.5301, -0.5226,  0.5625, -0.6098,\n",
      "        -0.8776, -0.6022, -0.5380, -0.9990, -0.6245, -0.5215, -0.8952, -0.9964,\n",
      "        -0.5524, -0.5155, -1.0000, -0.5274, -0.5534, -0.6643, -0.5549, -0.9926,\n",
      "        -0.5107, -0.9279, -0.6333, -0.8787, -0.9992, -0.6025, -0.5612, -0.5938,\n",
      "        -0.7585, -0.9992, -0.5567, -0.5193, -0.5299, -0.5356, -0.5516, -0.7285,\n",
      "        -0.5409, -0.9973, -0.5162, -0.5428, -0.5291, -0.9023, -0.5232, -0.5161,\n",
      "        -0.5648, -0.5797, -0.5139, -0.5903, -0.9235, -0.5181, -0.5286, -0.5597,\n",
      "        -0.9907, -0.5445, -0.6524, -0.5281, -0.5157, -0.8046, -0.9997, -0.6576,\n",
      "        -0.5202, -0.6454, -0.9803, -0.8053, -0.5307, -0.9541, -0.9970, -0.8184,\n",
      "        -0.5888, -0.5139, -0.5393, -0.5151, -0.5255, -0.5485, -0.5125, -0.5967,\n",
      "        -0.9891, -0.5107, -0.5591, -0.5373, -0.5299, -0.5472, -0.9944, -0.5265,\n",
      "        -0.5776, -0.5229, -0.5175, -0.9852, -0.5690, -0.9149, -0.5160, -0.9733,\n",
      "        -0.8883, -0.7058, -0.5126, -0.7002, -0.5988, -0.5276, -0.5501, -0.5163,\n",
      "        -0.6238, -0.5156, -0.5410, -0.9939, -0.9359, -0.5602, -0.5340, -0.5440,\n",
      "        -0.5326, -0.5387, -0.8187, -0.5165, -0.5118, -0.6317, -0.5441, -0.5267,\n",
      "        -0.5135, -0.6972, -0.6045, -0.9983, -0.5219, -0.5328, -0.5628, -0.5439,\n",
      "        -0.5211, -0.9960, -0.9988, -0.9965, -0.5053, -0.5158, -0.9993, -0.6660,\n",
      "        -0.5343, -0.5179, -0.9735, -0.5186, -0.7787, -0.9992, -0.5309, -0.5141,\n",
      "        -0.9985, -0.6459, -0.6618, -0.5186, -0.5126, -0.7356, -0.6973, -0.5175,\n",
      "        -0.6009, -0.6020, -0.7131, -1.0000, -0.9802, -0.5303, -0.6045, -0.5322,\n",
      "        -0.5947, -0.5123,  0.5193, -0.5770, -0.8761, -0.5144, -0.5483, -0.5126,\n",
      "        -0.5484, -0.5403, -0.5119, -0.5373, -0.5302, -0.6550, -0.8377, -0.5489,\n",
      "        -0.5681, -0.5636, -0.5599, -0.5792, -0.5957, -0.5410, -0.5198, -0.7748,\n",
      "         0.5441, -0.5473, -0.5576, -0.5221, -0.6881, -0.5320, -0.5613, -0.6129,\n",
      "        -0.5335, -0.5402, -0.5118, -0.5416, -0.7249, -0.8498, -1.0000, -0.6729,\n",
      "        -0.5441, -0.5202, -0.6132, -0.5831, -0.5129, -0.5100, -0.5307, -0.9963,\n",
      "        -0.5792, -0.5774, -0.5389, -0.8212, -0.9915, -0.9449, -0.9700, -0.8915,\n",
      "        -0.5251, -0.5921,  0.5179, -0.9985, -0.5684, -0.5182, -0.5450, -0.9810,\n",
      "        -0.5432, -0.7079, -0.5209, -0.5220, -0.5995, -0.6840, -0.5203, -0.5532,\n",
      "        -0.5940, -0.6614, -0.6275, -0.5292, -0.9255, -0.5379, -0.8380, -0.5481,\n",
      "        -0.5117, -0.5322, -0.5729, -0.5248, -0.5711, -0.5203, -0.8241, -0.6063,\n",
      "        -0.5374,  0.5028, -0.5145, -1.0000, -0.5759, -0.5519, -0.5462, -0.5475,\n",
      "        -0.8176, -0.5358, -0.5815, -0.6141, -0.8838, -0.5393, -0.8955,  0.7062,\n",
      "        -0.5107, -0.5174, -0.5489, -0.5587, -0.5311,  0.5140, -0.9776, -0.5351,\n",
      "        -0.5407, -0.5597, -0.5588, -0.7216, -0.9983, -0.5324, -0.6534, -0.9989,\n",
      "        -0.6988, -0.5913, -0.5480, -0.6760, -0.6164, -0.5908, -0.9549, -0.5223,\n",
      "        -0.5490, -0.6147, -0.5135, -0.5252, -0.8680, -0.5108, -0.5306, -0.9811,\n",
      "        -0.5087, -0.7991, -0.5329, -1.0000, -0.7814, -0.5275, -0.5261, -0.7081,\n",
      "        -0.5126, -0.5414, -0.6670, -0.6855, -0.5401, -0.5502, -0.5568, -0.7042,\n",
      "        -1.0000, -0.9519, -0.6417, -0.6788, -0.6871, -0.5216, -0.9973, -0.5711,\n",
      "        -0.5795, -0.9989, -0.5504, -0.5708, -0.5424, -0.7769, -0.9976, -0.5601,\n",
      "        -0.5119, -0.6201, -0.9308, -0.5324, -0.5737, -0.5295, -0.5584, -0.5211,\n",
      "        -0.5628, -0.5514, -0.5667, -0.5684, -0.5165, -0.6048, -0.5153, -0.9688,\n",
      "        -0.5330, -0.9870, -0.5640, -0.9247, -0.9687, -0.5644, -0.5274, -0.9921,\n",
      "        -0.9449, -0.5560, -0.5931, -0.6594, -0.7124, -0.6713, -0.5260, -0.5630,\n",
      "        -0.5500, -0.5166, -0.5265, -0.5715, -0.9669, -0.5271, -0.5215, -0.5314,\n",
      "        -0.6360, -0.5277, -0.5850, -0.5180, -0.5367, -0.7784, -0.5270, -0.5832,\n",
      "        -0.9530, -0.5679,  0.5108, -0.5384, -0.5649, -0.5549, -0.5577, -0.5296,\n",
      "        -0.6102, -0.5725, -0.5208, -0.6519, -0.6036, -0.9575, -0.5167, -0.5438,\n",
      "        -0.5820, -0.5091, -0.5608, -0.9854, -0.5356, -0.9894, -1.0000, -0.5504,\n",
      "        -0.5178, -0.8375, -0.7551, -0.5732, -0.7724, -0.5636, -0.8859, -1.0000,\n",
      "        -0.5855, -0.9977, -0.5260, -0.8895, -0.9501, -0.6649, -0.5692, -0.5626,\n",
      "        -0.5646, -0.5172, -0.5206, -0.6727, -0.5197, -0.7350, -0.5263, -0.5248,\n",
      "        -0.5213, -0.8466, -0.5832, -0.6278, -0.5331, -0.5308, -0.5428, -0.5713,\n",
      "        -0.8856, -0.5160, -0.5204, -0.7457, -0.5550, -0.5148, -0.5477, -0.5900,\n",
      "        -0.9884, -0.6611, -0.6883, -0.5349, -0.5359, -0.7997, -0.9996, -0.5530,\n",
      "        -0.5809, -0.7699, -0.8713, -0.9729, -0.9903, -0.8969, -0.5124, -0.5140,\n",
      "        -0.5150, -0.5849, -0.7262, -0.5960, -0.7232,  0.5085, -0.9763, -0.5147,\n",
      "        -0.5926, -0.5457, -0.5082, -0.7207, -0.9948, -0.5834, -0.5148, -0.5758],\n",
      "       device='cuda:0')\n",
      "tensor([[[[ 0.5166, -0.6225, -0.5421],\n",
      "          [-0.5192, -0.5169, -0.5170],\n",
      "          [ 0.5220,  0.5162,  0.5157]],\n",
      "\n",
      "         [[-0.5580, -0.6342, -0.5276],\n",
      "          [ 0.8580, -0.5342, -0.5234],\n",
      "          [-0.5263, -0.5281, -0.5200]],\n",
      "\n",
      "         [[ 0.5317,  0.5152, -0.5974],\n",
      "          [-0.5161, -0.5156, -0.5154],\n",
      "          [-0.5154, -0.5151, -0.6030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5271, -0.7712, -0.9991],\n",
      "          [-0.5156, -0.5224, -0.5279],\n",
      "          [ 0.5424,  0.5268,  0.9535]],\n",
      "\n",
      "         [[-0.5316, -0.5272, -0.5150],\n",
      "          [ 0.5182,  0.5281,  0.5285],\n",
      "          [-0.5199, -0.5255, -0.5171]],\n",
      "\n",
      "         [[ 0.5207, -0.9073, -0.5210],\n",
      "          [ 0.9986,  0.5328, -0.5179],\n",
      "          [-0.5159,  0.5198,  0.5286]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5207,  0.5216,  0.5154],\n",
      "          [ 0.5158,  0.5154,  0.5293],\n",
      "          [ 0.5184, -0.6018, -0.9867]],\n",
      "\n",
      "         [[ 0.5287,  0.5318,  0.5289],\n",
      "          [ 0.5180,  0.5149,  0.5323],\n",
      "          [ 0.5287,  0.5685,  0.5409]],\n",
      "\n",
      "         [[ 0.5257,  0.5215,  0.5155],\n",
      "          [ 0.5310,  0.5258,  0.5294],\n",
      "          [ 0.6919, -0.5930, -0.7424]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5314,  0.5194,  0.5291],\n",
      "          [ 0.5239,  0.5174,  0.5279],\n",
      "          [ 0.5371, -0.5286,  1.0000]],\n",
      "\n",
      "         [[-0.9999, -0.6499, -0.9997],\n",
      "          [-0.5283, -0.7243, -0.9999],\n",
      "          [-0.8651, -1.0000, -0.9996]],\n",
      "\n",
      "         [[ 0.5292,  0.5147,  0.5292],\n",
      "          [ 0.5257,  0.5231,  0.5314],\n",
      "          [ 0.5429, -0.9995, -0.9993]]],\n",
      "\n",
      "\n",
      "        [[[-0.5172, -0.5708,  0.6477],\n",
      "          [-0.5223, -0.5326, -0.5180],\n",
      "          [-0.5175, -0.5180, -0.5291]],\n",
      "\n",
      "         [[-0.6788, -0.9986,  0.5592],\n",
      "          [-0.5266, -0.5155, -0.5278],\n",
      "          [-0.5156, -0.5324, -0.5291]],\n",
      "\n",
      "         [[-0.5327, -0.7252,  0.9994],\n",
      "          [-0.5147, -0.5169, -0.5300],\n",
      "          [-0.5308, -0.5151, -0.5236]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5314, -0.5190,  1.0000],\n",
      "          [-0.5217, -0.5155, -0.5296],\n",
      "          [-0.5239, -0.5150, -0.5150]],\n",
      "\n",
      "         [[ 0.9999,  0.5161,  0.5254],\n",
      "          [ 0.6353,  0.5611,  0.7597],\n",
      "          [-1.0000,  0.9993, -0.6306]],\n",
      "\n",
      "         [[ 0.9985,  0.6351,  0.5246],\n",
      "          [-0.5185, -0.5259, -0.5231],\n",
      "          [-0.5206, -0.5323, -0.5321]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0000, -0.5301, -0.7666],\n",
      "          [-0.5163, -0.6529, -0.5301],\n",
      "          [ 1.0000,  0.6817, -0.5291]],\n",
      "\n",
      "         [[-0.9967,  1.0000, -1.0000],\n",
      "          [-0.5310, -0.5180, -0.5928],\n",
      "          [ 0.6867,  0.5210,  0.8672]],\n",
      "\n",
      "         [[-0.6466, -0.6639, -0.5581],\n",
      "          [-0.5204, -0.5219, -1.0000],\n",
      "          [-1.0000,  0.5834, -1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5259, -0.6167, -0.7869],\n",
      "          [-0.5167, -0.5253, -0.5199],\n",
      "          [-0.5299, -0.5540, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000, -0.7684],\n",
      "          [-0.5273, -0.5153, -0.6176],\n",
      "          [-0.5954, -0.5242, -0.5436]],\n",
      "\n",
      "         [[-0.5257, -0.5172, -1.0000],\n",
      "          [-0.5195, -0.5206, -0.5248],\n",
      "          [-0.5188, -0.5499, -0.5309]]],\n",
      "\n",
      "\n",
      "        [[[-0.5258, -0.5213, -1.0000],\n",
      "          [-0.5247, -0.5152, -0.5190],\n",
      "          [-0.5327, -0.5164, -0.5151]],\n",
      "\n",
      "         [[-0.5324, -0.5440, -0.8470],\n",
      "          [-0.5192, -0.5202, -0.5266],\n",
      "          [-0.5152, -0.5317, -0.5185]],\n",
      "\n",
      "         [[-0.6693, -0.9996,  0.5305],\n",
      "          [-0.5180, -0.5182, -0.5186],\n",
      "          [-0.5297, -0.5303, -0.5228]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5171, -0.5187, -0.7731],\n",
      "          [-0.5211, -0.5147, -0.5318],\n",
      "          [-0.5270, -0.5301, -0.5165]],\n",
      "\n",
      "         [[ 0.9983,  0.7544,  0.5832],\n",
      "          [ 0.9984,  0.6942,  0.8106],\n",
      "          [-0.5302, -0.7259, -0.5792]],\n",
      "\n",
      "         [[-0.5195,  0.9991,  0.5325],\n",
      "          [-0.5201, -0.5278, -0.5148],\n",
      "          [-0.5275, -0.5282, -0.5322]]],\n",
      "\n",
      "\n",
      "        [[[-0.6170,  0.9993,  0.9257],\n",
      "          [-0.5291, -0.5209, -0.5207],\n",
      "          [-0.5286, -1.0000, -0.5463]],\n",
      "\n",
      "         [[ 0.5957, -1.0000,  0.9275],\n",
      "          [-0.5239, -0.5701, -0.5592],\n",
      "          [ 0.6001,  0.5396,  0.5304]],\n",
      "\n",
      "         [[ 0.9999,  0.7062, -0.6405],\n",
      "          [-0.5183, -0.5272,  1.0000],\n",
      "          [-0.5687,  0.9979,  0.5255]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6085,  1.0000, -1.0000],\n",
      "          [-0.5229, -0.5194, -0.5290],\n",
      "          [-0.6362, -0.7861,  0.6405]],\n",
      "\n",
      "         [[ 0.5578,  0.5228,  0.5178],\n",
      "          [ 0.5878,  0.6162, -0.9991],\n",
      "          [-0.6108,  0.5637,  0.9968]],\n",
      "\n",
      "         [[-0.8361,  0.5504,  0.5294],\n",
      "          [-0.5311, -0.6328,  0.9994],\n",
      "          [-0.5691,  0.6334,  0.5224]]]], device='cuda:0')\n",
      "tensor([ 1.0000, -0.5525,  0.5159,  0.5281,  0.9548,  0.5152,  0.5297,  0.5182,\n",
      "        -0.5187,  0.5167,  0.5147,  0.5169,  0.5351,  0.5225,  0.5132, -0.5162,\n",
      "         0.5243,  0.5281,  0.5150,  1.0000,  0.5160,  0.5155,  0.5144,  0.5159,\n",
      "         0.5947,  0.5277,  0.5193,  0.6798,  0.5348,  0.5354, -0.5208,  0.5156,\n",
      "        -0.5155,  0.5200,  0.9497,  0.5361, -0.5804,  0.5204,  0.5172,  0.9395,\n",
      "         0.5297,  0.5142,  0.5157,  0.5168,  0.5247, -0.5132,  0.5157,  0.5142,\n",
      "         0.5149, -0.5124,  0.5397,  0.5304, -0.5127, -0.5235,  0.9582,  0.5208,\n",
      "         0.5112,  0.5141,  0.5152,  0.5400,  0.9981, -0.5193, -0.5167,  0.5163,\n",
      "         0.9757,  0.5176, -0.5263,  0.5152,  0.5138,  0.5117,  0.5190, -0.5280,\n",
      "         0.5175,  0.5162,  0.5184,  0.5161,  0.5402, -0.5200,  1.0000,  0.5186,\n",
      "         0.5162,  0.5261,  0.5171,  0.5915, -0.7407, -0.5199,  0.5491,  0.5286,\n",
      "        -0.5308,  0.5430,  0.5148,  0.5209,  0.5126,  0.5188,  0.5189,  0.7372,\n",
      "         1.0000, -0.5323,  0.5230,  0.5164,  0.8165, -0.5182,  0.5893,  0.5142,\n",
      "         0.5153, -0.5083, -0.5167,  0.5153,  0.8963,  0.5854,  0.5304,  0.5182,\n",
      "        -0.5270, -0.5635,  0.5189,  0.9311,  0.5172, -0.5164, -0.5151,  0.5181,\n",
      "         0.5266,  0.5160,  0.5146, -0.5115,  0.5184,  0.5148,  0.5381,  0.5102,\n",
      "        -0.5262,  0.5160,  1.0000,  0.5151,  0.5358,  0.5144,  0.5236, -0.5095,\n",
      "         0.5159,  0.5146,  0.5134, -0.5140,  0.5269,  0.5174,  0.5205, -0.5170,\n",
      "         0.5203,  0.5144,  0.5118,  0.5151,  0.5119,  0.5169, -0.5278,  0.8842,\n",
      "         0.6107,  0.5153, -0.5241,  0.5179,  0.5190,  0.5252,  0.5149,  0.5152,\n",
      "         0.5151,  0.5193, -0.5217,  0.5727,  1.0000,  0.5149,  0.5221,  0.5150,\n",
      "         0.5076, -0.5376,  0.8386, -0.5285,  0.5220, -0.5148,  0.5146,  0.5192,\n",
      "         0.5291,  0.5439,  0.5694,  0.9984, -0.5222,  0.5147,  0.5227,  0.5310,\n",
      "        -0.5271,  0.5285, -0.5286,  0.5288,  0.6863, -0.5195,  0.5166,  0.5146,\n",
      "         0.5438, -0.5369, -0.5201,  0.5163, -0.5149,  0.5437,  0.9995,  0.5163,\n",
      "         0.8319,  0.5306, -0.5279,  0.5153,  0.5167,  0.5185,  0.5170,  0.5209,\n",
      "         0.5196,  0.5173,  0.5283,  0.5273,  0.5229,  0.5163,  0.5190,  0.9609,\n",
      "         0.5200,  0.5303,  0.5268, -0.5192,  0.5377,  0.5172,  0.5172,  1.0000,\n",
      "         0.5314,  0.5201,  0.5273,  0.5179,  0.5235,  0.5094,  0.5342,  0.5197,\n",
      "        -0.5192,  0.5141,  0.5334,  0.5175,  0.5164,  0.5173,  0.5197,  0.5977,\n",
      "        -0.6587,  0.5193,  0.5149, -0.5222,  0.5158,  0.5127,  0.5386,  0.5136,\n",
      "        -0.5716,  0.5245,  0.5155,  0.5148,  0.5231,  0.9946,  0.5293,  0.5140,\n",
      "         0.5182,  0.5174,  0.5174,  0.5319,  0.5149, -0.5263, -0.5169,  0.5183,\n",
      "         0.5162,  0.5256,  0.5144, -0.6964,  0.5158,  0.5289,  0.5171,  0.5237,\n",
      "         0.5153,  0.5213, -0.5210,  0.5163,  0.5241,  0.5264,  0.5138,  0.5336,\n",
      "         0.5158, -0.9639, -0.5141,  0.5154,  0.5147,  0.5154,  0.5606,  0.5164,\n",
      "         0.5241,  0.5198,  0.5170,  0.5181,  0.5214,  0.5252,  0.5371,  0.5131,\n",
      "         0.5175,  0.5150,  0.9935,  0.5366,  0.5151,  0.5170,  0.5143,  1.0000,\n",
      "        -0.5226,  0.5466,  0.5222,  0.5181,  0.5192, -0.5196,  0.5177, -0.5265,\n",
      "         0.5152,  0.5162,  0.5235,  0.5276,  0.5160, -0.5660,  0.5409,  0.5178,\n",
      "         0.5316,  0.5219,  0.5154,  0.5148, -0.5247,  0.5167, -0.5183,  0.5152,\n",
      "         0.5185,  0.5177,  0.5104,  0.5209,  0.6091,  0.9904,  0.5201,  0.5208,\n",
      "        -0.9830, -0.5216, -1.0000,  0.5352,  0.5208,  0.5368,  0.5149,  0.5385,\n",
      "         0.5140,  0.9402,  0.5152,  0.5188,  0.9301,  0.9842, -0.5163, -0.5170,\n",
      "        -0.5166,  0.6698,  0.5139,  0.8047,  0.5170,  0.5231,  0.5183,  0.5230,\n",
      "        -0.5467, -0.5153,  0.5145,  0.5230, -0.9116,  0.7131,  0.5183,  0.5172,\n",
      "         0.9908,  0.5259,  0.7136,  0.5169,  0.8662,  0.5160,  0.5159,  0.5132,\n",
      "         0.5575,  0.9231,  0.5401, -0.5165,  0.5178,  0.5287,  0.5169,  0.5819,\n",
      "         0.5543,  0.5220,  0.5147,  0.5339, -0.5337,  0.5147,  0.5183,  0.9696,\n",
      "         0.5161,  0.9841,  0.5341,  0.5147,  0.5307,  0.5172,  0.5308,  0.5162,\n",
      "         0.5152,  0.5539,  0.5341,  0.5174,  0.5150,  0.9870,  0.5155,  0.5160,\n",
      "         0.5168,  0.8555,  0.5194,  0.5482,  0.5249,  0.5160,  0.5288,  0.9651,\n",
      "         0.5209,  1.0000,  0.5279,  0.5215,  0.5312,  0.5132,  0.5174,  0.5151,\n",
      "        -0.5188,  0.9227,  0.9938,  0.9937,  0.5145,  0.5194, -0.5140,  0.5147,\n",
      "        -0.9581, -0.5108,  0.8886,  0.9902,  0.5336, -0.5429,  0.5220,  0.5212,\n",
      "         0.5209,  0.5300,  0.5163,  0.9973,  0.5379, -0.5253,  0.5324, -0.5325,\n",
      "         0.5162,  0.5250, -0.5499,  0.5227,  0.5158,  0.5382,  0.5162,  0.5135,\n",
      "         0.5149,  1.0000, -0.5552,  0.5148,  0.5156,  0.5135,  0.5207,  0.5335,\n",
      "         0.5184, -0.5525,  0.5300,  0.5424, -0.5142,  0.9773,  0.5154,  0.5085,\n",
      "         0.9776,  0.5959, -0.5427,  0.5172,  0.6100,  0.5131,  0.5191,  0.5214,\n",
      "         0.5352,  0.5251, -0.5570,  0.5165,  0.5114,  0.5179,  0.5322,  0.5577,\n",
      "         0.5192,  0.5272, -0.5334,  0.5173,  0.5257,  0.5215,  0.5170,  0.5289,\n",
      "         0.5153, -0.5183,  0.5256, -0.5211,  0.5199,  0.5187,  0.6855,  0.5251,\n",
      "        -0.5235, -0.5173,  0.5292,  0.9610,  0.5169,  0.5178,  0.5154,  0.5223],\n",
      "       device='cuda:0')\n",
      "tensor([-0.5372, -0.9867, -1.0000, -0.9933, -0.9629, -1.0000, -0.9954, -0.9999,\n",
      "        -1.0000, -1.0000, -1.0000, -0.9983, -0.9923, -0.9969, -1.0000, -1.0000,\n",
      "        -0.9987, -0.9812, -1.0000, -0.4917, -1.0000, -1.0000, -1.0000, -0.9996,\n",
      "        -0.9654, -0.9970, -0.9982, -0.9552, -0.9897, -0.9894, -0.9972, -1.0000,\n",
      "        -0.9997, -0.9983, -0.6662, -0.9951, -0.9679, -0.9971, -0.9993, -0.9706,\n",
      "        -0.9972, -1.0000, -1.0000, -0.9997, -0.9968, -1.0000, -0.9993, -1.0000,\n",
      "        -1.0000, -1.0000, -0.9903, -1.0000, -1.0000, -0.9960, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -0.9899, -0.5299, -0.9985, -1.0000, -1.0000,\n",
      "        -0.5828, -0.9989, -1.0000, -0.9999, -1.0000, -1.0000, -0.9986, -0.9960,\n",
      "        -0.9940, -1.0000, -0.9982, -1.0000, -1.0000, -0.9983, -0.5127, -0.9973,\n",
      "        -1.0000, -0.9965, -0.9992, -0.9073, -0.9467, -1.0000, -0.9999, -0.9944,\n",
      "        -0.9969, -0.9888, -1.0000, -1.0000, -1.0000, -1.0000, -0.9985, -0.9486,\n",
      "        -0.5402, -1.0000, -0.9976, -1.0000, -0.9765, -0.9990, -0.9639, -1.0000,\n",
      "        -0.9999, -1.0000, -0.9972, -0.9997, -0.9405, -0.9646, -0.9890, -1.0000,\n",
      "        -0.9974, -0.9811, -0.9994, -0.8815, -1.0000, -1.0000, -1.0000, -0.9981,\n",
      "        -0.9903, -0.9999, -1.0000, -1.0000, -1.0000, -1.0000, -0.9800, -1.0000,\n",
      "        -0.9972, -1.0000, -0.9951, -0.9994, -0.9946, -1.0000, -0.9961, -1.0000,\n",
      "        -0.9998, -0.9999, -1.0000, -1.0000, -0.9996, -1.0000, -0.9973, -0.9280,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9955, -0.6909,\n",
      "        -0.9602, -1.0000, -0.9994, -0.9993, -0.9997, -0.9966, -1.0000, -1.0000,\n",
      "        -1.0000, -0.9995, -0.9970, -0.9729, -0.5483, -1.0000, -0.9994, -1.0000,\n",
      "        -1.0000, -0.9953, -0.9633, -0.9891, -0.9980, -1.0000, -1.0000, -0.9999,\n",
      "        -0.9932, -0.9923, -0.9996, -0.7687, -0.9990, -1.0000, -0.9975, -0.9951,\n",
      "        -0.9998, -0.9937, -0.9881, -0.9942, -0.9837, -0.9990, -0.9996, -1.0000,\n",
      "        -0.9895, -0.9891, -0.9997, -0.9998, -1.0000, -0.9877, -0.5222, -1.0000,\n",
      "        -0.9205, -0.9935, -0.9975, -1.0000, -0.9994, -0.9988, -0.9973, -1.0000,\n",
      "        -0.9985, -1.0000, -0.9935, -0.9982, -0.9966, -0.9998, -0.9983, -0.5947,\n",
      "        -0.9987, -0.9925, -0.9950, -0.9987, -0.9723, -0.9999, -1.0000, -0.6008,\n",
      "        -0.9967, -0.9979, -0.9997, -0.9985, -0.9969, -1.0000, -0.9949, -0.9975,\n",
      "        -0.9961, -1.0000, -0.9953, -0.9991, -1.0000, -0.9998, -0.9997, -0.9670,\n",
      "        -0.9454, -1.0000, -1.0000, -0.9988, -0.9997, -1.0000, -0.9937, -1.0000,\n",
      "        -0.9765, -0.9955, -1.0000, -1.0000, -0.9958, -0.6143, -0.9932, -1.0000,\n",
      "        -0.9998, -1.0000, -0.9994, -0.9954, -1.0000, -1.0000, -0.9991, -0.9985,\n",
      "        -1.0000, -0.9960, -1.0000, -1.0000, -1.0000, -0.9970, -1.0000, -0.9955,\n",
      "        -1.0000, -0.9998, -0.9969, -0.9993, -0.9983, -0.9944, -1.0000, -0.9933,\n",
      "        -0.9996, -0.6504, -1.0000, -0.9999, -1.0000, -1.0000, -0.9850, -1.0000,\n",
      "        -0.9888, -0.9989, -0.9994, -0.9866, -0.9967, -0.9989, -0.9880, -1.0000,\n",
      "        -1.0000, -0.9999, -0.7789, -0.9925, -0.9998, -0.9997, -1.0000, -0.6077,\n",
      "        -0.9969, -0.9890, -1.0000, -1.0000, -0.9982, -0.9983, -0.9990, -0.9995,\n",
      "        -1.0000, -1.0000, -0.9979, -0.9991, -1.0000, -0.9961, -0.9932, -0.9991,\n",
      "        -0.9970, -0.9977, -1.0000, -0.9976, -1.0000, -0.9997, -0.9988, -1.0000,\n",
      "        -0.9983, -1.0000, -1.0000, -0.9997, -0.9966, -0.5694, -1.0000, -0.9991,\n",
      "        -0.5579, -1.0000, -0.5258, -0.9914, -0.9977, -0.9958, -1.0000, -0.9954,\n",
      "        -1.0000, -0.5814, -1.0000, -0.9984, -0.6357, -0.9450, -1.0000, -0.9993,\n",
      "        -1.0000, -0.9618, -1.0000, -0.9113, -1.0000, -0.9969, -0.9987, -0.9968,\n",
      "        -0.9874, -1.0000, -1.0000, -1.0000, -0.6197, -1.0000, -0.9998, -0.9988,\n",
      "        -0.5599, -0.9990, -0.9909, -0.9595, -0.9659, -0.9995, -1.0000, -1.0000,\n",
      "        -1.0000, -0.6370, -0.9875, -1.0000, -0.9986, -0.9979, -0.9991, -0.9606,\n",
      "        -0.9804, -0.9963, -1.0000, -0.9927, -0.9927, -1.0000, -0.9997, -0.5779,\n",
      "        -1.0000, -0.5155, -0.9981, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -0.9997, -0.9806, -0.9976, -1.0000, -1.0000, -0.5502, -1.0000, -1.0000,\n",
      "        -1.0000, -0.9482, -0.9987, -0.9895, -0.9982, -0.9995, -0.9952, -0.7826,\n",
      "        -0.9982, -0.5175, -0.9945, -1.0000, -0.9956, -1.0000, -0.9993, -0.9999,\n",
      "        -0.9987, -0.8806, -0.6391, -0.5311, -1.0000, -0.9977, -1.0000, -1.0000,\n",
      "        -0.5527, -0.9994, -0.9531,  0.5499, -0.9970, -0.9923, -0.9966, -1.0000,\n",
      "        -0.9971, -0.9991, -1.0000, -0.5877, -0.9965, -1.0000, -0.9907, -0.9952,\n",
      "        -1.0000, -0.9981, -0.9848, -0.9987, -0.9999, -0.9850, -0.9998, -1.0000,\n",
      "        -1.0000, -0.5476, -0.9980, -1.0000, -0.9989, -1.0000, -0.9974, -0.9981,\n",
      "        -1.0000, -0.9889, -0.9960, -0.9901, -1.0000, -0.5553, -1.0000, -1.0000,\n",
      "        -0.7476, -0.9874, -1.0000, -0.9993, -0.9714, -1.0000, -1.0000, -0.9981,\n",
      "        -0.9943, -1.0000, -0.9813, -0.9995, -1.0000, -0.9992, -0.9983, -0.9805,\n",
      "        -0.9982, -0.9948, -0.9905, -0.9989, -0.9982, -0.9950, -1.0000, -0.9973,\n",
      "        -1.0000, -1.0000, -0.9919, -0.9982, -1.0000, -0.9971, -0.9718, -0.9966,\n",
      "        -0.9986, -1.0000, -0.9958, -0.6855, -1.0000, -0.9991, -1.0000, -1.0000],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.5713,  0.7737, -0.5264,  ...,  0.5326, -0.5268,  0.9165],\n",
      "        [-0.9878, -0.8074, -0.9537,  ..., -0.9884, -0.9938, -0.9684],\n",
      "        [-0.9424,  0.5429, -0.9207,  ...,  0.5307, -0.8336, -0.5679],\n",
      "        ...,\n",
      "        [ 0.9813, -0.5692,  0.6255,  ...,  0.5182,  0.5407,  0.5190],\n",
      "        [ 1.0000,  0.5266,  0.9125,  ...,  0.5234,  0.9595, -0.5234],\n",
      "        [ 0.9988,  0.5210,  0.5297,  ...,  0.5231,  0.5190,  0.6049]],\n",
      "       device='cuda:0')\n",
      "Test's ac is: 59.500%\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load(FILE)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        print(param.data)\n",
    "    for data in testloader:\n",
    "        net.eval()\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    print('Test\\'s ac is: %.3f%%' % (100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
